{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43ebd09",
   "metadata": {},
   "source": [
    "# Package and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d9c10",
   "metadata": {},
   "source": [
    "Inspiration:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "ResNet architecture - split convolutional layers into blocks based on size of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24cc331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:25:48.226856Z",
     "start_time": "2022-04-06T21:25:46.029274Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e92335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:25:51.672879Z",
     "start_time": "2022-04-06T21:25:50.469130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cbc2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T22:56:28.156942Z",
     "start_time": "2022-04-06T22:56:28.140985Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f928c9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Base CNN (2 Layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8d82af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:07:29.582304Z",
     "start_time": "2022-04-06T15:07:29.441680Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), #1 #### change to 64?\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17012585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:07:29.597264Z",
     "start_time": "2022-04-06T15:07:29.582304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efeb67e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:19:14.060274Z",
     "start_time": "2022-04-06T15:07:29.598261Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0127, acc 42.4120 \n",
      "Accuracy of the network on the 10000 test images: 54.41 %\n",
      "epoch : 2\n",
      "training loss: 0.0088, acc 59.7600 \n",
      "Accuracy of the network on the 10000 test images: 61.8 %\n",
      "epoch : 3\n",
      "training loss: 0.0071, acc 68.2540 \n",
      "Accuracy of the network on the 10000 test images: 66.86 %\n",
      "epoch : 4\n",
      "training loss: 0.0060, acc 73.6020 \n",
      "Accuracy of the network on the 10000 test images: 70.23 %\n",
      "epoch : 5\n",
      "training loss: 0.0051, acc 77.8820 \n",
      "Accuracy of the network on the 10000 test images: 70.84 %\n",
      "epoch : 6\n",
      "training loss: 0.0042, acc 81.9360 \n",
      "Accuracy of the network on the 10000 test images: 71.12 %\n",
      "epoch : 7\n",
      "training loss: 0.0034, acc 86.1220 \n",
      "Accuracy of the network on the 10000 test images: 71.53 %\n",
      "epoch : 8\n",
      "training loss: 0.0026, acc 90.0220 \n",
      "Accuracy of the network on the 10000 test images: 71.99 %\n",
      "epoch : 9\n",
      "training loss: 0.0019, acc 93.6380 \n",
      "Accuracy of the network on the 10000 test images: 72.12 %\n",
      "epoch : 10\n",
      "training loss: 0.0013, acc 96.5140 \n",
      "Accuracy of the network on the 10000 test images: 72.67 %\n",
      "epoch : 11\n",
      "training loss: 0.0009, acc 98.3940 \n",
      "Accuracy of the network on the 10000 test images: 72.37 %\n",
      "epoch : 12\n",
      "training loss: 0.0006, acc 99.3440 \n",
      "Accuracy of the network on the 10000 test images: 72.33 %\n",
      "epoch : 13\n",
      "training loss: 0.0004, acc 99.7200 \n",
      "Accuracy of the network on the 10000 test images: 72.66 %\n",
      "epoch : 14\n",
      "training loss: 0.0003, acc 99.8740 \n",
      "Accuracy of the network on the 10000 test images: 72.6 %\n",
      "epoch : 15\n",
      "training loss: 0.0002, acc 99.9420 \n",
      "Accuracy of the network on the 10000 test images: 72.59 %\n",
      "epoch : 16\n",
      "training loss: 0.0002, acc 99.9720 \n",
      "Accuracy of the network on the 10000 test images: 72.82 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.9860 \n",
      "Accuracy of the network on the 10000 test images: 72.81 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.9880 \n",
      "Accuracy of the network on the 10000 test images: 72.87 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 72.92 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.93 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.04 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.86 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.05 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.07 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.13 %\n",
      "epoch : 26\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.08 %\n",
      "epoch : 27\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.11 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.2 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.09 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.02 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.07 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.03 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.2 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.22 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.2 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.1 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.09 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.17 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.23 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.26 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.2 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.11 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.13 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.35 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.16 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.46 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.38 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.35 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.34 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.21 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592ed8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:19:14.076231Z",
     "start_time": "2022-04-06T15:19:14.062269Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net1 - training loss'] = running_loss_history\n",
    "results['net1 - training accuracy'] = running_corrects_history\n",
    "results['net1 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7e83c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41145ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:19:14.235805Z",
     "start_time": "2022-04-06T15:19:14.077229Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330b22e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:19:14.251762Z",
     "start_time": "2022-04-06T15:19:14.236802Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b26a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:33:01.619181Z",
     "start_time": "2022-04-06T15:19:14.252759Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0114, acc 48.5180 \n",
      "Accuracy of the network on the 10000 test images: 59.77 %\n",
      "epoch : 2\n",
      "training loss: 0.0074, acc 66.6260 \n",
      "Accuracy of the network on the 10000 test images: 68.75 %\n",
      "epoch : 3\n",
      "training loss: 0.0057, acc 74.8220 \n",
      "Accuracy of the network on the 10000 test images: 72.24 %\n",
      "epoch : 4\n",
      "training loss: 0.0044, acc 80.6320 \n",
      "Accuracy of the network on the 10000 test images: 73.45 %\n",
      "epoch : 5\n",
      "training loss: 0.0034, acc 86.1020 \n",
      "Accuracy of the network on the 10000 test images: 74.7 %\n",
      "epoch : 6\n",
      "training loss: 0.0023, acc 91.1780 \n",
      "Accuracy of the network on the 10000 test images: 75.26 %\n",
      "epoch : 7\n",
      "training loss: 0.0014, acc 95.5940 \n",
      "Accuracy of the network on the 10000 test images: 75.0 %\n",
      "epoch : 8\n",
      "training loss: 0.0008, acc 98.4500 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 9\n",
      "training loss: 0.0004, acc 99.5840 \n",
      "Accuracy of the network on the 10000 test images: 75.67 %\n",
      "epoch : 10\n",
      "training loss: 0.0002, acc 99.9040 \n",
      "Accuracy of the network on the 10000 test images: 75.81 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.9700 \n",
      "Accuracy of the network on the 10000 test images: 76.19 %\n",
      "epoch : 12\n",
      "training loss: 0.0001, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 75.99 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.24 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.37 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.13 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.25 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.26 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.23 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.29 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.32 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.25 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.32 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.27 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.39 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.31 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.45 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.36 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.42 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.46 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.31 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.4 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.51 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.56 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.48 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.46 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.43 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.59 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.5 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.59 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.55 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.39 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.58 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.65 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.63 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.7 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.59 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.49 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.49 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.6 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.58 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acee9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:33:01.634142Z",
     "start_time": "2022-04-06T15:33:01.620179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net2 - training loss'] = running_loss_history\n",
    "results['net2 - training accuracy'] = running_corrects_history\n",
    "results['net2 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9c2b7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa82d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:33:01.946306Z",
     "start_time": "2022-04-06T15:33:01.635139Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=65536, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38abc25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:33:01.961266Z",
     "start_time": "2022-04-06T15:33:01.947304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50406e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:51:22.051926Z",
     "start_time": "2022-04-06T15:33:01.962264Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0108, acc 50.8220 \n",
      "Accuracy of the network on the 10000 test images: 61.73 %\n",
      "epoch : 2\n",
      "training loss: 0.0068, acc 69.1860 \n",
      "Accuracy of the network on the 10000 test images: 68.59 %\n",
      "epoch : 3\n",
      "training loss: 0.0050, acc 78.1040 \n",
      "Accuracy of the network on the 10000 test images: 72.57 %\n",
      "epoch : 4\n",
      "training loss: 0.0034, acc 85.6640 \n",
      "Accuracy of the network on the 10000 test images: 74.36 %\n",
      "epoch : 5\n",
      "training loss: 0.0019, acc 92.9460 \n",
      "Accuracy of the network on the 10000 test images: 73.94 %\n",
      "epoch : 6\n",
      "training loss: 0.0008, acc 97.9880 \n",
      "Accuracy of the network on the 10000 test images: 74.81 %\n",
      "epoch : 7\n",
      "training loss: 0.0003, acc 99.7640 \n",
      "Accuracy of the network on the 10000 test images: 75.38 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.9500 \n",
      "Accuracy of the network on the 10000 test images: 76.51 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.9940 \n",
      "Accuracy of the network on the 10000 test images: 76.41 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.66 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.75 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.6 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.61 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.79 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.79 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.76 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.72 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.61 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.82 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.95 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.9 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.02 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.89 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.94 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.95 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.99 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.83 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.95 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.1 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.0 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.12 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.93 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.99 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.0 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.02 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.09 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.99 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.15 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.15 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.09 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.08 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.1 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.21 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.3 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.23 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.13 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.22 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.26 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.24 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.17 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480dea67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:51:22.067685Z",
     "start_time": "2022-04-06T15:51:22.052924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net3 - training loss'] = running_loss_history\n",
    "results['net3 - training accuracy'] = running_corrects_history\n",
    "results['net3 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd28b06",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9754f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:51:22.669077Z",
     "start_time": "2022-04-06T15:51:22.068683Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=131072, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f918819b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T15:51:22.685035Z",
     "start_time": "2022-04-06T15:51:22.670075Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c234529a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T16:21:41.636117Z",
     "start_time": "2022-04-06T15:51:22.686032Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0102, acc 53.2280 \n",
      "Accuracy of the network on the 10000 test images: 66.09 %\n",
      "epoch : 2\n",
      "training loss: 0.0060, acc 73.1140 \n",
      "Accuracy of the network on the 10000 test images: 72.04 %\n",
      "epoch : 3\n",
      "training loss: 0.0038, acc 83.7520 \n",
      "Accuracy of the network on the 10000 test images: 74.57 %\n",
      "epoch : 4\n",
      "training loss: 0.0019, acc 93.2400 \n",
      "Accuracy of the network on the 10000 test images: 76.02 %\n",
      "epoch : 5\n",
      "training loss: 0.0006, acc 98.7380 \n",
      "Accuracy of the network on the 10000 test images: 76.41 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.9440 \n",
      "Accuracy of the network on the 10000 test images: 78.2 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 78.4 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.54 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.52 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.53 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.54 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.68 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.62 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.52 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.6 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.77 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.7 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.83 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.71 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.78 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.82 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.8 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.76 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.82 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.83 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.9 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.87 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.89 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.93 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.94 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.95 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.91 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.87 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.06 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.11 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.07 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.03 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.99 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.04 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.11 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.1 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.15 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.16 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.24 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.13 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.22 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.15 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.24 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61f7b8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T16:21:41.651077Z",
     "start_time": "2022-04-06T16:21:41.637115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net4 - training loss'] = running_loss_history\n",
    "results['net4 - training accuracy'] = running_corrects_history\n",
    "results['net4 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d6af4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eee2a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T16:35:12.659594Z",
     "start_time": "2022-04-06T16:35:11.307096Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=262144, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(1024),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f01ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T16:35:12.675551Z",
     "start_time": "2022-04-06T16:35:12.660591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e5a7118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T17:58:43.806786Z",
     "start_time": "2022-04-06T16:35:12.676548Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0103, acc 52.4480 \n",
      "Accuracy of the network on the 10000 test images: 64.85 %\n",
      "epoch : 2\n",
      "training loss: 0.0058, acc 73.7400 \n",
      "Accuracy of the network on the 10000 test images: 72.32 %\n",
      "epoch : 3\n",
      "training loss: 0.0032, acc 86.6280 \n",
      "Accuracy of the network on the 10000 test images: 73.49 %\n",
      "epoch : 4\n",
      "training loss: 0.0010, acc 96.8480 \n",
      "Accuracy of the network on the 10000 test images: 74.84 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.7620 \n",
      "Accuracy of the network on the 10000 test images: 76.77 %\n",
      "epoch : 6\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 77.68 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.61 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.78 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.88 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.78 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.86 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.92 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.88 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.93 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.87 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.87 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.0 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.92 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.99 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.95 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.98 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.95 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.0 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.97 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.07 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.04 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.07 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.07 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.05 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.04 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.08 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.11 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.09 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.11 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.09 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.15 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.12 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.17 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.25 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.15 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.09 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.23 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.37 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.22 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.44 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.27 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70af0af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:01:22.062886Z",
     "start_time": "2022-04-06T18:01:22.049921Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-62e51a0b031d>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_loss_history = torch.tensor(running_loss_history)\n",
      "<ipython-input-10-62e51a0b031d>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_corrects_history = torch.tensor(running_corrects_history)\n",
      "<ipython-input-10-62e51a0b031d>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_acc_history = torch.tensor(test_acc_history)\n"
     ]
    }
   ],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net5 - training loss'] = running_loss_history\n",
    "results['net5 - training accuracy'] = running_corrects_history\n",
    "results['net5 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da47b7f",
   "metadata": {},
   "source": [
    "# ---------------- Changing Neurons in Each Network ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0f4f1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "953f2861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:01:22.123723Z",
     "start_time": "2022-04-06T18:01:22.063883Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17ff7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:01:22.139681Z",
     "start_time": "2022-04-06T18:01:22.124721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3601df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:10:48.306829Z",
     "start_time": "2022-04-06T18:01:22.140678Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0138, acc 38.6780 \n",
      "Accuracy of the network on the 10000 test images: 49.9 %\n",
      "epoch : 2\n",
      "training loss: 0.0099, acc 55.1600 \n",
      "Accuracy of the network on the 10000 test images: 59.08 %\n",
      "epoch : 3\n",
      "training loss: 0.0082, acc 63.3060 \n",
      "Accuracy of the network on the 10000 test images: 64.47 %\n",
      "epoch : 4\n",
      "training loss: 0.0070, acc 68.8340 \n",
      "Accuracy of the network on the 10000 test images: 67.85 %\n",
      "epoch : 5\n",
      "training loss: 0.0062, acc 72.6760 \n",
      "Accuracy of the network on the 10000 test images: 68.5 %\n",
      "epoch : 6\n",
      "training loss: 0.0055, acc 75.7360 \n",
      "Accuracy of the network on the 10000 test images: 68.91 %\n",
      "epoch : 7\n",
      "training loss: 0.0049, acc 78.4800 \n",
      "Accuracy of the network on the 10000 test images: 69.3 %\n",
      "epoch : 8\n",
      "training loss: 0.0043, acc 81.5360 \n",
      "Accuracy of the network on the 10000 test images: 70.66 %\n",
      "epoch : 9\n",
      "training loss: 0.0037, acc 84.4400 \n",
      "Accuracy of the network on the 10000 test images: 70.83 %\n",
      "epoch : 10\n",
      "training loss: 0.0031, acc 87.3500 \n",
      "Accuracy of the network on the 10000 test images: 70.1 %\n",
      "epoch : 11\n",
      "training loss: 0.0026, acc 90.3760 \n",
      "Accuracy of the network on the 10000 test images: 71.08 %\n",
      "epoch : 12\n",
      "training loss: 0.0020, acc 93.1840 \n",
      "Accuracy of the network on the 10000 test images: 70.88 %\n",
      "epoch : 13\n",
      "training loss: 0.0015, acc 95.4980 \n",
      "Accuracy of the network on the 10000 test images: 70.63 %\n",
      "epoch : 14\n",
      "training loss: 0.0011, acc 97.4180 \n",
      "Accuracy of the network on the 10000 test images: 70.86 %\n",
      "epoch : 15\n",
      "training loss: 0.0008, acc 98.5280 \n",
      "Accuracy of the network on the 10000 test images: 71.06 %\n",
      "epoch : 16\n",
      "training loss: 0.0006, acc 99.3020 \n",
      "Accuracy of the network on the 10000 test images: 70.71 %\n",
      "epoch : 17\n",
      "training loss: 0.0004, acc 99.6500 \n",
      "Accuracy of the network on the 10000 test images: 70.98 %\n",
      "epoch : 18\n",
      "training loss: 0.0003, acc 99.8440 \n",
      "Accuracy of the network on the 10000 test images: 70.95 %\n",
      "epoch : 19\n",
      "training loss: 0.0002, acc 99.9320 \n",
      "Accuracy of the network on the 10000 test images: 71.11 %\n",
      "epoch : 20\n",
      "training loss: 0.0002, acc 99.9640 \n",
      "Accuracy of the network on the 10000 test images: 70.81 %\n",
      "epoch : 21\n",
      "training loss: 0.0002, acc 99.9760 \n",
      "Accuracy of the network on the 10000 test images: 71.16 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 99.9860 \n",
      "Accuracy of the network on the 10000 test images: 70.99 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 99.9940 \n",
      "Accuracy of the network on the 10000 test images: 70.98 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.9960 \n",
      "Accuracy of the network on the 10000 test images: 71.0 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 99.9960 \n",
      "Accuracy of the network on the 10000 test images: 70.97 %\n",
      "epoch : 26\n",
      "training loss: 0.0001, acc 99.9960 \n",
      "Accuracy of the network on the 10000 test images: 71.18 %\n",
      "epoch : 27\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 70.87 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.08 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 71.09 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.2 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.23 %\n",
      "epoch : 32\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.34 %\n",
      "epoch : 33\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.41 %\n",
      "epoch : 34\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.2 %\n",
      "epoch : 35\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.3 %\n",
      "epoch : 36\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.21 %\n",
      "epoch : 37\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.29 %\n",
      "epoch : 38\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.09 %\n",
      "epoch : 39\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.28 %\n",
      "epoch : 40\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.27 %\n",
      "epoch : 41\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.25 %\n",
      "epoch : 42\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.12 %\n",
      "epoch : 43\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.38 %\n",
      "epoch : 44\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.46 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.51 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.51 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.3 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.34 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.44 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 71.37 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7690e4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:10:48.321789Z",
     "start_time": "2022-04-06T18:10:48.307827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net6 - training loss'] = running_loss_history\n",
    "results['net6 - training accuracy'] = running_corrects_history\n",
    "results['net6 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f3045",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e77401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:10:48.415764Z",
     "start_time": "2022-04-06T18:10:48.322787Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d796263a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:10:48.431526Z",
     "start_time": "2022-04-06T18:10:48.416761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83689cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:21:09.620111Z",
     "start_time": "2022-04-06T18:10:48.432524Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0124, acc 43.5500 \n",
      "Accuracy of the network on the 10000 test images: 55.86 %\n",
      "epoch : 2\n",
      "training loss: 0.0085, acc 61.5120 \n",
      "Accuracy of the network on the 10000 test images: 64.55 %\n",
      "epoch : 3\n",
      "training loss: 0.0068, acc 69.3980 \n",
      "Accuracy of the network on the 10000 test images: 68.92 %\n",
      "epoch : 4\n",
      "training loss: 0.0058, acc 74.2160 \n",
      "Accuracy of the network on the 10000 test images: 71.55 %\n",
      "epoch : 5\n",
      "training loss: 0.0049, acc 78.3460 \n",
      "Accuracy of the network on the 10000 test images: 72.67 %\n",
      "epoch : 6\n",
      "training loss: 0.0042, acc 81.9900 \n",
      "Accuracy of the network on the 10000 test images: 73.2 %\n",
      "epoch : 7\n",
      "training loss: 0.0034, acc 85.9200 \n",
      "Accuracy of the network on the 10000 test images: 74.19 %\n",
      "epoch : 8\n",
      "training loss: 0.0026, acc 89.6260 \n",
      "Accuracy of the network on the 10000 test images: 74.49 %\n",
      "epoch : 9\n",
      "training loss: 0.0019, acc 93.1560 \n",
      "Accuracy of the network on the 10000 test images: 74.47 %\n",
      "epoch : 10\n",
      "training loss: 0.0013, acc 96.1460 \n",
      "Accuracy of the network on the 10000 test images: 74.86 %\n",
      "epoch : 11\n",
      "training loss: 0.0009, acc 98.1140 \n",
      "Accuracy of the network on the 10000 test images: 74.4 %\n",
      "epoch : 12\n",
      "training loss: 0.0005, acc 99.2660 \n",
      "Accuracy of the network on the 10000 test images: 74.62 %\n",
      "epoch : 13\n",
      "training loss: 0.0003, acc 99.7500 \n",
      "Accuracy of the network on the 10000 test images: 74.78 %\n",
      "epoch : 14\n",
      "training loss: 0.0002, acc 99.9380 \n",
      "Accuracy of the network on the 10000 test images: 75.03 %\n",
      "epoch : 15\n",
      "training loss: 0.0002, acc 99.9740 \n",
      "Accuracy of the network on the 10000 test images: 75.03 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.9880 \n",
      "Accuracy of the network on the 10000 test images: 74.88 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.9960 \n",
      "Accuracy of the network on the 10000 test images: 75.05 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.9960 \n",
      "Accuracy of the network on the 10000 test images: 75.14 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 75.32 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 75.23 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.23 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.38 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.29 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.2 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.21 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.37 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.32 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.35 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.31 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.33 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.32 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.46 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.36 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.22 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.38 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.55 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.35 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.62 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.33 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.37 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.34 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.29 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.32 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.58 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.19 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.58 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.25 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374a25a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:21:09.636068Z",
     "start_time": "2022-04-06T18:21:09.621108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net7 - training loss'] = running_loss_history\n",
    "results['net7 - training accuracy'] = running_corrects_history\n",
    "results['net7 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae4266",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540075c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:21:09.806613Z",
     "start_time": "2022-04-06T18:21:09.637066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "455d912a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:21:09.822610Z",
     "start_time": "2022-04-06T18:21:09.807609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac294781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:33:36.595806Z",
     "start_time": "2022-04-06T18:21:09.823607Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0121, acc 44.4940 \n",
      "Accuracy of the network on the 10000 test images: 56.59 %\n",
      "epoch : 2\n",
      "training loss: 0.0082, acc 62.9260 \n",
      "Accuracy of the network on the 10000 test images: 64.48 %\n",
      "epoch : 3\n",
      "training loss: 0.0064, acc 71.2960 \n",
      "Accuracy of the network on the 10000 test images: 69.5 %\n",
      "epoch : 4\n",
      "training loss: 0.0051, acc 77.7640 \n",
      "Accuracy of the network on the 10000 test images: 72.25 %\n",
      "epoch : 5\n",
      "training loss: 0.0039, acc 83.5940 \n",
      "Accuracy of the network on the 10000 test images: 72.8 %\n",
      "epoch : 6\n",
      "training loss: 0.0028, acc 88.9240 \n",
      "Accuracy of the network on the 10000 test images: 72.95 %\n",
      "epoch : 7\n",
      "training loss: 0.0017, acc 94.0980 \n",
      "Accuracy of the network on the 10000 test images: 73.27 %\n",
      "epoch : 8\n",
      "training loss: 0.0010, acc 97.5720 \n",
      "Accuracy of the network on the 10000 test images: 73.51 %\n",
      "epoch : 9\n",
      "training loss: 0.0005, acc 99.4020 \n",
      "Accuracy of the network on the 10000 test images: 73.67 %\n",
      "epoch : 10\n",
      "training loss: 0.0002, acc 99.9120 \n",
      "Accuracy of the network on the 10000 test images: 74.14 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.9780 \n",
      "Accuracy of the network on the 10000 test images: 74.41 %\n",
      "epoch : 12\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 74.47 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 74.62 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.53 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.59 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.75 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.76 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.81 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.85 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.71 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.79 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.78 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.78 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.86 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.84 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.83 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.82 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.93 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.01 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.95 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.06 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.93 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.95 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.13 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.0 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.98 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.98 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.95 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 74.96 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.17 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.14 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.23 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.09 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.25 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.12 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.25 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.18 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.21 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.26 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6464f30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:33:36.611763Z",
     "start_time": "2022-04-06T18:33:36.596803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net8 - training loss'] = running_loss_history\n",
    "results['net8 - training accuracy'] = running_corrects_history\n",
    "results['net8 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a172b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5c4e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:33:36.914723Z",
     "start_time": "2022-04-06T18:33:36.612761Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=65536, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92a7e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:33:36.930680Z",
     "start_time": "2022-04-06T18:33:36.914723Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff46de4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:50:43.097153Z",
     "start_time": "2022-04-06T18:33:36.931678Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0113, acc 47.8100 \n",
      "Accuracy of the network on the 10000 test images: 60.22 %\n",
      "epoch : 2\n",
      "training loss: 0.0072, acc 67.4440 \n",
      "Accuracy of the network on the 10000 test images: 68.49 %\n",
      "epoch : 3\n",
      "training loss: 0.0053, acc 76.4100 \n",
      "Accuracy of the network on the 10000 test images: 72.04 %\n",
      "epoch : 4\n",
      "training loss: 0.0038, acc 83.8280 \n",
      "Accuracy of the network on the 10000 test images: 73.71 %\n",
      "epoch : 5\n",
      "training loss: 0.0023, acc 91.0380 \n",
      "Accuracy of the network on the 10000 test images: 73.23 %\n",
      "epoch : 6\n",
      "training loss: 0.0011, acc 96.7300 \n",
      "Accuracy of the network on the 10000 test images: 74.57 %\n",
      "epoch : 7\n",
      "training loss: 0.0004, acc 99.3940 \n",
      "Accuracy of the network on the 10000 test images: 74.76 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.9260 \n",
      "Accuracy of the network on the 10000 test images: 74.83 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.9860 \n",
      "Accuracy of the network on the 10000 test images: 75.24 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.49 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.48 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.53 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.45 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.52 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.47 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.46 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.52 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.59 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.57 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.67 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.52 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.62 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.61 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.54 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.66 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.64 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.66 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.53 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.68 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.67 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.61 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.64 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.65 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.77 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.73 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.89 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.79 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.74 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.77 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.79 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.74 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.8 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.81 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.72 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.77 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.85 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.84 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.83 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.74 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.68 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c435663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:50:43.112905Z",
     "start_time": "2022-04-06T18:50:43.098151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net9 - training loss'] = running_loss_history\n",
    "results['net9 - training accuracy'] = running_corrects_history\n",
    "results['net9 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68936a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 10 Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "556c41ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:50:43.713300Z",
     "start_time": "2022-04-06T18:50:43.113903Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=131072, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83171665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:50:43.728907Z",
     "start_time": "2022-04-06T18:50:43.713300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89903fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:19:49.932462Z",
     "start_time": "2022-04-06T18:50:43.729905Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0111, acc 48.9840 \n",
      "Accuracy of the network on the 10000 test images: 60.34 %\n",
      "epoch : 2\n",
      "training loss: 0.0070, acc 68.5720 \n",
      "Accuracy of the network on the 10000 test images: 68.31 %\n",
      "epoch : 3\n",
      "training loss: 0.0048, acc 78.9500 \n",
      "Accuracy of the network on the 10000 test images: 71.26 %\n",
      "epoch : 4\n",
      "training loss: 0.0028, acc 88.6920 \n",
      "Accuracy of the network on the 10000 test images: 72.02 %\n",
      "epoch : 5\n",
      "training loss: 0.0011, acc 96.8080 \n",
      "Accuracy of the network on the 10000 test images: 72.67 %\n",
      "epoch : 6\n",
      "training loss: 0.0003, acc 99.5680 \n",
      "Accuracy of the network on the 10000 test images: 73.62 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.9840 \n",
      "Accuracy of the network on the 10000 test images: 74.72 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 75.09 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.35 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.19 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.2 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.26 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.13 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.34 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.32 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.19 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.31 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.4 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.4 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.39 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.5 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.39 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.5 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.34 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.46 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.43 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.54 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.43 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.51 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.36 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.39 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.5 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.51 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.62 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.58 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.67 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.62 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.56 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.56 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.79 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.6 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.65 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.62 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.71 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.68 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.72 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.79 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.73 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.68 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.83 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f424c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:19:49.948419Z",
     "start_time": "2022-04-06T19:19:49.933459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net10 - training loss'] = running_loss_history\n",
    "results['net10 - training accuracy'] = running_corrects_history\n",
    "results['net10 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aebd6f",
   "metadata": {},
   "source": [
    "# ------------------- Added Convolutional Layer per Block -------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6cfad",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68043eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:19:49.996291Z",
     "start_time": "2022-04-06T19:19:49.949417Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b1291c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:19:50.011252Z",
     "start_time": "2022-04-06T19:19:49.998286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbea4d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:29:41.790822Z",
     "start_time": "2022-04-06T19:19:50.012250Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0135, acc 39.0920 \n",
      "Accuracy of the network on the 10000 test images: 50.31 %\n",
      "epoch : 2\n",
      "training loss: 0.0100, acc 54.1160 \n",
      "Accuracy of the network on the 10000 test images: 57.81 %\n",
      "epoch : 3\n",
      "training loss: 0.0085, acc 61.7300 \n",
      "Accuracy of the network on the 10000 test images: 63.45 %\n",
      "epoch : 4\n",
      "training loss: 0.0073, acc 67.4460 \n",
      "Accuracy of the network on the 10000 test images: 66.66 %\n",
      "epoch : 5\n",
      "training loss: 0.0064, acc 71.6060 \n",
      "Accuracy of the network on the 10000 test images: 68.45 %\n",
      "epoch : 6\n",
      "training loss: 0.0057, acc 74.9980 \n",
      "Accuracy of the network on the 10000 test images: 70.29 %\n",
      "epoch : 7\n",
      "training loss: 0.0051, acc 77.9280 \n",
      "Accuracy of the network on the 10000 test images: 70.6 %\n",
      "epoch : 8\n",
      "training loss: 0.0045, acc 80.9720 \n",
      "Accuracy of the network on the 10000 test images: 70.72 %\n",
      "epoch : 9\n",
      "training loss: 0.0039, acc 83.6180 \n",
      "Accuracy of the network on the 10000 test images: 71.33 %\n",
      "epoch : 10\n",
      "training loss: 0.0033, acc 86.7660 \n",
      "Accuracy of the network on the 10000 test images: 72.0 %\n",
      "epoch : 11\n",
      "training loss: 0.0027, acc 89.4760 \n",
      "Accuracy of the network on the 10000 test images: 71.76 %\n",
      "epoch : 12\n",
      "training loss: 0.0021, acc 92.4220 \n",
      "Accuracy of the network on the 10000 test images: 72.09 %\n",
      "epoch : 13\n",
      "training loss: 0.0016, acc 95.0240 \n",
      "Accuracy of the network on the 10000 test images: 71.83 %\n",
      "epoch : 14\n",
      "training loss: 0.0012, acc 97.1140 \n",
      "Accuracy of the network on the 10000 test images: 71.78 %\n",
      "epoch : 15\n",
      "training loss: 0.0008, acc 98.4560 \n",
      "Accuracy of the network on the 10000 test images: 72.08 %\n",
      "epoch : 16\n",
      "training loss: 0.0006, acc 99.2100 \n",
      "Accuracy of the network on the 10000 test images: 72.39 %\n",
      "epoch : 17\n",
      "training loss: 0.0004, acc 99.6940 \n",
      "Accuracy of the network on the 10000 test images: 72.59 %\n",
      "epoch : 18\n",
      "training loss: 0.0003, acc 99.8520 \n",
      "Accuracy of the network on the 10000 test images: 72.52 %\n",
      "epoch : 19\n",
      "training loss: 0.0002, acc 99.9320 \n",
      "Accuracy of the network on the 10000 test images: 72.59 %\n",
      "epoch : 20\n",
      "training loss: 0.0002, acc 99.9700 \n",
      "Accuracy of the network on the 10000 test images: 72.68 %\n",
      "epoch : 21\n",
      "training loss: 0.0002, acc 99.9800 \n",
      "Accuracy of the network on the 10000 test images: 72.7 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 99.9880 \n",
      "Accuracy of the network on the 10000 test images: 72.98 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 72.85 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.9940 \n",
      "Accuracy of the network on the 10000 test images: 72.8 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 72.93 %\n",
      "epoch : 26\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.89 %\n",
      "epoch : 27\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.76 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.9 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.86 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.86 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 72.76 %\n",
      "epoch : 32\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.92 %\n",
      "epoch : 33\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.05 %\n",
      "epoch : 34\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.91 %\n",
      "epoch : 35\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.88 %\n",
      "epoch : 36\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.94 %\n",
      "epoch : 37\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.89 %\n",
      "epoch : 38\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.88 %\n",
      "epoch : 39\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.96 %\n",
      "epoch : 40\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.83 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.02 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.94 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.91 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.94 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.1 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.99 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.06 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.91 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 72.95 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 73.07 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7b0fbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:29:41.806779Z",
     "start_time": "2022-04-06T19:29:41.791820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net11 - training loss'] = running_loss_history\n",
    "results['net11 - training accuracy'] = running_corrects_history\n",
    "results['net11 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f7b2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4d236ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:29:41.901542Z",
     "start_time": "2022-04-06T19:29:41.807777Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "041336eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:29:41.917500Z",
     "start_time": "2022-04-06T19:29:41.902539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7c79bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:40:45.602455Z",
     "start_time": "2022-04-06T19:29:41.918498Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0124, acc 42.8600 \n",
      "Accuracy of the network on the 10000 test images: 54.12 %\n",
      "epoch : 2\n",
      "training loss: 0.0088, acc 60.0320 \n",
      "Accuracy of the network on the 10000 test images: 63.21 %\n",
      "epoch : 3\n",
      "training loss: 0.0071, acc 67.9780 \n",
      "Accuracy of the network on the 10000 test images: 69.44 %\n",
      "epoch : 4\n",
      "training loss: 0.0061, acc 72.8540 \n",
      "Accuracy of the network on the 10000 test images: 70.57 %\n",
      "epoch : 5\n",
      "training loss: 0.0053, acc 76.6900 \n",
      "Accuracy of the network on the 10000 test images: 73.21 %\n",
      "epoch : 6\n",
      "training loss: 0.0046, acc 79.9300 \n",
      "Accuracy of the network on the 10000 test images: 74.2 %\n",
      "epoch : 7\n",
      "training loss: 0.0039, acc 83.0880 \n",
      "Accuracy of the network on the 10000 test images: 74.6 %\n",
      "epoch : 8\n",
      "training loss: 0.0032, acc 86.4560 \n",
      "Accuracy of the network on the 10000 test images: 75.11 %\n",
      "epoch : 9\n",
      "training loss: 0.0026, acc 89.8540 \n",
      "Accuracy of the network on the 10000 test images: 75.28 %\n",
      "epoch : 10\n",
      "training loss: 0.0019, acc 92.9060 \n",
      "Accuracy of the network on the 10000 test images: 75.47 %\n",
      "epoch : 11\n",
      "training loss: 0.0014, acc 95.6960 \n",
      "Accuracy of the network on the 10000 test images: 75.43 %\n",
      "epoch : 12\n",
      "training loss: 0.0009, acc 97.8740 \n",
      "Accuracy of the network on the 10000 test images: 75.65 %\n",
      "epoch : 13\n",
      "training loss: 0.0006, acc 99.0800 \n",
      "Accuracy of the network on the 10000 test images: 75.61 %\n",
      "epoch : 14\n",
      "training loss: 0.0004, acc 99.6840 \n",
      "Accuracy of the network on the 10000 test images: 75.59 %\n",
      "epoch : 15\n",
      "training loss: 0.0002, acc 99.9140 \n",
      "Accuracy of the network on the 10000 test images: 76.0 %\n",
      "epoch : 16\n",
      "training loss: 0.0002, acc 99.9740 \n",
      "Accuracy of the network on the 10000 test images: 76.07 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 76.01 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 76.12 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 76.13 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 99.9980 \n",
      "Accuracy of the network on the 10000 test images: 76.09 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.1 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.28 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 75.89 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.15 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.09 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.16 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.08 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.01 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.13 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.04 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.1 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.05 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.15 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.03 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.01 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.17 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.28 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.12 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.14 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.06 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.2 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.05 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.15 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.26 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.06 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.16 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.17 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.3 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.13 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 76.32 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b859642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:40:45.618385Z",
     "start_time": "2022-04-06T19:40:45.603425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net12 - training loss'] = running_loss_history\n",
    "results['net12 - training accuracy'] = running_corrects_history\n",
    "results['net12 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2b81f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "435c8114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:40:45.790923Z",
     "start_time": "2022-04-06T19:40:45.619382Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Flatten(start_dim=1, end_dim=-1)\n",
       "    (23): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (24): ReLU()\n",
       "    (25): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "363ac576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:40:45.806880Z",
     "start_time": "2022-04-06T19:40:45.791920Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f58b454d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:54:23.776281Z",
     "start_time": "2022-04-06T19:40:45.807877Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0119, acc 44.5060 \n",
      "Accuracy of the network on the 10000 test images: 55.5 %\n",
      "epoch : 2\n",
      "training loss: 0.0081, acc 62.9220 \n",
      "Accuracy of the network on the 10000 test images: 66.01 %\n",
      "epoch : 3\n",
      "training loss: 0.0064, acc 71.0520 \n",
      "Accuracy of the network on the 10000 test images: 71.41 %\n",
      "epoch : 4\n",
      "training loss: 0.0053, acc 76.3320 \n",
      "Accuracy of the network on the 10000 test images: 73.8 %\n",
      "epoch : 5\n",
      "training loss: 0.0044, acc 80.5840 \n",
      "Accuracy of the network on the 10000 test images: 75.14 %\n",
      "epoch : 6\n",
      "training loss: 0.0036, acc 84.7740 \n",
      "Accuracy of the network on the 10000 test images: 75.98 %\n",
      "epoch : 7\n",
      "training loss: 0.0027, acc 88.4800 \n",
      "Accuracy of the network on the 10000 test images: 76.06 %\n",
      "epoch : 8\n",
      "training loss: 0.0020, acc 92.4360 \n",
      "Accuracy of the network on the 10000 test images: 75.85 %\n",
      "epoch : 9\n",
      "training loss: 0.0012, acc 95.9640 \n",
      "Accuracy of the network on the 10000 test images: 76.19 %\n",
      "epoch : 10\n",
      "training loss: 0.0007, acc 98.4600 \n",
      "Accuracy of the network on the 10000 test images: 76.81 %\n",
      "epoch : 11\n",
      "training loss: 0.0003, acc 99.6920 \n",
      "Accuracy of the network on the 10000 test images: 77.11 %\n",
      "epoch : 12\n",
      "training loss: 0.0002, acc 99.9400 \n",
      "Accuracy of the network on the 10000 test images: 77.81 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 77.82 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.75 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.82 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.89 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.92 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.95 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.93 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.99 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.98 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.91 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.91 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.86 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.93 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.98 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.76 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.93 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.96 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.89 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.02 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.14 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.06 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.15 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.86 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.05 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.99 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.98 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.1 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.87 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.07 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.01 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.94 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.1 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.12 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.26 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1dcb6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:54:23.792238Z",
     "start_time": "2022-04-06T19:54:23.777278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net13 - training loss'] = running_loss_history\n",
    "results['net13 - training accuracy'] = running_corrects_history\n",
    "results['net13 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04074",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a407a44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:54:24.109360Z",
     "start_time": "2022-04-06T19:54:23.793236Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Flatten(start_dim=1, end_dim=-1)\n",
       "    (30): Linear(in_features=65536, out_features=1024, bias=True)\n",
       "    (31): ReLU()\n",
       "    (32): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9da2c848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T19:54:24.125318Z",
     "start_time": "2022-04-06T19:54:24.110358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a06bb541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T20:13:36.602616Z",
     "start_time": "2022-04-06T19:54:24.126315Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0117, acc 45.2900 \n",
      "Accuracy of the network on the 10000 test images: 59.48 %\n",
      "epoch : 2\n",
      "training loss: 0.0078, acc 64.5020 \n",
      "Accuracy of the network on the 10000 test images: 66.33 %\n",
      "epoch : 3\n",
      "training loss: 0.0060, acc 72.9580 \n",
      "Accuracy of the network on the 10000 test images: 71.66 %\n",
      "epoch : 4\n",
      "training loss: 0.0047, acc 78.9400 \n",
      "Accuracy of the network on the 10000 test images: 73.89 %\n",
      "epoch : 5\n",
      "training loss: 0.0037, acc 83.9980 \n",
      "Accuracy of the network on the 10000 test images: 75.71 %\n",
      "epoch : 6\n",
      "training loss: 0.0026, acc 89.1800 \n",
      "Accuracy of the network on the 10000 test images: 75.73 %\n",
      "epoch : 7\n",
      "training loss: 0.0016, acc 94.0960 \n",
      "Accuracy of the network on the 10000 test images: 75.93 %\n",
      "epoch : 8\n",
      "training loss: 0.0008, acc 97.7520 \n",
      "Accuracy of the network on the 10000 test images: 76.41 %\n",
      "epoch : 9\n",
      "training loss: 0.0003, acc 99.5080 \n",
      "Accuracy of the network on the 10000 test images: 77.3 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.9600 \n",
      "Accuracy of the network on the 10000 test images: 77.79 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.93 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.15 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.05 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.09 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.11 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.1 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.02 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.05 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.24 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.08 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.18 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.07 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.18 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.12 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.2 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.22 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.19 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.29 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.24 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.04 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 77.94 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.16 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.12 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.08 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.25 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.25 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.21 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.19 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.17 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.15 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.23 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36d9cd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T20:13:36.617944Z",
     "start_time": "2022-04-06T20:13:36.603641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net14 - training loss'] = running_loss_history\n",
    "results['net14 - training accuracy'] = running_corrects_history\n",
    "results['net14 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa0b44",
   "metadata": {},
   "source": [
    "## 10 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd4f4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:25:58.728529Z",
     "start_time": "2022-04-06T21:25:58.013675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (30): ReLU()\n",
       "    (31): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (32): ReLU()\n",
       "    (33): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): ReLU()\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): Flatten(start_dim=1, end_dim=-1)\n",
       "    (37): Linear(in_features=131072, out_features=1024, bias=True)\n",
       "    (38): ReLU()\n",
       "    (39): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (40): ReLU()\n",
       "    (41): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*16*16, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e42c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:25:58.743248Z",
     "start_time": "2022-04-06T21:25:58.729526Z"
    }
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c8aba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:59:07.448046Z",
     "start_time": "2022-04-06T21:25:59.462327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0117, acc 44.9540 \n",
      "Accuracy of the network on the 10000 test images: 57.77 %\n",
      "epoch : 2\n",
      "training loss: 0.0077, acc 65.0560 \n",
      "Accuracy of the network on the 10000 test images: 68.03 %\n",
      "epoch : 3\n",
      "training loss: 0.0057, acc 74.4900 \n",
      "Accuracy of the network on the 10000 test images: 72.72 %\n",
      "epoch : 4\n",
      "training loss: 0.0043, acc 81.1420 \n",
      "Accuracy of the network on the 10000 test images: 74.81 %\n",
      "epoch : 5\n",
      "training loss: 0.0030, acc 86.9520 \n",
      "Accuracy of the network on the 10000 test images: 75.42 %\n",
      "epoch : 6\n",
      "training loss: 0.0017, acc 93.3800 \n",
      "Accuracy of the network on the 10000 test images: 76.1 %\n",
      "epoch : 7\n",
      "training loss: 0.0008, acc 97.5640 \n",
      "Accuracy of the network on the 10000 test images: 76.68 %\n",
      "epoch : 8\n",
      "training loss: 0.0003, acc 99.5400 \n",
      "Accuracy of the network on the 10000 test images: 77.53 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.9560 \n",
      "Accuracy of the network on the 10000 test images: 78.54 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.74 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.91 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.83 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.9 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.88 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.95 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.9 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.0 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.99 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.98 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.01 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.94 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.04 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.04 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.9 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.83 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.14 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.05 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.94 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.99 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.12 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 78.96 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.03 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.03 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.17 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.14 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.18 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.14 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.31 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.18 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.08 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.09 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.17 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.27 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.21 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.15 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.23 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 79.25 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea4134a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T22:56:31.446566Z",
     "start_time": "2022-04-06T22:56:31.421242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ef79a6783269>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_loss_history = torch.tensor(running_loss_history)\n",
      "<ipython-input-9-ef79a6783269>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_corrects_history = torch.tensor(running_corrects_history)\n",
      "<ipython-input-9-ef79a6783269>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_acc_history = torch.tensor(test_acc_history)\n"
     ]
    }
   ],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net15 - training loss'] = running_loss_history\n",
    "results['net15 - training accuracy'] = running_corrects_history\n",
    "results['net15 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab411a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

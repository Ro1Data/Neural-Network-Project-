{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43ebd09",
   "metadata": {
    "id": "a43ebd09"
   },
   "source": [
    "# Package and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d9c10",
   "metadata": {
    "id": "e00d9c10"
   },
   "source": [
    "Inspiration:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "ResNet architecture - split convolutional layers into blocks based on size of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24cc331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:46:41.159304Z",
     "start_time": "2022-04-08T14:46:39.081824Z"
    },
    "id": "a24cc331"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e92335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:46:42.151226Z",
     "start_time": "2022-04-08T14:46:42.080143Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477,
     "referenced_widgets": [
      "eae018f67c894b60806bd13d8130ff85",
      "8e4528665c2f4af3915011d342b2d08d",
      "497df2fee2d64396bff5b1a9e0af5045",
      "ec84850f19bb4027bfcb827ab85acc2b",
      "fd53777ba1ad486d9bac399bbdaff141",
      "ce7cc300cfa44dc2b3b9bcf4e90e34a9",
      "1c63e3bda2df4c9b8c410e28fbb9ca1b",
      "12a60a5ab9a14d70810abc2a1099f32e",
      "081af12a75184c6091a274d028182ca9",
      "257c41c0442e4a648789ec978a8c0af4",
      "c80c5844880c49edab730d65c19e1712",
      "321f6569581949eda9fb739ab2fc5efb",
      "2bdba06d7623412da0a4cd0e26e6ff2e",
      "50ac188b44914b7a993937f82301afea",
      "ce500c3b7fec466184d9d847c5e0de5a",
      "200201f06282443aac2304647e1be222",
      "99b062f811b44176a910e48981ed191d",
      "74129c62b3124becb14b8172c4e7b864",
      "effc835a317841989b2eafe0a0e9ab72",
      "e2de304d953c47438bc4cd1ee43c7b65",
      "0fd7f50eaedd42c4902974456998b45e",
      "b54b372479cc4660978b192891cee13f",
      "00c8ae6c23824f0a8482a2b96df532fa",
      "9b24b28503dc40c4a0733f9d33725c6b",
      "1183a64f165d418e8b31d5cb7bdb4e78",
      "a8f28237e19c40b5b76178bb04dcf21f",
      "c298f35420314a29b61191720be89f53",
      "9e7281b04d4b4087bbaaae59c35a9092",
      "fd119b2e3a6049e6b788d3cf5908d5f6",
      "673cfc01d9504685922bb03716d2d7bc",
      "22edd38b97374ecb888b7cbef6c4c130",
      "c9e94b9355534bb78aaab90ec23ef81d",
      "26b1e7315e144eb9b74fbbfe7f2830d9",
      "95870391b50046c68a63b82a518dbafe",
      "23f51ee3583343b2ab490180cd3a7d27",
      "77e8dca5c85a4986a8131cd53c79d189",
      "c0c3c3e3866748879752d3ef50228a73",
      "bd68c9d150734460b97718412420afa9",
      "1d342f4d9282493aa040abfb8056e2a7",
      "8dd8b65a5ab94a6bb369dda20c307865",
      "a6d50fb183e34e42b58a7f24ba68d428",
      "3bafe01580514d84bb220735861ab9fe",
      "c86dd7860f6d4ba298c369c552814c55",
      "346d3e11a8cc429780c2020fe2f1b1a8"
     ]
    },
    "id": "89e92335",
    "outputId": "6278c8df-fd60-4055-c906-850604471ec3"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(( 0.5), ( 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "iZPEV1Uueojx",
   "metadata": {
    "id": "iZPEV1Uueojx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cbc2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:47:12.380506Z",
     "start_time": "2022-04-08T14:47:12.368541Z"
    },
    "id": "83cbc2f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f928c9",
   "metadata": {
    "id": "76f928c9"
   },
   "source": [
    "# Base CNN (2 Layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8d82af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:37:40.272901Z",
     "start_time": "2022-04-08T11:37:40.115322Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e8d82af",
    "outputId": "2230c57e-44df-43b4-ea58-41f2afe1a786",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1 #### change to 64?\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17012585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:37:42.976341Z",
     "start_time": "2022-04-08T11:37:42.971363Z"
    },
    "id": "17012585"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efeb67e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:42:08.492395Z",
     "start_time": "2022-04-08T11:37:43.548350Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "efeb67e3",
    "outputId": "1409bbe0-ce77-4d35-e5c3-4aa6698ed4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0032, acc 90.3050 \n",
      "Accuracy of the network on the 10000 test images: 97.09 %\n",
      "epoch : 2\n",
      "training loss: 0.0007, acc 97.7317 \n",
      "Accuracy of the network on the 10000 test images: 98.27 %\n",
      "epoch : 3\n",
      "training loss: 0.0004, acc 98.4567 \n",
      "Accuracy of the network on the 10000 test images: 98.51 %\n",
      "epoch : 4\n",
      "training loss: 0.0004, acc 98.8250 \n",
      "Accuracy of the network on the 10000 test images: 98.67 %\n",
      "epoch : 5\n",
      "training loss: 0.0003, acc 98.9717 \n",
      "Accuracy of the network on the 10000 test images: 98.69 %\n",
      "epoch : 6\n",
      "training loss: 0.0003, acc 99.1650 \n",
      "Accuracy of the network on the 10000 test images: 98.89 %\n",
      "epoch : 7\n",
      "training loss: 0.0002, acc 99.2900 \n",
      "Accuracy of the network on the 10000 test images: 98.83 %\n",
      "epoch : 8\n",
      "training loss: 0.0002, acc 99.3883 \n",
      "Accuracy of the network on the 10000 test images: 99.01 %\n",
      "epoch : 9\n",
      "training loss: 0.0002, acc 99.4867 \n",
      "Accuracy of the network on the 10000 test images: 98.91 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.5867 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.6583 \n",
      "Accuracy of the network on the 10000 test images: 99.02 %\n",
      "epoch : 12\n",
      "training loss: 0.0001, acc 99.7183 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.7583 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 99.7950 \n",
      "Accuracy of the network on the 10000 test images: 99.0 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 99.8317 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.8617 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.8933 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.9000 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.9333 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 99.9450 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 99.9600 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9567 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9667 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9733 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592ed8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:42:27.401377Z",
     "start_time": "2022-04-08T11:42:27.388412Z"
    },
    "id": "592ed8e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-31a04b1e1451>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_loss_history = torch.tensor(running_loss_history)\n",
      "<ipython-input-8-31a04b1e1451>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  running_corrects_history = torch.tensor(running_corrects_history)\n",
      "<ipython-input-8-31a04b1e1451>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_acc_history = torch.tensor(test_acc_history)\n"
     ]
    }
   ],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net1 - training loss'] = running_loss_history\n",
    "results['net1 - training accuracy'] = running_corrects_history\n",
    "results['net1 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7e83c",
   "metadata": {
    "id": "59b7e83c"
   },
   "source": [
    "# 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41145ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:44:21.753508Z",
     "start_time": "2022-04-08T11:44:21.624367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41145ab7",
    "outputId": "c09a0404-6d58-4999-959f-4f3fc99440b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330b22e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:44:21.769466Z",
     "start_time": "2022-04-08T11:44:21.754505Z"
    },
    "id": "330b22e1"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b26a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:50:34.433470Z",
     "start_time": "2022-04-08T11:44:21.770463Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "23b26a0e",
    "outputId": "efb83f6b-6509-4c48-ba3e-7d7bbfea68f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0024, acc 93.0517 \n",
      "Accuracy of the network on the 10000 test images: 98.12 %\n",
      "epoch : 2\n",
      "training loss: 0.0004, acc 98.4933 \n",
      "Accuracy of the network on the 10000 test images: 98.74 %\n",
      "epoch : 3\n",
      "training loss: 0.0003, acc 98.9767 \n",
      "Accuracy of the network on the 10000 test images: 99.01 %\n",
      "epoch : 4\n",
      "training loss: 0.0002, acc 99.2567 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.4183 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.5567 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.6733 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.7517 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.8067 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.8817 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.9167 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9350 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9450 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9667 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9783 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acee9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:50:34.448445Z",
     "start_time": "2022-04-08T11:50:34.434468Z"
    },
    "id": "6acee9cb"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net2 - training loss'] = running_loss_history\n",
    "results['net2 - training accuracy'] = running_corrects_history\n",
    "results['net2 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9c2b7",
   "metadata": {
    "id": "cbc9c2b7"
   },
   "source": [
    "# 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa82d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:50:34.712803Z",
     "start_time": "2022-04-08T11:50:34.449443Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baa82d83",
    "outputId": "709b7382-819a-44c6-a085-a3212504b1b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38abc25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T11:50:34.728510Z",
     "start_time": "2022-04-08T11:50:34.713770Z"
    },
    "id": "38abc25a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50406e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:00:47.820805Z",
     "start_time": "2022-04-08T11:50:34.728510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "50406e31",
    "outputId": "55ddd866-fdb2-4cac-9fa9-a119580c0405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0017, acc 94.6467 \n",
      "Accuracy of the network on the 10000 test images: 98.64 %\n",
      "epoch : 2\n",
      "training loss: 0.0003, acc 98.9000 \n",
      "Accuracy of the network on the 10000 test images: 98.97 %\n",
      "epoch : 3\n",
      "training loss: 0.0002, acc 99.3000 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.5083 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 5\n",
      "training loss: 0.0001, acc 99.7067 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.7883 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.8700 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9100 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9533 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9750 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480dea67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:00:47.836729Z",
     "start_time": "2022-04-08T12:00:47.821797Z"
    },
    "id": "480dea67"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net3 - training loss'] = running_loss_history\n",
    "results['net3 - training accuracy'] = running_corrects_history\n",
    "results['net3 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd28b06",
   "metadata": {
    "id": "5dd28b06"
   },
   "source": [
    "# 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9754f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:00:48.351353Z",
     "start_time": "2022-04-08T12:00:47.837727Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9754f0d",
    "outputId": "5bd54fe4-8609-4640-af40-9f7306cbcdf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f918819b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:00:48.367311Z",
     "start_time": "2022-04-08T12:00:48.351353Z"
    },
    "id": "f918819b"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c234529a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:21:44.666629Z",
     "start_time": "2022-04-08T12:00:48.368308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "c234529a",
    "outputId": "2e4c07a8-504a-428e-a64e-0139dc857f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0014, acc 95.1683 \n",
      "Accuracy of the network on the 10000 test images: 98.83 %\n",
      "epoch : 2\n",
      "training loss: 0.0002, acc 99.1300 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 3\n",
      "training loss: 0.0001, acc 99.5117 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.7300 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 5\n",
      "training loss: 0.0001, acc 99.8900 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 6\n",
      "training loss: 0.0000, acc 99.9567 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 99.9767 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f7b8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:21:44.682095Z",
     "start_time": "2022-04-08T12:21:44.667105Z"
    },
    "id": "61f7b8b5"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net4 - training loss'] = running_loss_history\n",
    "results['net4 - training accuracy'] = running_corrects_history\n",
    "results['net4 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d6af4",
   "metadata": {
    "id": "653d6af4"
   },
   "source": [
    "# 10 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eee2a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:21:45.741263Z",
     "start_time": "2022-04-08T12:21:44.683065Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eee2a21",
    "outputId": "f7c898d7-4c2c-409d-85b1-de0b0d6b6a34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=200704, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(1024),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(200704, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f01ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T12:21:45.756629Z",
     "start_time": "2022-04-08T12:21:45.742233Z"
    },
    "id": "d4f01ef2"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5a7118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:27:15.635751Z",
     "start_time": "2022-04-08T12:21:45.757626Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e5a7118",
    "outputId": "717b745b-18e7-4db0-b90f-d580825706a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0012, acc 95.5667 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 2\n",
      "training loss: 0.0002, acc 99.3767 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 3\n",
      "training loss: 0.0001, acc 99.7950 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 4\n",
      "training loss: 0.0000, acc 99.9550 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 5\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 6\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70af0af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:27:15.650710Z",
     "start_time": "2022-04-08T13:27:15.636748Z"
    },
    "id": "70af0af5",
    "outputId": "45b22e73-c7c2-4484-d7e9-8a1097d18f78"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net5 - training loss'] = running_loss_history\n",
    "results['net5 - training accuracy'] = running_corrects_history\n",
    "results['net5 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da47b7f",
   "metadata": {
    "id": "9da47b7f"
   },
   "source": [
    "# ---------------- Changing Neurons in Each Network ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0f4f1",
   "metadata": {
    "id": "77e0f4f1"
   },
   "source": [
    "## 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953f2861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:27:15.696588Z",
     "start_time": "2022-04-08T13:27:15.651709Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "953f2861",
    "outputId": "c5c23e0d-630c-42ab-9977-35c7edb9b57a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e17ff7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:27:15.712546Z",
     "start_time": "2022-04-08T13:27:15.697586Z"
    },
    "id": "e17ff7e2"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3601df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:30:51.120371Z",
     "start_time": "2022-04-08T13:27:15.713544Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "5d3601df",
    "outputId": "8fa4f44b-4bba-401f-820d-4cd8de2c2ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0040, acc 88.4967 \n",
      "Accuracy of the network on the 10000 test images: 96.69 %\n",
      "epoch : 2\n",
      "training loss: 0.0008, acc 97.2167 \n",
      "Accuracy of the network on the 10000 test images: 97.81 %\n",
      "epoch : 3\n",
      "training loss: 0.0006, acc 98.0517 \n",
      "Accuracy of the network on the 10000 test images: 98.15 %\n",
      "epoch : 4\n",
      "training loss: 0.0004, acc 98.4700 \n",
      "Accuracy of the network on the 10000 test images: 98.49 %\n",
      "epoch : 5\n",
      "training loss: 0.0004, acc 98.7083 \n",
      "Accuracy of the network on the 10000 test images: 98.75 %\n",
      "epoch : 6\n",
      "training loss: 0.0003, acc 98.8967 \n",
      "Accuracy of the network on the 10000 test images: 98.78 %\n",
      "epoch : 7\n",
      "training loss: 0.0003, acc 99.0383 \n",
      "Accuracy of the network on the 10000 test images: 98.86 %\n",
      "epoch : 8\n",
      "training loss: 0.0002, acc 99.1500 \n",
      "Accuracy of the network on the 10000 test images: 98.9 %\n",
      "epoch : 9\n",
      "training loss: 0.0002, acc 99.2467 \n",
      "Accuracy of the network on the 10000 test images: 98.84 %\n",
      "epoch : 10\n",
      "training loss: 0.0002, acc 99.3333 \n",
      "Accuracy of the network on the 10000 test images: 98.99 %\n",
      "epoch : 11\n",
      "training loss: 0.0002, acc 99.4283 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 12\n",
      "training loss: 0.0002, acc 99.4867 \n",
      "Accuracy of the network on the 10000 test images: 98.97 %\n",
      "epoch : 13\n",
      "training loss: 0.0002, acc 99.5450 \n",
      "Accuracy of the network on the 10000 test images: 99.02 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 99.5883 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 99.6483 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.6800 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.7250 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.7933 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.7833 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 99.8167 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 99.8350 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 99.8400 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 99.8633 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.8833 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 99.8950 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 26\n",
      "training loss: 0.0001, acc 99.9133 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 27\n",
      "training loss: 0.0001, acc 99.9167 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 99.9183 \n",
      "Accuracy of the network on the 10000 test images: 99.03 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.9250 \n",
      "Accuracy of the network on the 10000 test images: 99.05 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 99.9433 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9517 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9617 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9600 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9717 \n",
      "Accuracy of the network on the 10000 test images: 99.1 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9800 \n",
      "Accuracy of the network on the 10000 test images: 99.05 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.02 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7690e4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:30:51.135331Z",
     "start_time": "2022-04-08T13:30:51.121369Z"
    },
    "id": "7690e4e6"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net6 - training loss'] = running_loss_history\n",
    "results['net6 - training accuracy'] = running_corrects_history\n",
    "results['net6 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f3045",
   "metadata": {
    "id": "9b0f3045"
   },
   "source": [
    "## 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87e77401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:30:51.212127Z",
     "start_time": "2022-04-08T13:30:51.136329Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87e77401",
    "outputId": "fc77a4f1-0157-476c-e997-26d2e7e6f28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d796263a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:30:51.227088Z",
     "start_time": "2022-04-08T13:30:51.213125Z"
    },
    "id": "d796263a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83689cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:35:16.731625Z",
     "start_time": "2022-04-08T13:30:51.229082Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "83689cd0",
    "outputId": "fcb64137-4911-444d-cbee-9654ab797c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0030, acc 90.5750 \n",
      "Accuracy of the network on the 10000 test images: 97.77 %\n",
      "epoch : 2\n",
      "training loss: 0.0005, acc 98.0800 \n",
      "Accuracy of the network on the 10000 test images: 98.49 %\n",
      "epoch : 3\n",
      "training loss: 0.0004, acc 98.6900 \n",
      "Accuracy of the network on the 10000 test images: 98.74 %\n",
      "epoch : 4\n",
      "training loss: 0.0003, acc 98.9083 \n",
      "Accuracy of the network on the 10000 test images: 98.83 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.1667 \n",
      "Accuracy of the network on the 10000 test images: 99.0 %\n",
      "epoch : 6\n",
      "training loss: 0.0002, acc 99.2700 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 7\n",
      "training loss: 0.0002, acc 99.4000 \n",
      "Accuracy of the network on the 10000 test images: 99.0 %\n",
      "epoch : 8\n",
      "training loss: 0.0002, acc 99.4950 \n",
      "Accuracy of the network on the 10000 test images: 99.0 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.5683 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.6217 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.6983 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 12\n",
      "training loss: 0.0001, acc 99.7600 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.7783 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 99.8100 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 99.8467 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.8783 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.9000 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.9100 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9367 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9517 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9617 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9700 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9733 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9800 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9783 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.2 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.2 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "374a25a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:35:16.747583Z",
     "start_time": "2022-04-08T13:35:16.732623Z"
    },
    "id": "374a25a4"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net7 - training loss'] = running_loss_history\n",
    "results['net7 - training accuracy'] = running_corrects_history\n",
    "results['net7 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae4266",
   "metadata": {
    "id": "6bae4266"
   },
   "source": [
    "## 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "540075c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:35:16.888179Z",
     "start_time": "2022-04-08T13:35:16.748581Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "540075c0",
    "outputId": "2772efc3-be11-4cf7-cb83-068e6c032829"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "455d912a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:35:16.904169Z",
     "start_time": "2022-04-08T13:35:16.889176Z"
    },
    "id": "455d912a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac294781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:41:47.792007Z",
     "start_time": "2022-04-08T13:35:16.905166Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ac294781",
    "outputId": "4c87e299-d0e4-497f-a361-9f8e058e7f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0024, acc 92.5867 \n",
      "Accuracy of the network on the 10000 test images: 98.35 %\n",
      "epoch : 2\n",
      "training loss: 0.0004, acc 98.5567 \n",
      "Accuracy of the network on the 10000 test images: 98.72 %\n",
      "epoch : 3\n",
      "training loss: 0.0003, acc 99.0033 \n",
      "Accuracy of the network on the 10000 test images: 98.94 %\n",
      "epoch : 4\n",
      "training loss: 0.0002, acc 99.2967 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.4900 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.6050 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.7017 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.7850 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.8433 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.8883 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9167 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9467 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9700 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9717 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6464f30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:41:47.806967Z",
     "start_time": "2022-04-08T13:41:47.793004Z"
    },
    "id": "6464f30e"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net8 - training loss'] = running_loss_history\n",
    "results['net8 - training accuracy'] = running_corrects_history\n",
    "results['net8 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a172b",
   "metadata": {
    "id": "7e7a172b"
   },
   "source": [
    "## 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f5c4e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:41:48.057298Z",
     "start_time": "2022-04-08T13:41:47.807964Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f5c4e3c",
    "outputId": "108d26c7-461c-4707-94ff-313b8d9e4efb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c92a7e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:41:48.073255Z",
     "start_time": "2022-04-08T13:41:48.058295Z"
    },
    "id": "c92a7e42"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff46de4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:52:19.885663Z",
     "start_time": "2022-04-08T13:41:48.074253Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ff46de4f",
    "outputId": "a990e219-5279-4aff-8aa3-58b0a6e62cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0019, acc 93.4833 \n",
      "Accuracy of the network on the 10000 test images: 98.48 %\n",
      "epoch : 2\n",
      "training loss: 0.0003, acc 98.7983 \n",
      "Accuracy of the network on the 10000 test images: 99.06 %\n",
      "epoch : 3\n",
      "training loss: 0.0002, acc 99.2500 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.4650 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 5\n",
      "training loss: 0.0001, acc 99.6717 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.7967 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.8617 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9317 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9683 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c435663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:52:19.901621Z",
     "start_time": "2022-04-08T13:52:19.886661Z"
    },
    "id": "2c435663"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net9 - training loss'] = running_loss_history\n",
    "results['net9 - training accuracy'] = running_corrects_history\n",
    "results['net9 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68936a",
   "metadata": {
    "id": "ba68936a"
   },
   "source": [
    "## 10 Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "556c41ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:52:20.402282Z",
     "start_time": "2022-04-08T13:52:19.902618Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "556c41ef",
    "outputId": "f3f35195-c6c5-4d54-b7ab-5ad8b80167fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83171665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:52:20.417243Z",
     "start_time": "2022-04-08T13:52:20.403279Z"
    },
    "id": "83171665"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89903fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:13:48.321842Z",
     "start_time": "2022-04-08T13:52:20.418239Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "89903fe7",
    "outputId": "70103797-e4dd-4d23-f136-ce444c68d2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0016, acc 94.6050 \n",
      "Accuracy of the network on the 10000 test images: 98.86 %\n",
      "epoch : 2\n",
      "training loss: 0.0002, acc 99.1383 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 3\n",
      "training loss: 0.0001, acc 99.5383 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.7917 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 5\n",
      "training loss: 0.0000, acc 99.9217 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 6\n",
      "training loss: 0.0000, acc 99.9650 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.33 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f424c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:13:48.337799Z",
     "start_time": "2022-04-08T14:13:48.322839Z"
    },
    "id": "0f424c18"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net10 - training loss'] = running_loss_history\n",
    "results['net10 - training accuracy'] = running_corrects_history\n",
    "results['net10 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aebd6f",
   "metadata": {
    "id": "12aebd6f"
   },
   "source": [
    "# ------------------- Added Convolutional Layer per Block -------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6cfad",
   "metadata": {
    "id": "bad6cfad"
   },
   "source": [
    "## 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68043eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:13:48.385699Z",
     "start_time": "2022-04-08T14:13:48.338796Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68043eed",
    "outputId": "f5cac548-581b-4361-98dc-a6ae91b413d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b1291c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:13:48.401628Z",
     "start_time": "2022-04-08T14:13:48.386701Z"
    },
    "id": "6b1291c6"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbea4d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:17:45.681937Z",
     "start_time": "2022-04-08T14:13:48.402626Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "cbea4d34",
    "outputId": "53502d48-6e2f-425c-f3c7-5474739e1aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0040, acc 89.1467 \n",
      "Accuracy of the network on the 10000 test images: 96.88 %\n",
      "epoch : 2\n",
      "training loss: 0.0008, acc 97.3683 \n",
      "Accuracy of the network on the 10000 test images: 98.06 %\n",
      "epoch : 3\n",
      "training loss: 0.0005, acc 98.2217 \n",
      "Accuracy of the network on the 10000 test images: 98.51 %\n",
      "epoch : 4\n",
      "training loss: 0.0004, acc 98.5733 \n",
      "Accuracy of the network on the 10000 test images: 98.7 %\n",
      "epoch : 5\n",
      "training loss: 0.0003, acc 98.8083 \n",
      "Accuracy of the network on the 10000 test images: 98.79 %\n",
      "epoch : 6\n",
      "training loss: 0.0003, acc 98.9967 \n",
      "Accuracy of the network on the 10000 test images: 98.9 %\n",
      "epoch : 7\n",
      "training loss: 0.0003, acc 99.1450 \n",
      "Accuracy of the network on the 10000 test images: 98.86 %\n",
      "epoch : 8\n",
      "training loss: 0.0002, acc 99.2200 \n",
      "Accuracy of the network on the 10000 test images: 98.96 %\n",
      "epoch : 9\n",
      "training loss: 0.0002, acc 99.2683 \n",
      "Accuracy of the network on the 10000 test images: 98.96 %\n",
      "epoch : 10\n",
      "training loss: 0.0002, acc 99.3867 \n",
      "Accuracy of the network on the 10000 test images: 99.04 %\n",
      "epoch : 11\n",
      "training loss: 0.0002, acc 99.4417 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 12\n",
      "training loss: 0.0002, acc 99.5033 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.5283 \n",
      "Accuracy of the network on the 10000 test images: 99.12 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 99.5933 \n",
      "Accuracy of the network on the 10000 test images: 99.07 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 99.6200 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.6883 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 17\n",
      "training loss: 0.0001, acc 99.7133 \n",
      "Accuracy of the network on the 10000 test images: 99.15 %\n",
      "epoch : 18\n",
      "training loss: 0.0001, acc 99.7350 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 19\n",
      "training loss: 0.0001, acc 99.7500 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 20\n",
      "training loss: 0.0001, acc 99.7667 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 99.8050 \n",
      "Accuracy of the network on the 10000 test images: 99.11 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 99.8333 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 99.8417 \n",
      "Accuracy of the network on the 10000 test images: 99.21 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.8667 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 99.8833 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 26\n",
      "training loss: 0.0001, acc 99.8917 \n",
      "Accuracy of the network on the 10000 test images: 99.27 %\n",
      "epoch : 27\n",
      "training loss: 0.0001, acc 99.9017 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 99.9200 \n",
      "Accuracy of the network on the 10000 test images: 99.2 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.9183 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 99.9217 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 99.9317 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9367 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9500 \n",
      "Accuracy of the network on the 10000 test images: 99.16 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9550 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9533 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9617 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9650 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9767 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9717 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9750 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 99.9817 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.26 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 99.9900 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.23 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7b0fbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:17:45.696897Z",
     "start_time": "2022-04-08T14:17:45.682935Z"
    },
    "id": "b7b0fbf5"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net11 - training loss'] = running_loss_history\n",
    "results['net11 - training accuracy'] = running_corrects_history\n",
    "results['net11 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f7b2",
   "metadata": {
    "id": "26e6f7b2"
   },
   "source": [
    "## 4 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4d236ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:17:45.775687Z",
     "start_time": "2022-04-08T14:17:45.697895Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4d236ce",
    "outputId": "e7195b89-739b-44c6-d20d-816c53714e24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "041336eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:17:45.790647Z",
     "start_time": "2022-04-08T14:17:45.776684Z"
    },
    "id": "041336eb"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7c79bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:22:46.931032Z",
     "start_time": "2022-04-08T14:17:45.791644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "d7c79bd2",
    "outputId": "03fb6389-d464-42a3-e4aa-1fe9d38c242e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0029, acc 91.6683 \n",
      "Accuracy of the network on the 10000 test images: 98.31 %\n",
      "epoch : 2\n",
      "training loss: 0.0005, acc 98.2750 \n",
      "Accuracy of the network on the 10000 test images: 98.85 %\n",
      "epoch : 3\n",
      "training loss: 0.0003, acc 98.8400 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 4\n",
      "training loss: 0.0003, acc 99.0200 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.2783 \n",
      "Accuracy of the network on the 10000 test images: 99.08 %\n",
      "epoch : 6\n",
      "training loss: 0.0002, acc 99.2917 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 7\n",
      "training loss: 0.0002, acc 99.4350 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.5067 \n",
      "Accuracy of the network on the 10000 test images: 99.22 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.5983 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.6717 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 11\n",
      "training loss: 0.0001, acc 99.6867 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 12\n",
      "training loss: 0.0001, acc 99.7717 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 13\n",
      "training loss: 0.0001, acc 99.8050 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 14\n",
      "training loss: 0.0001, acc 99.8083 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 15\n",
      "training loss: 0.0001, acc 99.8600 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 16\n",
      "training loss: 0.0001, acc 99.8817 \n",
      "Accuracy of the network on the 10000 test images: 99.28 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.8917 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9300 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9367 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9650 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9717 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9833 \n",
      "Accuracy of the network on the 10000 test images: 99.38 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b859642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:22:46.945992Z",
     "start_time": "2022-04-08T14:22:46.932030Z"
    },
    "id": "8b859642"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net12 - training loss'] = running_loss_history\n",
    "results['net12 - training accuracy'] = running_corrects_history\n",
    "results['net12 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2b81f",
   "metadata": {
    "id": "7ac2b81f"
   },
   "source": [
    "## 6 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "435c8114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:22:47.087613Z",
     "start_time": "2022-04-08T14:22:46.946989Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "435c8114",
    "outputId": "0ae7f8d0-8d2f-4248-9868-41af9d6b5f39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Flatten(start_dim=1, end_dim=-1)\n",
       "    (23): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (24): ReLU()\n",
       "    (25): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "363ac576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:22:47.103570Z",
     "start_time": "2022-04-08T14:22:47.088611Z"
    },
    "id": "363ac576"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f58b454d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:30:10.306803Z",
     "start_time": "2022-04-08T14:22:47.104569Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "f58b454d",
    "outputId": "4d057841-a49a-4bab-b210-a79b876f4918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0021, acc 93.5967 \n",
      "Accuracy of the network on the 10000 test images: 98.6 %\n",
      "epoch : 2\n",
      "training loss: 0.0004, acc 98.6967 \n",
      "Accuracy of the network on the 10000 test images: 98.93 %\n",
      "epoch : 3\n",
      "training loss: 0.0003, acc 99.0733 \n",
      "Accuracy of the network on the 10000 test images: 99.09 %\n",
      "epoch : 4\n",
      "training loss: 0.0002, acc 99.2783 \n",
      "Accuracy of the network on the 10000 test images: 99.14 %\n",
      "epoch : 5\n",
      "training loss: 0.0002, acc 99.4433 \n",
      "Accuracy of the network on the 10000 test images: 99.18 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.5217 \n",
      "Accuracy of the network on the 10000 test images: 99.17 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.6683 \n",
      "Accuracy of the network on the 10000 test images: 99.3 %\n",
      "epoch : 8\n",
      "training loss: 0.0001, acc 99.7350 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 9\n",
      "training loss: 0.0001, acc 99.7933 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 10\n",
      "training loss: 0.0001, acc 99.8817 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9167 \n",
      "Accuracy of the network on the 10000 test images: 99.32 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9600 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9767 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9733 \n",
      "Accuracy of the network on the 10000 test images: 99.37 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9850 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9917 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9967 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.51 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1dcb6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:30:10.322735Z",
     "start_time": "2022-04-08T14:30:10.307775Z"
    },
    "id": "f1dcb6c8"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net13 - training loss'] = running_loss_history\n",
    "results['net13 - training accuracy'] = running_corrects_history\n",
    "results['net13 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04074",
   "metadata": {
    "id": "5eb04074"
   },
   "source": [
    "## 8 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a407a44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:30:10.572069Z",
     "start_time": "2022-04-08T14:30:10.323732Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a407a44a",
    "outputId": "07d587ec-a4db-4be4-986e-bdbb5f5f9143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Flatten(start_dim=1, end_dim=-1)\n",
       "    (30): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (31): ReLU()\n",
       "    (32): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9da2c848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:30:10.588041Z",
     "start_time": "2022-04-08T14:30:10.573066Z"
    },
    "id": "9da2c848"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a06bb541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:42:28.543134Z",
     "start_time": "2022-04-08T14:30:10.589038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "a06bb541",
    "outputId": "9c621f3f-fe33-4bb5-c401-05dff8fd77d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0020, acc 93.2017 \n",
      "Accuracy of the network on the 10000 test images: 98.67 %\n",
      "epoch : 2\n",
      "training loss: 0.0003, acc 98.8700 \n",
      "Accuracy of the network on the 10000 test images: 99.13 %\n",
      "epoch : 3\n",
      "training loss: 0.0002, acc 99.2267 \n",
      "Accuracy of the network on the 10000 test images: 99.25 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.4867 \n",
      "Accuracy of the network on the 10000 test images: 99.19 %\n",
      "epoch : 5\n",
      "training loss: 0.0001, acc 99.6533 \n",
      "Accuracy of the network on the 10000 test images: 99.35 %\n",
      "epoch : 6\n",
      "training loss: 0.0001, acc 99.7333 \n",
      "Accuracy of the network on the 10000 test images: 99.36 %\n",
      "epoch : 7\n",
      "training loss: 0.0001, acc 99.8333 \n",
      "Accuracy of the network on the 10000 test images: 99.24 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9033 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9633 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9567 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9883 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9933 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9950 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.51 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.5 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.48 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.49 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.52 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36d9cd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:42:28.559092Z",
     "start_time": "2022-04-08T14:42:28.544132Z"
    },
    "id": "36d9cd53"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net14 - training loss'] = running_loss_history\n",
    "results['net14 - training accuracy'] = running_corrects_history\n",
    "results['net14 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa0b44",
   "metadata": {
    "id": "11fa0b44"
   },
   "source": [
    "## 10 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd4f4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:47:23.884820Z",
     "start_time": "2022-04-08T14:47:23.321278Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cd4f4dc",
    "outputId": "45f6c3f4-255f-4a5c-cd9b-44716ff5270e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (30): ReLU()\n",
       "    (31): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (32): ReLU()\n",
       "    (33): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): ReLU()\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): Flatten(start_dim=1, end_dim=-1)\n",
       "    (37): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (38): ReLU()\n",
       "    (39): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (40): ReLU()\n",
       "    (41): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e42c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T14:47:24.134816Z",
     "start_time": "2022-04-08T14:47:24.119857Z"
    },
    "id": "22e42c81"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c8aba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T15:12:34.213839Z",
     "start_time": "2022-04-08T14:47:36.530425Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "d0c8aba4",
    "outputId": "798dfe3b-b656-4431-8740-d9511f7d2867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0017, acc 93.5667 \n",
      "Accuracy of the network on the 10000 test images: 98.99 %\n",
      "epoch : 2\n",
      "training loss: 0.0003, acc 99.0183 \n",
      "Accuracy of the network on the 10000 test images: 99.15 %\n",
      "epoch : 3\n",
      "training loss: 0.0001, acc 99.4583 \n",
      "Accuracy of the network on the 10000 test images: 99.29 %\n",
      "epoch : 4\n",
      "training loss: 0.0001, acc 99.7117 \n",
      "Accuracy of the network on the 10000 test images: 98.98 %\n",
      "epoch : 5\n",
      "training loss: 0.0001, acc 99.8317 \n",
      "Accuracy of the network on the 10000 test images: 99.4 %\n",
      "epoch : 6\n",
      "training loss: 0.0000, acc 99.9450 \n",
      "Accuracy of the network on the 10000 test images: 99.31 %\n",
      "epoch : 7\n",
      "training loss: 0.0000, acc 99.9733 \n",
      "Accuracy of the network on the 10000 test images: 99.34 %\n",
      "epoch : 8\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.39 %\n",
      "epoch : 9\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 10\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 11\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 12\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 13\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 14\n",
      "training loss: 0.0000, acc 99.9983 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 15\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 16\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.47 %\n",
      "epoch : 17\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 18\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 19\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 20\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 21\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.41 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.43 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.42 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.45 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.46 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 99.44 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea4134a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T15:12:34.229766Z",
     "start_time": "2022-04-08T15:12:34.214805Z"
    },
    "id": "6ea4134a"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net15 - training loss'] = running_loss_history\n",
    "results['net15 - training accuracy'] = running_corrects_history\n",
    "results['net15 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_mnist.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "net_builder_FashionMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c8ae6c23824f0a8482a2b96df532fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b24b28503dc40c4a0733f9d33725c6b",
       "IPY_MODEL_1183a64f165d418e8b31d5cb7bdb4e78",
       "IPY_MODEL_a8f28237e19c40b5b76178bb04dcf21f"
      ],
      "layout": "IPY_MODEL_c298f35420314a29b61191720be89f53"
     }
    },
    "081af12a75184c6091a274d028182ca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fd7f50eaedd42c4902974456998b45e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1183a64f165d418e8b31d5cb7bdb4e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_673cfc01d9504685922bb03716d2d7bc",
      "max": 4422102,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22edd38b97374ecb888b7cbef6c4c130",
      "value": 4422102
     }
    },
    "12a60a5ab9a14d70810abc2a1099f32e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c63e3bda2df4c9b8c410e28fbb9ca1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d342f4d9282493aa040abfb8056e2a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "200201f06282443aac2304647e1be222": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22edd38b97374ecb888b7cbef6c4c130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23f51ee3583343b2ab490180cd3a7d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d342f4d9282493aa040abfb8056e2a7",
      "placeholder": "​",
      "style": "IPY_MODEL_8dd8b65a5ab94a6bb369dda20c307865",
      "value": ""
     }
    },
    "257c41c0442e4a648789ec978a8c0af4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26b1e7315e144eb9b74fbbfe7f2830d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bdba06d7623412da0a4cd0e26e6ff2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99b062f811b44176a910e48981ed191d",
      "placeholder": "​",
      "style": "IPY_MODEL_74129c62b3124becb14b8172c4e7b864",
      "value": ""
     }
    },
    "321f6569581949eda9fb739ab2fc5efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bdba06d7623412da0a4cd0e26e6ff2e",
       "IPY_MODEL_50ac188b44914b7a993937f82301afea",
       "IPY_MODEL_ce500c3b7fec466184d9d847c5e0de5a"
      ],
      "layout": "IPY_MODEL_200201f06282443aac2304647e1be222"
     }
    },
    "346d3e11a8cc429780c2020fe2f1b1a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3bafe01580514d84bb220735861ab9fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "497df2fee2d64396bff5b1a9e0af5045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12a60a5ab9a14d70810abc2a1099f32e",
      "max": 26421880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_081af12a75184c6091a274d028182ca9",
      "value": 26421880
     }
    },
    "50ac188b44914b7a993937f82301afea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_effc835a317841989b2eafe0a0e9ab72",
      "max": 29515,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2de304d953c47438bc4cd1ee43c7b65",
      "value": 29515
     }
    },
    "673cfc01d9504685922bb03716d2d7bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74129c62b3124becb14b8172c4e7b864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77e8dca5c85a4986a8131cd53c79d189": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6d50fb183e34e42b58a7f24ba68d428",
      "max": 5148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3bafe01580514d84bb220735861ab9fe",
      "value": 5148
     }
    },
    "8dd8b65a5ab94a6bb369dda20c307865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e4528665c2f4af3915011d342b2d08d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce7cc300cfa44dc2b3b9bcf4e90e34a9",
      "placeholder": "​",
      "style": "IPY_MODEL_1c63e3bda2df4c9b8c410e28fbb9ca1b",
      "value": ""
     }
    },
    "95870391b50046c68a63b82a518dbafe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23f51ee3583343b2ab490180cd3a7d27",
       "IPY_MODEL_77e8dca5c85a4986a8131cd53c79d189",
       "IPY_MODEL_c0c3c3e3866748879752d3ef50228a73"
      ],
      "layout": "IPY_MODEL_bd68c9d150734460b97718412420afa9"
     }
    },
    "99b062f811b44176a910e48981ed191d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b24b28503dc40c4a0733f9d33725c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e7281b04d4b4087bbaaae59c35a9092",
      "placeholder": "​",
      "style": "IPY_MODEL_fd119b2e3a6049e6b788d3cf5908d5f6",
      "value": ""
     }
    },
    "9e7281b04d4b4087bbaaae59c35a9092": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6d50fb183e34e42b58a7f24ba68d428": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8f28237e19c40b5b76178bb04dcf21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9e94b9355534bb78aaab90ec23ef81d",
      "placeholder": "​",
      "style": "IPY_MODEL_26b1e7315e144eb9b74fbbfe7f2830d9",
      "value": " 4422656/? [00:00&lt;00:00, 5520316.62it/s]"
     }
    },
    "b54b372479cc4660978b192891cee13f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd68c9d150734460b97718412420afa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0c3c3e3866748879752d3ef50228a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c86dd7860f6d4ba298c369c552814c55",
      "placeholder": "​",
      "style": "IPY_MODEL_346d3e11a8cc429780c2020fe2f1b1a8",
      "value": " 6144/? [00:00&lt;00:00, 70133.56it/s]"
     }
    },
    "c298f35420314a29b61191720be89f53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c80c5844880c49edab730d65c19e1712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c86dd7860f6d4ba298c369c552814c55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e94b9355534bb78aaab90ec23ef81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce500c3b7fec466184d9d847c5e0de5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fd7f50eaedd42c4902974456998b45e",
      "placeholder": "​",
      "style": "IPY_MODEL_b54b372479cc4660978b192891cee13f",
      "value": " 29696/? [00:00&lt;00:00, 335629.31it/s]"
     }
    },
    "ce7cc300cfa44dc2b3b9bcf4e90e34a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2de304d953c47438bc4cd1ee43c7b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eae018f67c894b60806bd13d8130ff85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e4528665c2f4af3915011d342b2d08d",
       "IPY_MODEL_497df2fee2d64396bff5b1a9e0af5045",
       "IPY_MODEL_ec84850f19bb4027bfcb827ab85acc2b"
      ],
      "layout": "IPY_MODEL_fd53777ba1ad486d9bac399bbdaff141"
     }
    },
    "ec84850f19bb4027bfcb827ab85acc2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_257c41c0442e4a648789ec978a8c0af4",
      "placeholder": "​",
      "style": "IPY_MODEL_c80c5844880c49edab730d65c19e1712",
      "value": " 26422272/? [00:01&lt;00:00, 28569721.22it/s]"
     }
    },
    "effc835a317841989b2eafe0a0e9ab72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd119b2e3a6049e6b788d3cf5908d5f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd53777ba1ad486d9bac399bbdaff141": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

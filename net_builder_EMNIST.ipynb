{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43ebd09",
   "metadata": {
    "id": "a43ebd09"
   },
   "source": [
    "# Package and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d9c10",
   "metadata": {
    "id": "e00d9c10"
   },
   "source": [
    "Inspiration:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "ResNet architecture - split convolutional layers into blocks based on size of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24cc331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T06:44:54.584919Z",
     "start_time": "2022-04-08T06:44:52.358696Z"
    },
    "id": "a24cc331"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e92335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T06:44:55.815290Z",
     "start_time": "2022-04-08T06:44:55.693977Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89e92335",
    "outputId": "7af0a3c6-8bf5-472e-9761-3b3f0743e68e"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(( 0.5), ( 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root='./data', train=True,\n",
    "                                        download=True, split='balanced',transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.EMNIST(root='./data', train=False,\n",
    "                                       download=True,split='balanced', transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "iZPEV1Uueojx",
   "metadata": {
    "id": "iZPEV1Uueojx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cbc2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T06:44:58.085679Z",
     "start_time": "2022-04-08T06:44:58.077668Z"
    },
    "id": "83cbc2f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f928c9",
   "metadata": {
    "id": "76f928c9"
   },
   "source": [
    "# Base CNN (2 Layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8d82af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:35:04.868280Z",
     "start_time": "2022-04-07T20:35:04.730210Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e8d82af",
    "outputId": "0deed006-5052-4781-9435-7a8aada4c479",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1 #### change to 64?\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17012585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:35:27.319165Z",
     "start_time": "2022-04-07T20:35:27.307198Z"
    },
    "id": "17012585"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efeb67e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:42:32.010501Z",
     "start_time": "2022-04-07T20:35:28.098117Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "efeb67e3",
    "outputId": "796736ae-b99a-4f7c-e79b-e0839c1540b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0102, acc 66.4929 \n",
      "Accuracy of the network on the 10000 test images: 80.77659574468085 %\n",
      "epoch : 2\n",
      "training loss: 0.0040, acc 83.5284 \n",
      "Accuracy of the network on the 10000 test images: 83.78191489361703 %\n",
      "epoch : 3\n",
      "training loss: 0.0034, acc 85.7890 \n",
      "Accuracy of the network on the 10000 test images: 84.94148936170212 %\n",
      "epoch : 4\n",
      "training loss: 0.0030, acc 86.9761 \n",
      "Accuracy of the network on the 10000 test images: 85.68085106382979 %\n",
      "epoch : 5\n",
      "training loss: 0.0028, acc 87.8590 \n",
      "Accuracy of the network on the 10000 test images: 85.86702127659575 %\n",
      "epoch : 6\n",
      "training loss: 0.0026, acc 88.6082 \n",
      "Accuracy of the network on the 10000 test images: 86.76063829787235 %\n",
      "epoch : 7\n",
      "training loss: 0.0024, acc 89.1950 \n",
      "Accuracy of the network on the 10000 test images: 87.1436170212766 %\n",
      "epoch : 8\n",
      "training loss: 0.0023, acc 89.7145 \n",
      "Accuracy of the network on the 10000 test images: 87.42553191489361 %\n",
      "epoch : 9\n",
      "training loss: 0.0022, acc 90.2314 \n",
      "Accuracy of the network on the 10000 test images: 87.67021276595744 %\n",
      "epoch : 10\n",
      "training loss: 0.0021, acc 90.6995 \n",
      "Accuracy of the network on the 10000 test images: 87.91489361702128 %\n",
      "epoch : 11\n",
      "training loss: 0.0020, acc 91.1764 \n",
      "Accuracy of the network on the 10000 test images: 87.80851063829788 %\n",
      "epoch : 12\n",
      "training loss: 0.0019, acc 91.5355 \n",
      "Accuracy of the network on the 10000 test images: 88.12765957446808 %\n",
      "epoch : 13\n",
      "training loss: 0.0018, acc 91.9672 \n",
      "Accuracy of the network on the 10000 test images: 87.89893617021276 %\n",
      "epoch : 14\n",
      "training loss: 0.0017, acc 92.3236 \n",
      "Accuracy of the network on the 10000 test images: 87.9627659574468 %\n",
      "epoch : 15\n",
      "training loss: 0.0016, acc 92.8617 \n",
      "Accuracy of the network on the 10000 test images: 87.90957446808511 %\n",
      "epoch : 16\n",
      "training loss: 0.0015, acc 93.1817 \n",
      "Accuracy of the network on the 10000 test images: 87.68085106382979 %\n",
      "epoch : 17\n",
      "training loss: 0.0015, acc 93.5541 \n",
      "Accuracy of the network on the 10000 test images: 88.13829787234043 %\n",
      "epoch : 18\n",
      "training loss: 0.0014, acc 93.9149 \n",
      "Accuracy of the network on the 10000 test images: 88.07446808510639 %\n",
      "epoch : 19\n",
      "training loss: 0.0013, acc 94.2367 \n",
      "Accuracy of the network on the 10000 test images: 87.75 %\n",
      "epoch : 20\n",
      "training loss: 0.0012, acc 94.6525 \n",
      "Accuracy of the network on the 10000 test images: 87.77127659574468 %\n",
      "epoch : 21\n",
      "training loss: 0.0012, acc 94.9477 \n",
      "Accuracy of the network on the 10000 test images: 87.83510638297872 %\n",
      "epoch : 22\n",
      "training loss: 0.0011, acc 95.2544 \n",
      "Accuracy of the network on the 10000 test images: 87.92553191489361 %\n",
      "epoch : 23\n",
      "training loss: 0.0011, acc 95.5204 \n",
      "Accuracy of the network on the 10000 test images: 87.8563829787234 %\n",
      "epoch : 24\n",
      "training loss: 0.0010, acc 95.8981 \n",
      "Accuracy of the network on the 10000 test images: 87.76063829787235 %\n",
      "epoch : 25\n",
      "training loss: 0.0009, acc 96.1303 \n",
      "Accuracy of the network on the 10000 test images: 87.87765957446808 %\n",
      "epoch : 26\n",
      "training loss: 0.0009, acc 96.4087 \n",
      "Accuracy of the network on the 10000 test images: 87.55851063829788 %\n",
      "epoch : 27\n",
      "training loss: 0.0008, acc 96.6684 \n",
      "Accuracy of the network on the 10000 test images: 87.44680851063829 %\n",
      "epoch : 28\n",
      "training loss: 0.0008, acc 96.8599 \n",
      "Accuracy of the network on the 10000 test images: 87.72340425531915 %\n",
      "epoch : 29\n",
      "training loss: 0.0008, acc 97.1365 \n",
      "Accuracy of the network on the 10000 test images: 87.73936170212765 %\n",
      "epoch : 30\n",
      "training loss: 0.0007, acc 97.3493 \n",
      "Accuracy of the network on the 10000 test images: 87.46808510638297 %\n",
      "epoch : 31\n",
      "training loss: 0.0007, acc 97.4876 \n",
      "Accuracy of the network on the 10000 test images: 87.18085106382979 %\n",
      "epoch : 32\n",
      "training loss: 0.0006, acc 97.6915 \n",
      "Accuracy of the network on the 10000 test images: 87.41489361702128 %\n",
      "epoch : 33\n",
      "training loss: 0.0006, acc 97.8316 \n",
      "Accuracy of the network on the 10000 test images: 87.71808510638297 %\n",
      "epoch : 34\n",
      "training loss: 0.0006, acc 97.9415 \n",
      "Accuracy of the network on the 10000 test images: 87.43085106382979 %\n",
      "epoch : 35\n",
      "training loss: 0.0005, acc 98.0709 \n",
      "Accuracy of the network on the 10000 test images: 87.40425531914893 %\n",
      "epoch : 36\n",
      "training loss: 0.0005, acc 98.2234 \n",
      "Accuracy of the network on the 10000 test images: 87.43085106382979 %\n",
      "epoch : 37\n",
      "training loss: 0.0005, acc 98.3103 \n",
      "Accuracy of the network on the 10000 test images: 87.31914893617021 %\n",
      "epoch : 38\n",
      "training loss: 0.0005, acc 98.4016 \n",
      "Accuracy of the network on the 10000 test images: 87.47872340425532 %\n",
      "epoch : 39\n",
      "training loss: 0.0005, acc 98.4264 \n",
      "Accuracy of the network on the 10000 test images: 87.36702127659575 %\n",
      "epoch : 40\n",
      "training loss: 0.0004, acc 98.5204 \n",
      "Accuracy of the network on the 10000 test images: 87.29787234042553 %\n",
      "epoch : 41\n",
      "training loss: 0.0004, acc 98.6268 \n",
      "Accuracy of the network on the 10000 test images: 87.40425531914893 %\n",
      "epoch : 42\n",
      "training loss: 0.0004, acc 98.6312 \n",
      "Accuracy of the network on the 10000 test images: 87.42021276595744 %\n",
      "epoch : 43\n",
      "training loss: 0.0004, acc 98.7429 \n",
      "Accuracy of the network on the 10000 test images: 87.24468085106383 %\n",
      "epoch : 44\n",
      "training loss: 0.0004, acc 98.7731 \n",
      "Accuracy of the network on the 10000 test images: 87.18085106382979 %\n",
      "epoch : 45\n",
      "training loss: 0.0004, acc 98.8777 \n",
      "Accuracy of the network on the 10000 test images: 87.35106382978724 %\n",
      "epoch : 46\n",
      "training loss: 0.0004, acc 98.8883 \n",
      "Accuracy of the network on the 10000 test images: 87.25 %\n",
      "epoch : 47\n",
      "training loss: 0.0004, acc 98.9105 \n",
      "Accuracy of the network on the 10000 test images: 87.43085106382979 %\n",
      "epoch : 48\n",
      "training loss: 0.0003, acc 98.9796 \n",
      "Accuracy of the network on the 10000 test images: 87.31914893617021 %\n",
      "epoch : 49\n",
      "training loss: 0.0003, acc 99.0337 \n",
      "Accuracy of the network on the 10000 test images: 87.25531914893617 %\n",
      "epoch : 50\n",
      "training loss: 0.0003, acc 99.0736 \n",
      "Accuracy of the network on the 10000 test images: 87.22340425531915 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592ed8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:43:45.076970Z",
     "start_time": "2022-04-07T20:43:45.051039Z"
    },
    "id": "592ed8e7"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net1 - training loss'] = running_loss_history\n",
    "results['net1 - training accuracy'] = running_corrects_history\n",
    "results['net1 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7e83c",
   "metadata": {
    "id": "59b7e83c"
   },
   "source": [
    "# 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41145ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:48:38.146458Z",
     "start_time": "2022-04-07T20:48:38.022791Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41145ab7",
    "outputId": "c09a0404-6d58-4999-959f-4f3fc99440b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "330b22e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:48:38.162417Z",
     "start_time": "2022-04-07T20:48:38.147456Z"
    },
    "id": "330b22e1"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b26a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:58:49.655131Z",
     "start_time": "2022-04-07T20:48:38.163385Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "23b26a0e",
    "outputId": "efb83f6b-6509-4c48-ba3e-7d7bbfea68f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0076, acc 74.3741 \n",
      "Accuracy of the network on the 10000 test images: 83.14893617021276 %\n",
      "epoch : 2\n",
      "training loss: 0.0032, acc 86.3014 \n",
      "Accuracy of the network on the 10000 test images: 85.90425531914893 %\n",
      "epoch : 3\n",
      "training loss: 0.0027, acc 88.0771 \n",
      "Accuracy of the network on the 10000 test images: 87.33510638297872 %\n",
      "epoch : 4\n",
      "training loss: 0.0024, acc 89.0186 \n",
      "Accuracy of the network on the 10000 test images: 87.92553191489361 %\n",
      "epoch : 5\n",
      "training loss: 0.0022, acc 89.7810 \n",
      "Accuracy of the network on the 10000 test images: 87.45212765957447 %\n",
      "epoch : 6\n",
      "training loss: 0.0021, acc 90.4885 \n",
      "Accuracy of the network on the 10000 test images: 88.63829787234043 %\n",
      "epoch : 7\n",
      "training loss: 0.0020, acc 90.9920 \n",
      "Accuracy of the network on the 10000 test images: 88.7872340425532 %\n",
      "epoch : 8\n",
      "training loss: 0.0018, acc 91.4832 \n",
      "Accuracy of the network on the 10000 test images: 88.55851063829788 %\n",
      "epoch : 9\n",
      "training loss: 0.0017, acc 92.0816 \n",
      "Accuracy of the network on the 10000 test images: 88.82978723404256 %\n",
      "epoch : 10\n",
      "training loss: 0.0016, acc 92.5106 \n",
      "Accuracy of the network on the 10000 test images: 88.75531914893617 %\n",
      "epoch : 11\n",
      "training loss: 0.0015, acc 93.0825 \n",
      "Accuracy of the network on the 10000 test images: 88.93617021276596 %\n",
      "epoch : 12\n",
      "training loss: 0.0014, acc 93.4043 \n",
      "Accuracy of the network on the 10000 test images: 88.76063829787235 %\n",
      "epoch : 13\n",
      "training loss: 0.0013, acc 93.8723 \n",
      "Accuracy of the network on the 10000 test images: 88.88829787234043 %\n",
      "epoch : 14\n",
      "training loss: 0.0012, acc 94.3688 \n",
      "Accuracy of the network on the 10000 test images: 88.97340425531915 %\n",
      "epoch : 15\n",
      "training loss: 0.0012, acc 94.7518 \n",
      "Accuracy of the network on the 10000 test images: 88.57446808510639 %\n",
      "epoch : 16\n",
      "training loss: 0.0011, acc 95.2500 \n",
      "Accuracy of the network on the 10000 test images: 88.79255319148936 %\n",
      "epoch : 17\n",
      "training loss: 0.0010, acc 95.6791 \n",
      "Accuracy of the network on the 10000 test images: 88.5372340425532 %\n",
      "epoch : 18\n",
      "training loss: 0.0009, acc 96.1197 \n",
      "Accuracy of the network on the 10000 test images: 88.73936170212765 %\n",
      "epoch : 19\n",
      "training loss: 0.0008, acc 96.3892 \n",
      "Accuracy of the network on the 10000 test images: 88.47872340425532 %\n",
      "epoch : 20\n",
      "training loss: 0.0008, acc 96.7624 \n",
      "Accuracy of the network on the 10000 test images: 88.59042553191489 %\n",
      "epoch : 21\n",
      "training loss: 0.0007, acc 97.0665 \n",
      "Accuracy of the network on the 10000 test images: 88.42021276595744 %\n",
      "epoch : 22\n",
      "training loss: 0.0007, acc 97.4229 \n",
      "Accuracy of the network on the 10000 test images: 88.13297872340425 %\n",
      "epoch : 23\n",
      "training loss: 0.0006, acc 97.6188 \n",
      "Accuracy of the network on the 10000 test images: 88.26595744680851 %\n",
      "epoch : 24\n",
      "training loss: 0.0006, acc 97.8963 \n",
      "Accuracy of the network on the 10000 test images: 88.20212765957447 %\n",
      "epoch : 25\n",
      "training loss: 0.0005, acc 98.0355 \n",
      "Accuracy of the network on the 10000 test images: 88.15425531914893 %\n",
      "epoch : 26\n",
      "training loss: 0.0005, acc 98.2500 \n",
      "Accuracy of the network on the 10000 test images: 88.26595744680851 %\n",
      "epoch : 27\n",
      "training loss: 0.0005, acc 98.4122 \n",
      "Accuracy of the network on the 10000 test images: 87.88829787234043 %\n",
      "epoch : 28\n",
      "training loss: 0.0004, acc 98.5337 \n",
      "Accuracy of the network on the 10000 test images: 88.28191489361703 %\n",
      "epoch : 29\n",
      "training loss: 0.0004, acc 98.6241 \n",
      "Accuracy of the network on the 10000 test images: 87.84574468085107 %\n",
      "epoch : 30\n",
      "training loss: 0.0004, acc 98.7624 \n",
      "Accuracy of the network on the 10000 test images: 88.11170212765957 %\n",
      "epoch : 31\n",
      "training loss: 0.0003, acc 98.8998 \n",
      "Accuracy of the network on the 10000 test images: 88.09574468085107 %\n",
      "epoch : 32\n",
      "training loss: 0.0003, acc 98.9380 \n",
      "Accuracy of the network on the 10000 test images: 88.25531914893617 %\n",
      "epoch : 33\n",
      "training loss: 0.0003, acc 99.0505 \n",
      "Accuracy of the network on the 10000 test images: 88.07446808510639 %\n",
      "epoch : 34\n",
      "training loss: 0.0003, acc 99.0692 \n",
      "Accuracy of the network on the 10000 test images: 88.12765957446808 %\n",
      "epoch : 35\n",
      "training loss: 0.0003, acc 99.2101 \n",
      "Accuracy of the network on the 10000 test images: 88.19148936170212 %\n",
      "epoch : 36\n",
      "training loss: 0.0003, acc 99.3413 \n",
      "Accuracy of the network on the 10000 test images: 88.23936170212765 %\n",
      "epoch : 37\n",
      "training loss: 0.0002, acc 99.3856 \n",
      "Accuracy of the network on the 10000 test images: 88.2872340425532 %\n",
      "epoch : 38\n",
      "training loss: 0.0002, acc 99.4264 \n",
      "Accuracy of the network on the 10000 test images: 88.21808510638297 %\n",
      "epoch : 39\n",
      "training loss: 0.0002, acc 99.4911 \n",
      "Accuracy of the network on the 10000 test images: 88.07978723404256 %\n",
      "epoch : 40\n",
      "training loss: 0.0002, acc 99.5603 \n",
      "Accuracy of the network on the 10000 test images: 88.20212765957447 %\n",
      "epoch : 41\n",
      "training loss: 0.0002, acc 99.6560 \n",
      "Accuracy of the network on the 10000 test images: 87.96808510638297 %\n",
      "epoch : 42\n",
      "training loss: 0.0002, acc 99.7278 \n",
      "Accuracy of the network on the 10000 test images: 87.97340425531915 %\n",
      "epoch : 43\n",
      "training loss: 0.0002, acc 99.7261 \n",
      "Accuracy of the network on the 10000 test images: 88.02127659574468 %\n",
      "epoch : 44\n",
      "training loss: 0.0001, acc 99.7872 \n",
      "Accuracy of the network on the 10000 test images: 88.00531914893617 %\n",
      "epoch : 45\n",
      "training loss: 0.0001, acc 99.7864 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 46\n",
      "training loss: 0.0001, acc 99.8785 \n",
      "Accuracy of the network on the 10000 test images: 88.1436170212766 %\n",
      "epoch : 47\n",
      "training loss: 0.0001, acc 99.9096 \n",
      "Accuracy of the network on the 10000 test images: 87.97340425531915 %\n",
      "epoch : 48\n",
      "training loss: 0.0001, acc 99.9530 \n",
      "Accuracy of the network on the 10000 test images: 87.97872340425532 %\n",
      "epoch : 49\n",
      "training loss: 0.0001, acc 99.9814 \n",
      "Accuracy of the network on the 10000 test images: 88.22872340425532 %\n",
      "epoch : 50\n",
      "training loss: 0.0001, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 88.12765957446808 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6acee9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:58:49.671089Z",
     "start_time": "2022-04-07T20:58:49.656129Z"
    },
    "id": "6acee9cb"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net2 - training loss'] = running_loss_history\n",
    "results['net2 - training accuracy'] = running_corrects_history\n",
    "results['net2 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9c2b7",
   "metadata": {
    "id": "cbc9c2b7"
   },
   "source": [
    "# 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa82d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:58:49.924410Z",
     "start_time": "2022-04-07T20:58:49.672087Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baa82d83",
    "outputId": "709b7382-819a-44c6-a085-a3212504b1b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38abc25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T20:58:49.940368Z",
     "start_time": "2022-04-07T20:58:49.925409Z"
    },
    "id": "38abc25a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50406e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:16:31.182361Z",
     "start_time": "2022-04-07T20:58:49.941365Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "50406e31",
    "outputId": "55ddd866-fdb2-4cac-9fa9-a119580c0405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0062, acc 77.7456 \n",
      "Accuracy of the network on the 10000 test images: 86.03191489361703 %\n",
      "epoch : 2\n",
      "training loss: 0.0027, acc 87.9699 \n",
      "Accuracy of the network on the 10000 test images: 88.04255319148936 %\n",
      "epoch : 3\n",
      "training loss: 0.0023, acc 89.4176 \n",
      "Accuracy of the network on the 10000 test images: 88.88297872340425 %\n",
      "epoch : 4\n",
      "training loss: 0.0021, acc 90.3218 \n",
      "Accuracy of the network on the 10000 test images: 88.85106382978724 %\n",
      "epoch : 5\n",
      "training loss: 0.0019, acc 91.1179 \n",
      "Accuracy of the network on the 10000 test images: 88.38829787234043 %\n",
      "epoch : 6\n",
      "training loss: 0.0017, acc 91.7979 \n",
      "Accuracy of the network on the 10000 test images: 89.33510638297872 %\n",
      "epoch : 7\n",
      "training loss: 0.0016, acc 92.6614 \n",
      "Accuracy of the network on the 10000 test images: 88.07978723404256 %\n",
      "epoch : 8\n",
      "training loss: 0.0014, acc 93.3661 \n",
      "Accuracy of the network on the 10000 test images: 89.22340425531915 %\n",
      "epoch : 9\n",
      "training loss: 0.0013, acc 94.0612 \n",
      "Accuracy of the network on the 10000 test images: 89.23936170212765 %\n",
      "epoch : 10\n",
      "training loss: 0.0011, acc 94.7952 \n",
      "Accuracy of the network on the 10000 test images: 89.07446808510639 %\n",
      "epoch : 11\n",
      "training loss: 0.0010, acc 95.4238 \n",
      "Accuracy of the network on the 10000 test images: 88.75 %\n",
      "epoch : 12\n",
      "training loss: 0.0009, acc 96.1427 \n",
      "Accuracy of the network on the 10000 test images: 89.08510638297872 %\n",
      "epoch : 13\n",
      "training loss: 0.0008, acc 96.7784 \n",
      "Accuracy of the network on the 10000 test images: 88.7127659574468 %\n",
      "epoch : 14\n",
      "training loss: 0.0007, acc 97.2660 \n",
      "Accuracy of the network on the 10000 test images: 88.37234042553192 %\n",
      "epoch : 15\n",
      "training loss: 0.0006, acc 97.6543 \n",
      "Accuracy of the network on the 10000 test images: 88.94680851063829 %\n",
      "epoch : 16\n",
      "training loss: 0.0005, acc 98.0434 \n",
      "Accuracy of the network on the 10000 test images: 88.81914893617021 %\n",
      "epoch : 17\n",
      "training loss: 0.0005, acc 98.3121 \n",
      "Accuracy of the network on the 10000 test images: 88.56914893617021 %\n",
      "epoch : 18\n",
      "training loss: 0.0004, acc 98.5851 \n",
      "Accuracy of the network on the 10000 test images: 88.77127659574468 %\n",
      "epoch : 19\n",
      "training loss: 0.0004, acc 98.7367 \n",
      "Accuracy of the network on the 10000 test images: 88.81382978723404 %\n",
      "epoch : 20\n",
      "training loss: 0.0003, acc 98.9158 \n",
      "Accuracy of the network on the 10000 test images: 88.76595744680851 %\n",
      "epoch : 21\n",
      "training loss: 0.0003, acc 99.0381 \n",
      "Accuracy of the network on the 10000 test images: 88.26595744680851 %\n",
      "epoch : 22\n",
      "training loss: 0.0003, acc 99.1498 \n",
      "Accuracy of the network on the 10000 test images: 88.47872340425532 %\n",
      "epoch : 23\n",
      "training loss: 0.0002, acc 99.2881 \n",
      "Accuracy of the network on the 10000 test images: 88.43617021276596 %\n",
      "epoch : 24\n",
      "training loss: 0.0002, acc 99.3830 \n",
      "Accuracy of the network on the 10000 test images: 88.29787234042553 %\n",
      "epoch : 25\n",
      "training loss: 0.0002, acc 99.4504 \n",
      "Accuracy of the network on the 10000 test images: 88.40425531914893 %\n",
      "epoch : 26\n",
      "training loss: 0.0002, acc 99.5851 \n",
      "Accuracy of the network on the 10000 test images: 88.1436170212766 %\n",
      "epoch : 27\n",
      "training loss: 0.0002, acc 99.6667 \n",
      "Accuracy of the network on the 10000 test images: 88.39893617021276 %\n",
      "epoch : 28\n",
      "training loss: 0.0001, acc 99.7615 \n",
      "Accuracy of the network on the 10000 test images: 88.26063829787235 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.8387 \n",
      "Accuracy of the network on the 10000 test images: 88.68617021276596 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 99.8059 \n",
      "Accuracy of the network on the 10000 test images: 88.33510638297872 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 99.9628 \n",
      "Accuracy of the network on the 10000 test images: 88.6063829787234 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9991 \n",
      "Accuracy of the network on the 10000 test images: 88.70744680851064 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9991 \n",
      "Accuracy of the network on the 10000 test images: 88.81914893617021 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9991 \n",
      "Accuracy of the network on the 10000 test images: 88.83510638297872 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.75531914893617 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.84042553191489 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.79787234042553 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.81914893617021 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.85106382978724 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.82978723404256 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.8563829787234 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.88297872340425 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.8936170212766 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.90425531914893 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.79255319148936 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.86170212765957 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.92021276595744 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.78191489361703 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.75531914893617 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480dea67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:16:31.198319Z",
     "start_time": "2022-04-07T21:16:31.183359Z"
    },
    "id": "480dea67"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net3 - training loss'] = running_loss_history\n",
    "results['net3 - training accuracy'] = running_corrects_history\n",
    "results['net3 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd28b06",
   "metadata": {
    "id": "5dd28b06"
   },
   "source": [
    "# 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9754f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:16:31.713940Z",
     "start_time": "2022-04-07T21:16:31.199316Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9754f0d",
    "outputId": "5bd54fe4-8609-4640-af40-9f7306cbcdf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f918819b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:16:31.728901Z",
     "start_time": "2022-04-07T21:16:31.714937Z"
    },
    "id": "f918819b"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c234529a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:54:32.728235Z",
     "start_time": "2022-04-07T21:16:31.729898Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "c234529a",
    "outputId": "2e4c07a8-504a-428e-a64e-0139dc857f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0054, acc 79.6383 \n",
      "Accuracy of the network on the 10000 test images: 86.65425531914893 %\n",
      "epoch : 2\n",
      "training loss: 0.0025, acc 88.7252 \n",
      "Accuracy of the network on the 10000 test images: 88.05319148936171 %\n",
      "epoch : 3\n",
      "training loss: 0.0021, acc 90.2934 \n",
      "Accuracy of the network on the 10000 test images: 88.69680851063829 %\n",
      "epoch : 4\n",
      "training loss: 0.0018, acc 91.3395 \n",
      "Accuracy of the network on the 10000 test images: 88.95744680851064 %\n",
      "epoch : 5\n",
      "training loss: 0.0016, acc 92.5771 \n",
      "Accuracy of the network on the 10000 test images: 89.02127659574468 %\n",
      "epoch : 6\n",
      "training loss: 0.0014, acc 93.5505 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 7\n",
      "training loss: 0.0012, acc 94.6020 \n",
      "Accuracy of the network on the 10000 test images: 89.18617021276596 %\n",
      "epoch : 8\n",
      "training loss: 0.0010, acc 95.6312 \n",
      "Accuracy of the network on the 10000 test images: 89.28191489361703 %\n",
      "epoch : 9\n",
      "training loss: 0.0008, acc 96.6206 \n",
      "Accuracy of the network on the 10000 test images: 88.57446808510639 %\n",
      "epoch : 10\n",
      "training loss: 0.0006, acc 97.3484 \n",
      "Accuracy of the network on the 10000 test images: 88.80851063829788 %\n",
      "epoch : 11\n",
      "training loss: 0.0005, acc 97.9566 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 12\n",
      "training loss: 0.0004, acc 98.3918 \n",
      "Accuracy of the network on the 10000 test images: 89.11702127659575 %\n",
      "epoch : 13\n",
      "training loss: 0.0004, acc 98.6764 \n",
      "Accuracy of the network on the 10000 test images: 88.65425531914893 %\n",
      "epoch : 14\n",
      "training loss: 0.0003, acc 98.9530 \n",
      "Accuracy of the network on the 10000 test images: 88.52659574468085 %\n",
      "epoch : 15\n",
      "training loss: 0.0003, acc 99.1605 \n",
      "Accuracy of the network on the 10000 test images: 88.68617021276596 %\n",
      "epoch : 16\n",
      "training loss: 0.0002, acc 99.2154 \n",
      "Accuracy of the network on the 10000 test images: 88.77127659574468 %\n",
      "epoch : 17\n",
      "training loss: 0.0002, acc 99.3670 \n",
      "Accuracy of the network on the 10000 test images: 88.64893617021276 %\n",
      "epoch : 18\n",
      "training loss: 0.0002, acc 99.4229 \n",
      "Accuracy of the network on the 10000 test images: 88.55851063829788 %\n",
      "epoch : 19\n",
      "training loss: 0.0002, acc 99.3715 \n",
      "Accuracy of the network on the 10000 test images: 88.35106382978724 %\n",
      "epoch : 20\n",
      "training loss: 0.0002, acc 99.4575 \n",
      "Accuracy of the network on the 10000 test images: 88.40957446808511 %\n",
      "epoch : 21\n",
      "training loss: 0.0002, acc 99.3821 \n",
      "Accuracy of the network on the 10000 test images: 88.50531914893617 %\n",
      "epoch : 22\n",
      "training loss: 0.0001, acc 99.6392 \n",
      "Accuracy of the network on the 10000 test images: 88.51595744680851 %\n",
      "epoch : 23\n",
      "training loss: 0.0001, acc 99.8839 \n",
      "Accuracy of the network on the 10000 test images: 88.73936170212765 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.8174 \n",
      "Accuracy of the network on the 10000 test images: 88.89893617021276 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9885 \n",
      "Accuracy of the network on the 10000 test images: 88.91489361702128 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9947 \n",
      "Accuracy of the network on the 10000 test images: 89.12765957446808 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.08510638297872 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.12234042553192 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.17553191489361 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.16489361702128 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.18085106382979 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.17553191489361 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.15425531914893 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.2127659574468 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.20744680851064 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.10106382978724 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.13297872340425 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.25 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.2127659574468 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.23404255319149 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.10106382978724 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.26063829787235 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.11702127659575 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.14893617021276 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.2127659574468 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.2127659574468 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f7b8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:54:32.743196Z",
     "start_time": "2022-04-07T21:54:32.729233Z"
    },
    "id": "61f7b8b5"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net4 - training loss'] = running_loss_history\n",
    "results['net4 - training accuracy'] = running_corrects_history\n",
    "results['net4 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d6af4",
   "metadata": {
    "id": "653d6af4"
   },
   "source": [
    "# 10 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eee2a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:54:33.751528Z",
     "start_time": "2022-04-07T21:54:32.744192Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eee2a21",
    "outputId": "f7c898d7-4c2c-409d-85b1-de0b0d6b6a34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=200704, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(1024),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(200704, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f01ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T21:54:33.767458Z",
     "start_time": "2022-04-07T21:54:33.752496Z"
    },
    "id": "d4f01ef2"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e5a7118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:53:35.603356Z",
     "start_time": "2022-04-07T21:54:33.768454Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e5a7118",
    "outputId": "717b745b-18e7-4db0-b90f-d580825706a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0049, acc 80.9681 \n",
      "Accuracy of the network on the 10000 test images: 87.39893617021276 %\n",
      "epoch : 2\n",
      "training loss: 0.0023, acc 89.3715 \n",
      "Accuracy of the network on the 10000 test images: 88.37765957446808 %\n",
      "epoch : 3\n",
      "training loss: 0.0018, acc 91.2145 \n",
      "Accuracy of the network on the 10000 test images: 89.13829787234043 %\n",
      "epoch : 4\n",
      "training loss: 0.0014, acc 93.0709 \n",
      "Accuracy of the network on the 10000 test images: 89.30851063829788 %\n",
      "epoch : 5\n",
      "training loss: 0.0011, acc 94.5355 \n",
      "Accuracy of the network on the 10000 test images: 88.59042553191489 %\n",
      "epoch : 6\n",
      "training loss: 0.0009, acc 96.1170 \n",
      "Accuracy of the network on the 10000 test images: 88.68085106382979 %\n",
      "epoch : 7\n",
      "training loss: 0.0006, acc 97.2376 \n",
      "Accuracy of the network on the 10000 test images: 89.03191489361703 %\n",
      "epoch : 8\n",
      "training loss: 0.0005, acc 98.0275 \n",
      "Accuracy of the network on the 10000 test images: 88.9627659574468 %\n",
      "epoch : 9\n",
      "training loss: 0.0004, acc 98.5825 \n",
      "Accuracy of the network on the 10000 test images: 89.0372340425532 %\n",
      "epoch : 10\n",
      "training loss: 0.0003, acc 98.8723 \n",
      "Accuracy of the network on the 10000 test images: 89.0 %\n",
      "epoch : 11\n",
      "training loss: 0.0003, acc 99.0390 \n",
      "Accuracy of the network on the 10000 test images: 88.79255319148936 %\n",
      "epoch : 12\n",
      "training loss: 0.0002, acc 99.1844 \n",
      "Accuracy of the network on the 10000 test images: 89.09042553191489 %\n",
      "epoch : 13\n",
      "training loss: 0.0002, acc 99.3218 \n",
      "Accuracy of the network on the 10000 test images: 89.01595744680851 %\n",
      "epoch : 14\n",
      "training loss: 0.0002, acc 99.3732 \n",
      "Accuracy of the network on the 10000 test images: 89.10106382978724 %\n",
      "epoch : 15\n",
      "training loss: 0.0002, acc 99.3449 \n",
      "Accuracy of the network on the 10000 test images: 88.6063829787234 %\n",
      "epoch : 16\n",
      "training loss: 0.0003, acc 98.7775 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 17\n",
      "training loss: 0.0004, acc 98.4601 \n",
      "Accuracy of the network on the 10000 test images: 88.59574468085107 %\n",
      "epoch : 18\n",
      "training loss: 0.0003, acc 98.8590 \n",
      "Accuracy of the network on the 10000 test images: 88.58510638297872 %\n",
      "epoch : 19\n",
      "training loss: 0.0003, acc 99.1534 \n",
      "Accuracy of the network on the 10000 test images: 88.52127659574468 %\n",
      "epoch : 20\n",
      "training loss: 0.0002, acc 99.5080 \n",
      "Accuracy of the network on the 10000 test images: 88.73404255319149 %\n",
      "epoch : 21\n",
      "training loss: 0.0001, acc 99.7252 \n",
      "Accuracy of the network on the 10000 test images: 88.82446808510639 %\n",
      "epoch : 22\n",
      "training loss: 0.0000, acc 99.9149 \n",
      "Accuracy of the network on the 10000 test images: 88.90957446808511 %\n",
      "epoch : 23\n",
      "training loss: 0.0000, acc 99.9424 \n",
      "Accuracy of the network on the 10000 test images: 89.12765957446808 %\n",
      "epoch : 24\n",
      "training loss: 0.0000, acc 99.9956 \n",
      "Accuracy of the network on the 10000 test images: 89.19680851063829 %\n",
      "epoch : 25\n",
      "training loss: 0.0000, acc 99.9982 \n",
      "Accuracy of the network on the 10000 test images: 89.26595744680851 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.27127659574468 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.33510638297872 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.29787234042553 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.23936170212765 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.28191489361703 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.29255319148936 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.30319148936171 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.35106382978724 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.30851063829788 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.34574468085107 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.32978723404256 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.43085106382979 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.41489361702128 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.33510638297872 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.36702127659575 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.43085106382979 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.37765957446808 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.36702127659575 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.3936170212766 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.38297872340425 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.44148936170212 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.45212765957447 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.42021276595744 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.35106382978724 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.47872340425532 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70af0af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:53:35.619313Z",
     "start_time": "2022-04-07T23:53:35.604353Z"
    },
    "id": "70af0af5",
    "outputId": "45b22e73-c7c2-4484-d7e9-8a1097d18f78"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net5 - training loss'] = running_loss_history\n",
    "results['net5 - training accuracy'] = running_corrects_history\n",
    "results['net5 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da47b7f",
   "metadata": {
    "id": "9da47b7f"
   },
   "source": [
    "# ---------------- Changing Neurons in Each Network ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0f4f1",
   "metadata": {
    "id": "77e0f4f1"
   },
   "source": [
    "## 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "953f2861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:53:35.665194Z",
     "start_time": "2022-04-07T23:53:35.620312Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "953f2861",
    "outputId": "c5c23e0d-630c-42ab-9977-35c7edb9b57a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e17ff7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:53:35.681151Z",
     "start_time": "2022-04-07T23:53:35.666191Z"
    },
    "id": "e17ff7e2"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d3601df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:58:50.312250Z",
     "start_time": "2022-04-07T23:53:35.682149Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "5d3601df",
    "outputId": "8fa4f44b-4bba-401f-820d-4cd8de2c2ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0121, acc 61.9973 \n",
      "Accuracy of the network on the 10000 test images: 78.7127659574468 %\n",
      "epoch : 2\n",
      "training loss: 0.0045, acc 81.6817 \n",
      "Accuracy of the network on the 10000 test images: 83.04255319148936 %\n",
      "epoch : 3\n",
      "training loss: 0.0037, acc 84.6747 \n",
      "Accuracy of the network on the 10000 test images: 84.19680851063829 %\n",
      "epoch : 4\n",
      "training loss: 0.0033, acc 86.1170 \n",
      "Accuracy of the network on the 10000 test images: 85.38829787234043 %\n",
      "epoch : 5\n",
      "training loss: 0.0030, acc 86.9778 \n",
      "Accuracy of the network on the 10000 test images: 85.9627659574468 %\n",
      "epoch : 6\n",
      "training loss: 0.0028, acc 87.6649 \n",
      "Accuracy of the network on the 10000 test images: 86.11170212765957 %\n",
      "epoch : 7\n",
      "training loss: 0.0027, acc 88.3555 \n",
      "Accuracy of the network on the 10000 test images: 86.62765957446808 %\n",
      "epoch : 8\n",
      "training loss: 0.0025, acc 88.7651 \n",
      "Accuracy of the network on the 10000 test images: 86.87234042553192 %\n",
      "epoch : 9\n",
      "training loss: 0.0024, acc 89.3254 \n",
      "Accuracy of the network on the 10000 test images: 87.31382978723404 %\n",
      "epoch : 10\n",
      "training loss: 0.0023, acc 89.7979 \n",
      "Accuracy of the network on the 10000 test images: 86.88297872340425 %\n",
      "epoch : 11\n",
      "training loss: 0.0022, acc 90.0798 \n",
      "Accuracy of the network on the 10000 test images: 87.61170212765957 %\n",
      "epoch : 12\n",
      "training loss: 0.0021, acc 90.4973 \n",
      "Accuracy of the network on the 10000 test images: 87.7127659574468 %\n",
      "epoch : 13\n",
      "training loss: 0.0020, acc 90.9264 \n",
      "Accuracy of the network on the 10000 test images: 87.72872340425532 %\n",
      "epoch : 14\n",
      "training loss: 0.0020, acc 91.2314 \n",
      "Accuracy of the network on the 10000 test images: 87.88829787234043 %\n",
      "epoch : 15\n",
      "training loss: 0.0019, acc 91.3936 \n",
      "Accuracy of the network on the 10000 test images: 87.88829787234043 %\n",
      "epoch : 16\n",
      "training loss: 0.0018, acc 91.8821 \n",
      "Accuracy of the network on the 10000 test images: 87.63829787234043 %\n",
      "epoch : 17\n",
      "training loss: 0.0017, acc 92.1232 \n",
      "Accuracy of the network on the 10000 test images: 88.20212765957447 %\n",
      "epoch : 18\n",
      "training loss: 0.0017, acc 92.4486 \n",
      "Accuracy of the network on the 10000 test images: 88.21808510638297 %\n",
      "epoch : 19\n",
      "training loss: 0.0016, acc 92.7624 \n",
      "Accuracy of the network on the 10000 test images: 87.90957446808511 %\n",
      "epoch : 20\n",
      "training loss: 0.0016, acc 93.0745 \n",
      "Accuracy of the network on the 10000 test images: 88.13829787234043 %\n",
      "epoch : 21\n",
      "training loss: 0.0015, acc 93.3165 \n",
      "Accuracy of the network on the 10000 test images: 88.15425531914893 %\n",
      "epoch : 22\n",
      "training loss: 0.0014, acc 93.6481 \n",
      "Accuracy of the network on the 10000 test images: 87.82446808510639 %\n",
      "epoch : 23\n",
      "training loss: 0.0014, acc 93.9495 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 24\n",
      "training loss: 0.0013, acc 94.2589 \n",
      "Accuracy of the network on the 10000 test images: 87.87765957446808 %\n",
      "epoch : 25\n",
      "training loss: 0.0013, acc 94.4840 \n",
      "Accuracy of the network on the 10000 test images: 87.80319148936171 %\n",
      "epoch : 26\n",
      "training loss: 0.0012, acc 94.6693 \n",
      "Accuracy of the network on the 10000 test images: 87.36170212765957 %\n",
      "epoch : 27\n",
      "training loss: 0.0012, acc 95.0275 \n",
      "Accuracy of the network on the 10000 test images: 87.39893617021276 %\n",
      "epoch : 28\n",
      "training loss: 0.0011, acc 95.2145 \n",
      "Accuracy of the network on the 10000 test images: 87.44680851063829 %\n",
      "epoch : 29\n",
      "training loss: 0.0011, acc 95.5239 \n",
      "Accuracy of the network on the 10000 test images: 87.76595744680851 %\n",
      "epoch : 30\n",
      "training loss: 0.0010, acc 95.7385 \n",
      "Accuracy of the network on the 10000 test images: 87.78191489361703 %\n",
      "epoch : 31\n",
      "training loss: 0.0010, acc 95.9823 \n",
      "Accuracy of the network on the 10000 test images: 87.65425531914893 %\n",
      "epoch : 32\n",
      "training loss: 0.0009, acc 96.2731 \n",
      "Accuracy of the network on the 10000 test images: 87.60106382978724 %\n",
      "epoch : 33\n",
      "training loss: 0.0009, acc 96.3954 \n",
      "Accuracy of the network on the 10000 test images: 87.75531914893617 %\n",
      "epoch : 34\n",
      "training loss: 0.0009, acc 96.5940 \n",
      "Accuracy of the network on the 10000 test images: 87.18085106382979 %\n",
      "epoch : 35\n",
      "training loss: 0.0008, acc 96.8298 \n",
      "Accuracy of the network on the 10000 test images: 87.23404255319149 %\n",
      "epoch : 36\n",
      "training loss: 0.0008, acc 96.9982 \n",
      "Accuracy of the network on the 10000 test images: 87.60106382978724 %\n",
      "epoch : 37\n",
      "training loss: 0.0007, acc 97.2207 \n",
      "Accuracy of the network on the 10000 test images: 87.20744680851064 %\n",
      "epoch : 38\n",
      "training loss: 0.0007, acc 97.3156 \n",
      "Accuracy of the network on the 10000 test images: 87.56382978723404 %\n",
      "epoch : 39\n",
      "training loss: 0.0007, acc 97.5053 \n",
      "Accuracy of the network on the 10000 test images: 87.46808510638297 %\n",
      "epoch : 40\n",
      "training loss: 0.0006, acc 97.6303 \n",
      "Accuracy of the network on the 10000 test images: 87.40425531914893 %\n",
      "epoch : 41\n",
      "training loss: 0.0006, acc 97.7731 \n",
      "Accuracy of the network on the 10000 test images: 87.29787234042553 %\n",
      "epoch : 42\n",
      "training loss: 0.0006, acc 97.8644 \n",
      "Accuracy of the network on the 10000 test images: 87.19680851063829 %\n",
      "epoch : 43\n",
      "training loss: 0.0006, acc 97.9858 \n",
      "Accuracy of the network on the 10000 test images: 87.43085106382979 %\n",
      "epoch : 44\n",
      "training loss: 0.0006, acc 98.0851 \n",
      "Accuracy of the network on the 10000 test images: 87.59574468085107 %\n",
      "epoch : 45\n",
      "training loss: 0.0005, acc 98.1959 \n",
      "Accuracy of the network on the 10000 test images: 87.47872340425532 %\n",
      "epoch : 46\n",
      "training loss: 0.0005, acc 98.2411 \n",
      "Accuracy of the network on the 10000 test images: 87.41489361702128 %\n",
      "epoch : 47\n",
      "training loss: 0.0005, acc 98.3697 \n",
      "Accuracy of the network on the 10000 test images: 87.43085106382979 %\n",
      "epoch : 48\n",
      "training loss: 0.0005, acc 98.3998 \n",
      "Accuracy of the network on the 10000 test images: 87.36170212765957 %\n",
      "epoch : 49\n",
      "training loss: 0.0005, acc 98.4557 \n",
      "Accuracy of the network on the 10000 test images: 87.06382978723404 %\n",
      "epoch : 50\n",
      "training loss: 0.0004, acc 98.5479 \n",
      "Accuracy of the network on the 10000 test images: 87.36170212765957 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7690e4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:58:50.328208Z",
     "start_time": "2022-04-07T23:58:50.313247Z"
    },
    "id": "7690e4e6"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net6 - training loss'] = running_loss_history\n",
    "results['net6 - training accuracy'] = running_corrects_history\n",
    "results['net6 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f3045",
   "metadata": {
    "id": "9b0f3045"
   },
   "source": [
    "## 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e77401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:58:50.407995Z",
     "start_time": "2022-04-07T23:58:50.329205Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87e77401",
    "outputId": "fc77a4f1-0157-476c-e997-26d2e7e6f28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d796263a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:58:50.423952Z",
     "start_time": "2022-04-07T23:58:50.408993Z"
    },
    "id": "d796263a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83689cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:05:37.273972Z",
     "start_time": "2022-04-07T23:58:50.426943Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "83689cd0",
    "outputId": "fcb64137-4911-444d-cbee-9654ab797c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0095, acc 69.0621 \n",
      "Accuracy of the network on the 10000 test images: 83.07446808510639 %\n",
      "epoch : 2\n",
      "training loss: 0.0035, acc 85.0160 \n",
      "Accuracy of the network on the 10000 test images: 85.37234042553192 %\n",
      "epoch : 3\n",
      "training loss: 0.0030, acc 87.0284 \n",
      "Accuracy of the network on the 10000 test images: 86.30319148936171 %\n",
      "epoch : 4\n",
      "training loss: 0.0027, acc 88.0479 \n",
      "Accuracy of the network on the 10000 test images: 87.27127659574468 %\n",
      "epoch : 5\n",
      "training loss: 0.0025, acc 88.8537 \n",
      "Accuracy of the network on the 10000 test images: 87.90425531914893 %\n",
      "epoch : 6\n",
      "training loss: 0.0023, acc 89.4353 \n",
      "Accuracy of the network on the 10000 test images: 88.22872340425532 %\n",
      "epoch : 7\n",
      "training loss: 0.0022, acc 89.9140 \n",
      "Accuracy of the network on the 10000 test images: 88.55851063829788 %\n",
      "epoch : 8\n",
      "training loss: 0.0021, acc 90.1897 \n",
      "Accuracy of the network on the 10000 test images: 88.26063829787235 %\n",
      "epoch : 9\n",
      "training loss: 0.0020, acc 90.7278 \n",
      "Accuracy of the network on the 10000 test images: 88.23404255319149 %\n",
      "epoch : 10\n",
      "training loss: 0.0019, acc 91.0895 \n",
      "Accuracy of the network on the 10000 test images: 88.48936170212765 %\n",
      "epoch : 11\n",
      "training loss: 0.0019, acc 91.3644 \n",
      "Accuracy of the network on the 10000 test images: 88.66489361702128 %\n",
      "epoch : 12\n",
      "training loss: 0.0018, acc 91.7908 \n",
      "Accuracy of the network on the 10000 test images: 88.59574468085107 %\n",
      "epoch : 13\n",
      "training loss: 0.0017, acc 92.0443 \n",
      "Accuracy of the network on the 10000 test images: 88.62765957446808 %\n",
      "epoch : 14\n",
      "training loss: 0.0016, acc 92.4380 \n",
      "Accuracy of the network on the 10000 test images: 88.57446808510639 %\n",
      "epoch : 15\n",
      "training loss: 0.0016, acc 92.8050 \n",
      "Accuracy of the network on the 10000 test images: 88.5 %\n",
      "epoch : 16\n",
      "training loss: 0.0015, acc 93.1108 \n",
      "Accuracy of the network on the 10000 test images: 88.74468085106383 %\n",
      "epoch : 17\n",
      "training loss: 0.0014, acc 93.4548 \n",
      "Accuracy of the network on the 10000 test images: 88.84574468085107 %\n",
      "epoch : 18\n",
      "training loss: 0.0014, acc 93.7775 \n",
      "Accuracy of the network on the 10000 test images: 88.77127659574468 %\n",
      "epoch : 19\n",
      "training loss: 0.0013, acc 94.0434 \n",
      "Accuracy of the network on the 10000 test images: 88.45212765957447 %\n",
      "epoch : 20\n",
      "training loss: 0.0012, acc 94.4140 \n",
      "Accuracy of the network on the 10000 test images: 88.74468085106383 %\n",
      "epoch : 21\n",
      "training loss: 0.0012, acc 94.8085 \n",
      "Accuracy of the network on the 10000 test images: 88.47340425531915 %\n",
      "epoch : 22\n",
      "training loss: 0.0011, acc 95.0523 \n",
      "Accuracy of the network on the 10000 test images: 88.68085106382979 %\n",
      "epoch : 23\n",
      "training loss: 0.0011, acc 95.3741 \n",
      "Accuracy of the network on the 10000 test images: 88.51063829787235 %\n",
      "epoch : 24\n",
      "training loss: 0.0010, acc 95.6312 \n",
      "Accuracy of the network on the 10000 test images: 88.42553191489361 %\n",
      "epoch : 25\n",
      "training loss: 0.0009, acc 95.9052 \n",
      "Accuracy of the network on the 10000 test images: 88.6063829787234 %\n",
      "epoch : 26\n",
      "training loss: 0.0009, acc 96.2278 \n",
      "Accuracy of the network on the 10000 test images: 88.25 %\n",
      "epoch : 27\n",
      "training loss: 0.0008, acc 96.5248 \n",
      "Accuracy of the network on the 10000 test images: 88.14893617021276 %\n",
      "epoch : 28\n",
      "training loss: 0.0008, acc 96.8005 \n",
      "Accuracy of the network on the 10000 test images: 88.18085106382979 %\n",
      "epoch : 29\n",
      "training loss: 0.0007, acc 97.0195 \n",
      "Accuracy of the network on the 10000 test images: 87.98404255319149 %\n",
      "epoch : 30\n",
      "training loss: 0.0007, acc 97.2766 \n",
      "Accuracy of the network on the 10000 test images: 87.84042553191489 %\n",
      "epoch : 31\n",
      "training loss: 0.0007, acc 97.4371 \n",
      "Accuracy of the network on the 10000 test images: 88.09042553191489 %\n",
      "epoch : 32\n",
      "training loss: 0.0006, acc 97.6569 \n",
      "Accuracy of the network on the 10000 test images: 88.22872340425532 %\n",
      "epoch : 33\n",
      "training loss: 0.0006, acc 97.7287 \n",
      "Accuracy of the network on the 10000 test images: 87.79787234042553 %\n",
      "epoch : 34\n",
      "training loss: 0.0006, acc 97.9371 \n",
      "Accuracy of the network on the 10000 test images: 87.95744680851064 %\n",
      "epoch : 35\n",
      "training loss: 0.0005, acc 98.0585 \n",
      "Accuracy of the network on the 10000 test images: 87.8936170212766 %\n",
      "epoch : 36\n",
      "training loss: 0.0005, acc 98.1888 \n",
      "Accuracy of the network on the 10000 test images: 88.05851063829788 %\n",
      "epoch : 37\n",
      "training loss: 0.0005, acc 98.3342 \n",
      "Accuracy of the network on the 10000 test images: 88.05851063829788 %\n",
      "epoch : 38\n",
      "training loss: 0.0005, acc 98.4202 \n",
      "Accuracy of the network on the 10000 test images: 87.57446808510639 %\n",
      "epoch : 39\n",
      "training loss: 0.0004, acc 98.5417 \n",
      "Accuracy of the network on the 10000 test images: 88.04255319148936 %\n",
      "epoch : 40\n",
      "training loss: 0.0004, acc 98.5762 \n",
      "Accuracy of the network on the 10000 test images: 87.80319148936171 %\n",
      "epoch : 41\n",
      "training loss: 0.0004, acc 98.6995 \n",
      "Accuracy of the network on the 10000 test images: 87.70744680851064 %\n",
      "epoch : 42\n",
      "training loss: 0.0004, acc 98.7633 \n",
      "Accuracy of the network on the 10000 test images: 87.57446808510639 %\n",
      "epoch : 43\n",
      "training loss: 0.0004, acc 98.8192 \n",
      "Accuracy of the network on the 10000 test images: 87.7127659574468 %\n",
      "epoch : 44\n",
      "training loss: 0.0003, acc 98.9149 \n",
      "Accuracy of the network on the 10000 test images: 87.55851063829788 %\n",
      "epoch : 45\n",
      "training loss: 0.0003, acc 98.9903 \n",
      "Accuracy of the network on the 10000 test images: 87.51595744680851 %\n",
      "epoch : 46\n",
      "training loss: 0.0003, acc 99.0231 \n",
      "Accuracy of the network on the 10000 test images: 87.66489361702128 %\n",
      "epoch : 47\n",
      "training loss: 0.0003, acc 99.1241 \n",
      "Accuracy of the network on the 10000 test images: 87.40957446808511 %\n",
      "epoch : 48\n",
      "training loss: 0.0003, acc 99.0940 \n",
      "Accuracy of the network on the 10000 test images: 87.36702127659575 %\n",
      "epoch : 49\n",
      "training loss: 0.0003, acc 99.2092 \n",
      "Accuracy of the network on the 10000 test images: 87.70744680851064 %\n",
      "epoch : 50\n",
      "training loss: 0.0003, acc 99.2332 \n",
      "Accuracy of the network on the 10000 test images: 87.39893617021276 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "374a25a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:05:37.289901Z",
     "start_time": "2022-04-08T00:05:37.275938Z"
    },
    "id": "374a25a4"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net7 - training loss'] = running_loss_history\n",
    "results['net7 - training accuracy'] = running_corrects_history\n",
    "results['net7 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae4266",
   "metadata": {
    "id": "6bae4266"
   },
   "source": [
    "## 6 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "540075c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:05:37.416563Z",
     "start_time": "2022-04-08T00:05:37.290898Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "540075c0",
    "outputId": "2772efc3-be11-4cf7-cb83-068e6c032829"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (18): ReLU()\n",
       "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "455d912a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:05:37.432520Z",
     "start_time": "2022-04-08T00:05:37.417559Z"
    },
    "id": "455d912a"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac294781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:16:02.387894Z",
     "start_time": "2022-04-08T00:05:37.433517Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ac294781",
    "outputId": "4c87e299-d0e4-497f-a361-9f8e058e7f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0074, acc 74.6746 \n",
      "Accuracy of the network on the 10000 test images: 85.19148936170212 %\n",
      "epoch : 2\n",
      "training loss: 0.0030, acc 86.8245 \n",
      "Accuracy of the network on the 10000 test images: 87.2127659574468 %\n",
      "epoch : 3\n",
      "training loss: 0.0026, acc 88.4601 \n",
      "Accuracy of the network on the 10000 test images: 87.59042553191489 %\n",
      "epoch : 4\n",
      "training loss: 0.0023, acc 89.2908 \n",
      "Accuracy of the network on the 10000 test images: 88.15425531914893 %\n",
      "epoch : 5\n",
      "training loss: 0.0022, acc 89.8812 \n",
      "Accuracy of the network on the 10000 test images: 88.69148936170212 %\n",
      "epoch : 6\n",
      "training loss: 0.0020, acc 90.6002 \n",
      "Accuracy of the network on the 10000 test images: 88.97872340425532 %\n",
      "epoch : 7\n",
      "training loss: 0.0019, acc 91.1108 \n",
      "Accuracy of the network on the 10000 test images: 88.7872340425532 %\n",
      "epoch : 8\n",
      "training loss: 0.0018, acc 91.5674 \n",
      "Accuracy of the network on the 10000 test images: 89.16489361702128 %\n",
      "epoch : 9\n",
      "training loss: 0.0017, acc 92.1028 \n",
      "Accuracy of the network on the 10000 test images: 88.72872340425532 %\n",
      "epoch : 10\n",
      "training loss: 0.0016, acc 92.4184 \n",
      "Accuracy of the network on the 10000 test images: 88.93085106382979 %\n",
      "epoch : 11\n",
      "training loss: 0.0015, acc 92.9176 \n",
      "Accuracy of the network on the 10000 test images: 89.07446808510639 %\n",
      "epoch : 12\n",
      "training loss: 0.0014, acc 93.4787 \n",
      "Accuracy of the network on the 10000 test images: 89.0 %\n",
      "epoch : 13\n",
      "training loss: 0.0013, acc 93.8227 \n",
      "Accuracy of the network on the 10000 test images: 89.01595744680851 %\n",
      "epoch : 14\n",
      "training loss: 0.0012, acc 94.3856 \n",
      "Accuracy of the network on the 10000 test images: 89.02659574468085 %\n",
      "epoch : 15\n",
      "training loss: 0.0011, acc 94.8812 \n",
      "Accuracy of the network on the 10000 test images: 88.70744680851064 %\n",
      "epoch : 16\n",
      "training loss: 0.0010, acc 95.3759 \n",
      "Accuracy of the network on the 10000 test images: 88.89893617021276 %\n",
      "epoch : 17\n",
      "training loss: 0.0010, acc 95.7846 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 18\n",
      "training loss: 0.0009, acc 96.2358 \n",
      "Accuracy of the network on the 10000 test images: 88.74468085106383 %\n",
      "epoch : 19\n",
      "training loss: 0.0008, acc 96.5851 \n",
      "Accuracy of the network on the 10000 test images: 88.65957446808511 %\n",
      "epoch : 20\n",
      "training loss: 0.0007, acc 97.0071 \n",
      "Accuracy of the network on the 10000 test images: 88.46808510638297 %\n",
      "epoch : 21\n",
      "training loss: 0.0007, acc 97.3351 \n",
      "Accuracy of the network on the 10000 test images: 88.70212765957447 %\n",
      "epoch : 22\n",
      "training loss: 0.0006, acc 97.5869 \n",
      "Accuracy of the network on the 10000 test images: 88.40957446808511 %\n",
      "epoch : 23\n",
      "training loss: 0.0006, acc 97.8856 \n",
      "Accuracy of the network on the 10000 test images: 88.45744680851064 %\n",
      "epoch : 24\n",
      "training loss: 0.0005, acc 98.1667 \n",
      "Accuracy of the network on the 10000 test images: 88.51595744680851 %\n",
      "epoch : 25\n",
      "training loss: 0.0005, acc 98.2731 \n",
      "Accuracy of the network on the 10000 test images: 88.60106382978724 %\n",
      "epoch : 26\n",
      "training loss: 0.0004, acc 98.4619 \n",
      "Accuracy of the network on the 10000 test images: 88.41489361702128 %\n",
      "epoch : 27\n",
      "training loss: 0.0004, acc 98.5514 \n",
      "Accuracy of the network on the 10000 test images: 87.97872340425532 %\n",
      "epoch : 28\n",
      "training loss: 0.0004, acc 98.7828 \n",
      "Accuracy of the network on the 10000 test images: 88.49468085106383 %\n",
      "epoch : 29\n",
      "training loss: 0.0003, acc 98.8918 \n",
      "Accuracy of the network on the 10000 test images: 88.13829787234043 %\n",
      "epoch : 30\n",
      "training loss: 0.0003, acc 98.9521 \n",
      "Accuracy of the network on the 10000 test images: 88.16489361702128 %\n",
      "epoch : 31\n",
      "training loss: 0.0003, acc 99.0222 \n",
      "Accuracy of the network on the 10000 test images: 88.12234042553192 %\n",
      "epoch : 32\n",
      "training loss: 0.0003, acc 99.1436 \n",
      "Accuracy of the network on the 10000 test images: 88.42553191489361 %\n",
      "epoch : 33\n",
      "training loss: 0.0003, acc 99.1995 \n",
      "Accuracy of the network on the 10000 test images: 88.1063829787234 %\n",
      "epoch : 34\n",
      "training loss: 0.0003, acc 99.3138 \n",
      "Accuracy of the network on the 10000 test images: 88.04787234042553 %\n",
      "epoch : 35\n",
      "training loss: 0.0003, acc 99.3192 \n",
      "Accuracy of the network on the 10000 test images: 87.95212765957447 %\n",
      "epoch : 36\n",
      "training loss: 0.0002, acc 99.4459 \n",
      "Accuracy of the network on the 10000 test images: 88.16489361702128 %\n",
      "epoch : 37\n",
      "training loss: 0.0002, acc 99.5559 \n",
      "Accuracy of the network on the 10000 test images: 88.23404255319149 %\n",
      "epoch : 38\n",
      "training loss: 0.0002, acc 99.5098 \n",
      "Accuracy of the network on the 10000 test images: 87.98404255319149 %\n",
      "epoch : 39\n",
      "training loss: 0.0002, acc 99.5940 \n",
      "Accuracy of the network on the 10000 test images: 87.98936170212765 %\n",
      "epoch : 40\n",
      "training loss: 0.0002, acc 99.6383 \n",
      "Accuracy of the network on the 10000 test images: 88.24468085106383 %\n",
      "epoch : 41\n",
      "training loss: 0.0001, acc 99.7784 \n",
      "Accuracy of the network on the 10000 test images: 88.2127659574468 %\n",
      "epoch : 42\n",
      "training loss: 0.0001, acc 99.8564 \n",
      "Accuracy of the network on the 10000 test images: 88.18085106382979 %\n",
      "epoch : 43\n",
      "training loss: 0.0001, acc 99.9424 \n",
      "Accuracy of the network on the 10000 test images: 88.17553191489361 %\n",
      "epoch : 44\n",
      "training loss: 0.0001, acc 99.9539 \n",
      "Accuracy of the network on the 10000 test images: 88.32446808510639 %\n",
      "epoch : 45\n",
      "training loss: 0.0001, acc 99.9734 \n",
      "Accuracy of the network on the 10000 test images: 88.31382978723404 %\n",
      "epoch : 46\n",
      "training loss: 0.0001, acc 99.9982 \n",
      "Accuracy of the network on the 10000 test images: 88.37234042553192 %\n",
      "epoch : 47\n",
      "training loss: 0.0001, acc 99.9982 \n",
      "Accuracy of the network on the 10000 test images: 88.42553191489361 %\n",
      "epoch : 48\n",
      "training loss: 0.0001, acc 99.9991 \n",
      "Accuracy of the network on the 10000 test images: 88.56914893617021 %\n",
      "epoch : 49\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.55319148936171 %\n",
      "epoch : 50\n",
      "training loss: 0.0001, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.55319148936171 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6464f30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:16:02.403851Z",
     "start_time": "2022-04-08T00:16:02.388892Z"
    },
    "id": "6464f30e"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net8 - training loss'] = running_loss_history\n",
    "results['net8 - training accuracy'] = running_corrects_history\n",
    "results['net8 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a172b",
   "metadata": {
    "id": "7e7a172b"
   },
   "source": [
    "## 8 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f5c4e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:16:02.643211Z",
     "start_time": "2022-04-08T00:16:02.404849Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f5c4e3c",
    "outputId": "108d26c7-461c-4707-94ff-313b8d9e4efb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Flatten(start_dim=1, end_dim=-1)\n",
       "    (22): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c92a7e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:16:02.659169Z",
     "start_time": "2022-04-08T00:16:02.644209Z"
    },
    "id": "c92a7e42"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff46de4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:34:04.196037Z",
     "start_time": "2022-04-08T00:16:02.660167Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ff46de4f",
    "outputId": "a990e219-5279-4aff-8aa3-58b0a6e62cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0064, acc 77.1667 \n",
      "Accuracy of the network on the 10000 test images: 86.29255319148936 %\n",
      "epoch : 2\n",
      "training loss: 0.0027, acc 87.7358 \n",
      "Accuracy of the network on the 10000 test images: 87.9627659574468 %\n",
      "epoch : 3\n",
      "training loss: 0.0023, acc 89.3050 \n",
      "Accuracy of the network on the 10000 test images: 88.36702127659575 %\n",
      "epoch : 4\n",
      "training loss: 0.0021, acc 90.2376 \n",
      "Accuracy of the network on the 10000 test images: 88.65425531914893 %\n",
      "epoch : 5\n",
      "training loss: 0.0019, acc 91.0301 \n",
      "Accuracy of the network on the 10000 test images: 88.90425531914893 %\n",
      "epoch : 6\n",
      "training loss: 0.0017, acc 91.8493 \n",
      "Accuracy of the network on the 10000 test images: 89.14893617021276 %\n",
      "epoch : 7\n",
      "training loss: 0.0016, acc 92.5656 \n",
      "Accuracy of the network on the 10000 test images: 89.28191489361703 %\n",
      "epoch : 8\n",
      "training loss: 0.0014, acc 93.3174 \n",
      "Accuracy of the network on the 10000 test images: 89.16489361702128 %\n",
      "epoch : 9\n",
      "training loss: 0.0013, acc 94.1259 \n",
      "Accuracy of the network on the 10000 test images: 89.07978723404256 %\n",
      "epoch : 10\n",
      "training loss: 0.0011, acc 94.8245 \n",
      "Accuracy of the network on the 10000 test images: 89.21808510638297 %\n",
      "epoch : 11\n",
      "training loss: 0.0010, acc 95.6108 \n",
      "Accuracy of the network on the 10000 test images: 88.98404255319149 %\n",
      "epoch : 12\n",
      "training loss: 0.0008, acc 96.3156 \n",
      "Accuracy of the network on the 10000 test images: 89.19148936170212 %\n",
      "epoch : 13\n",
      "training loss: 0.0007, acc 96.9034 \n",
      "Accuracy of the network on the 10000 test images: 88.82978723404256 %\n",
      "epoch : 14\n",
      "training loss: 0.0006, acc 97.4823 \n",
      "Accuracy of the network on the 10000 test images: 88.69148936170212 %\n",
      "epoch : 15\n",
      "training loss: 0.0005, acc 97.8670 \n",
      "Accuracy of the network on the 10000 test images: 88.93617021276596 %\n",
      "epoch : 16\n",
      "training loss: 0.0005, acc 98.2713 \n",
      "Accuracy of the network on the 10000 test images: 88.74468085106383 %\n",
      "epoch : 17\n",
      "training loss: 0.0004, acc 98.4601 \n",
      "Accuracy of the network on the 10000 test images: 88.66489361702128 %\n",
      "epoch : 18\n",
      "training loss: 0.0004, acc 98.6356 \n",
      "Accuracy of the network on the 10000 test images: 88.53191489361703 %\n",
      "epoch : 19\n",
      "training loss: 0.0003, acc 98.8715 \n",
      "Accuracy of the network on the 10000 test images: 88.76595744680851 %\n",
      "epoch : 20\n",
      "training loss: 0.0003, acc 98.9778 \n",
      "Accuracy of the network on the 10000 test images: 88.6436170212766 %\n",
      "epoch : 21\n",
      "training loss: 0.0003, acc 99.1285 \n",
      "Accuracy of the network on the 10000 test images: 88.51595744680851 %\n",
      "epoch : 22\n",
      "training loss: 0.0003, acc 99.2296 \n",
      "Accuracy of the network on the 10000 test images: 88.41489361702128 %\n",
      "epoch : 23\n",
      "training loss: 0.0002, acc 99.2872 \n",
      "Accuracy of the network on the 10000 test images: 88.6436170212766 %\n",
      "epoch : 24\n",
      "training loss: 0.0002, acc 99.3289 \n",
      "Accuracy of the network on the 10000 test images: 88.5372340425532 %\n",
      "epoch : 25\n",
      "training loss: 0.0002, acc 99.3404 \n",
      "Accuracy of the network on the 10000 test images: 88.24468085106383 %\n",
      "epoch : 26\n",
      "training loss: 0.0003, acc 99.2243 \n",
      "Accuracy of the network on the 10000 test images: 87.76595744680851 %\n",
      "epoch : 27\n",
      "training loss: 0.0003, acc 99.0878 \n",
      "Accuracy of the network on the 10000 test images: 88.41489361702128 %\n",
      "epoch : 28\n",
      "training loss: 0.0002, acc 99.5257 \n",
      "Accuracy of the network on the 10000 test images: 88.36702127659575 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.7367 \n",
      "Accuracy of the network on the 10000 test images: 88.44680851063829 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 99.8076 \n",
      "Accuracy of the network on the 10000 test images: 88.57978723404256 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 99.9273 \n",
      "Accuracy of the network on the 10000 test images: 88.83510638297872 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 99.9867 \n",
      "Accuracy of the network on the 10000 test images: 88.87765957446808 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9991 \n",
      "Accuracy of the network on the 10000 test images: 89.01063829787235 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.91489361702128 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.89893617021276 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.94680851063829 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.95744680851064 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.91489361702128 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.93617021276596 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.01063829787235 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.9627659574468 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.02659574468085 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.87234042553192 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.93085106382979 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.00531914893617 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.94680851063829 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.89893617021276 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.82978723404256 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.90425531914893 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.87234042553192 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c435663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:34:04.211995Z",
     "start_time": "2022-04-08T00:34:04.196037Z"
    },
    "id": "2c435663"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net9 - training loss'] = running_loss_history\n",
    "results['net9 - training accuracy'] = running_corrects_history\n",
    "results['net9 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68936a",
   "metadata": {
    "id": "ba68936a"
   },
   "source": [
    "## 10 Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "556c41ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:34:04.683734Z",
     "start_time": "2022-04-08T00:34:04.212993Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "556c41ef",
    "outputId": "f3f35195-c6c5-4d54-b7ab-5ad8b80167fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_2(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Flatten(start_dim=1, end_dim=-1)\n",
       "    (27): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (28): ReLU()\n",
       "    (29): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_2()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83171665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T00:34:04.698694Z",
     "start_time": "2022-04-08T00:34:04.684732Z"
    },
    "id": "83171665"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89903fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:12:30.806357Z",
     "start_time": "2022-04-08T00:34:04.699693Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "89903fe7",
    "outputId": "70103797-e4dd-4d23-f136-ce444c68d2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0058, acc 78.2385 \n",
      "Accuracy of the network on the 10000 test images: 86.52659574468085 %\n",
      "epoch : 2\n",
      "training loss: 0.0025, acc 88.4264 \n",
      "Accuracy of the network on the 10000 test images: 87.83510638297872 %\n",
      "epoch : 3\n",
      "training loss: 0.0021, acc 90.0426 \n",
      "Accuracy of the network on the 10000 test images: 88.37765957446808 %\n",
      "epoch : 4\n",
      "training loss: 0.0018, acc 91.2553 \n",
      "Accuracy of the network on the 10000 test images: 88.96808510638297 %\n",
      "epoch : 5\n",
      "training loss: 0.0016, acc 92.2748 \n",
      "Accuracy of the network on the 10000 test images: 88.97340425531915 %\n",
      "epoch : 6\n",
      "training loss: 0.0014, acc 93.3839 \n",
      "Accuracy of the network on the 10000 test images: 88.79255319148936 %\n",
      "epoch : 7\n",
      "training loss: 0.0012, acc 94.5904 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 8\n",
      "training loss: 0.0010, acc 95.7207 \n",
      "Accuracy of the network on the 10000 test images: 88.73404255319149 %\n",
      "epoch : 9\n",
      "training loss: 0.0008, acc 96.6747 \n",
      "Accuracy of the network on the 10000 test images: 88.35106382978724 %\n",
      "epoch : 10\n",
      "training loss: 0.0006, acc 97.4530 \n",
      "Accuracy of the network on the 10000 test images: 88.58510638297872 %\n",
      "epoch : 11\n",
      "training loss: 0.0005, acc 98.0842 \n",
      "Accuracy of the network on the 10000 test images: 88.69148936170212 %\n",
      "epoch : 12\n",
      "training loss: 0.0004, acc 98.4938 \n",
      "Accuracy of the network on the 10000 test images: 88.68617021276596 %\n",
      "epoch : 13\n",
      "training loss: 0.0003, acc 98.7802 \n",
      "Accuracy of the network on the 10000 test images: 88.79787234042553 %\n",
      "epoch : 14\n",
      "training loss: 0.0003, acc 98.9991 \n",
      "Accuracy of the network on the 10000 test images: 88.86702127659575 %\n",
      "epoch : 15\n",
      "training loss: 0.0002, acc 99.2172 \n",
      "Accuracy of the network on the 10000 test images: 88.8563829787234 %\n",
      "epoch : 16\n",
      "training loss: 0.0002, acc 99.2872 \n",
      "Accuracy of the network on the 10000 test images: 88.59042553191489 %\n",
      "epoch : 17\n",
      "training loss: 0.0002, acc 99.3848 \n",
      "Accuracy of the network on the 10000 test images: 88.67553191489361 %\n",
      "epoch : 18\n",
      "training loss: 0.0002, acc 99.3298 \n",
      "Accuracy of the network on the 10000 test images: 88.43085106382979 %\n",
      "epoch : 19\n",
      "training loss: 0.0003, acc 99.1020 \n",
      "Accuracy of the network on the 10000 test images: 88.23404255319149 %\n",
      "epoch : 20\n",
      "training loss: 0.0003, acc 98.9317 \n",
      "Accuracy of the network on the 10000 test images: 88.54787234042553 %\n",
      "epoch : 21\n",
      "training loss: 0.0003, acc 98.9184 \n",
      "Accuracy of the network on the 10000 test images: 88.05851063829788 %\n",
      "epoch : 22\n",
      "training loss: 0.0003, acc 99.0195 \n",
      "Accuracy of the network on the 10000 test images: 88.45212765957447 %\n",
      "epoch : 23\n",
      "training loss: 0.0002, acc 99.4459 \n",
      "Accuracy of the network on the 10000 test images: 88.16489361702128 %\n",
      "epoch : 24\n",
      "training loss: 0.0001, acc 99.7083 \n",
      "Accuracy of the network on the 10000 test images: 88.41489361702128 %\n",
      "epoch : 25\n",
      "training loss: 0.0001, acc 99.9362 \n",
      "Accuracy of the network on the 10000 test images: 88.75 %\n",
      "epoch : 26\n",
      "training loss: 0.0000, acc 99.9920 \n",
      "Accuracy of the network on the 10000 test images: 88.98404255319149 %\n",
      "epoch : 27\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 88.97340425531915 %\n",
      "epoch : 28\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.07978723404256 %\n",
      "epoch : 29\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.11170212765957 %\n",
      "epoch : 30\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.15425531914893 %\n",
      "epoch : 31\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.16489361702128 %\n",
      "epoch : 32\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.07446808510639 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.19148936170212 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.20744680851064 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.12765957446808 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.13297872340425 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.14893617021276 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.23936170212765 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.22340425531915 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.17553191489361 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.14893617021276 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.23404255319149 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.15425531914893 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.18617021276596 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.23404255319149 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.18085106382979 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1436170212766 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.13829787234043 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.11702127659575 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f424c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:12:30.821798Z",
     "start_time": "2022-04-08T01:12:30.806357Z"
    },
    "id": "0f424c18"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net10 - training loss'] = running_loss_history\n",
    "results['net10 - training accuracy'] = running_corrects_history\n",
    "results['net10 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aebd6f",
   "metadata": {
    "id": "12aebd6f"
   },
   "source": [
    "# ------------------- Added Convolutional Layer per Block -------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6cfad",
   "metadata": {
    "id": "bad6cfad"
   },
   "source": [
    "## 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68043eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:12:30.869670Z",
     "start_time": "2022-04-08T01:12:30.822796Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68043eed",
    "outputId": "f5cac548-581b-4361-98dc-a6ae91b413d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_2_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_2_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_2_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b1291c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:12:30.884630Z",
     "start_time": "2022-04-08T01:12:30.870668Z"
    },
    "id": "6b1291c6"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbea4d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:18:22.660029Z",
     "start_time": "2022-04-08T01:12:30.885627Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "cbea4d34",
    "outputId": "53502d48-6e2f-425c-f3c7-5474739e1aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0118, acc 63.0381 \n",
      "Accuracy of the network on the 10000 test images: 79.38297872340425 %\n",
      "epoch : 2\n",
      "training loss: 0.0043, acc 82.2447 \n",
      "Accuracy of the network on the 10000 test images: 83.0372340425532 %\n",
      "epoch : 3\n",
      "training loss: 0.0036, acc 84.9229 \n",
      "Accuracy of the network on the 10000 test images: 84.90425531914893 %\n",
      "epoch : 4\n",
      "training loss: 0.0032, acc 86.1543 \n",
      "Accuracy of the network on the 10000 test images: 85.94680851063829 %\n",
      "epoch : 5\n",
      "training loss: 0.0030, acc 87.1791 \n",
      "Accuracy of the network on the 10000 test images: 86.31914893617021 %\n",
      "epoch : 6\n",
      "training loss: 0.0028, acc 87.8555 \n",
      "Accuracy of the network on the 10000 test images: 86.47872340425532 %\n",
      "epoch : 7\n",
      "training loss: 0.0026, acc 88.3546 \n",
      "Accuracy of the network on the 10000 test images: 87.31914893617021 %\n",
      "epoch : 8\n",
      "training loss: 0.0025, acc 88.8679 \n",
      "Accuracy of the network on the 10000 test images: 87.68085106382979 %\n",
      "epoch : 9\n",
      "training loss: 0.0024, acc 89.2145 \n",
      "Accuracy of the network on the 10000 test images: 87.24468085106383 %\n",
      "epoch : 10\n",
      "training loss: 0.0023, acc 89.6489 \n",
      "Accuracy of the network on the 10000 test images: 87.86170212765957 %\n",
      "epoch : 11\n",
      "training loss: 0.0022, acc 90.0390 \n",
      "Accuracy of the network on the 10000 test images: 87.71808510638297 %\n",
      "epoch : 12\n",
      "training loss: 0.0021, acc 90.2473 \n",
      "Accuracy of the network on the 10000 test images: 88.12765957446808 %\n",
      "epoch : 13\n",
      "training loss: 0.0021, acc 90.5878 \n",
      "Accuracy of the network on the 10000 test images: 87.84574468085107 %\n",
      "epoch : 14\n",
      "training loss: 0.0020, acc 90.9255 \n",
      "Accuracy of the network on the 10000 test images: 88.33510638297872 %\n",
      "epoch : 15\n",
      "training loss: 0.0019, acc 91.1294 \n",
      "Accuracy of the network on the 10000 test images: 88.43617021276596 %\n",
      "epoch : 16\n",
      "training loss: 0.0019, acc 91.4823 \n",
      "Accuracy of the network on the 10000 test images: 88.32446808510639 %\n",
      "epoch : 17\n",
      "training loss: 0.0018, acc 91.7793 \n",
      "Accuracy of the network on the 10000 test images: 88.29787234042553 %\n",
      "epoch : 18\n",
      "training loss: 0.0017, acc 91.9814 \n",
      "Accuracy of the network on the 10000 test images: 88.10106382978724 %\n",
      "epoch : 19\n",
      "training loss: 0.0017, acc 92.2048 \n",
      "Accuracy of the network on the 10000 test images: 88.52127659574468 %\n",
      "epoch : 20\n",
      "training loss: 0.0016, acc 92.5718 \n",
      "Accuracy of the network on the 10000 test images: 88.55851063829788 %\n",
      "epoch : 21\n",
      "training loss: 0.0016, acc 92.7793 \n",
      "Accuracy of the network on the 10000 test images: 88.38297872340425 %\n",
      "epoch : 22\n",
      "training loss: 0.0015, acc 93.1188 \n",
      "Accuracy of the network on the 10000 test images: 88.41489361702128 %\n",
      "epoch : 23\n",
      "training loss: 0.0015, acc 93.3395 \n",
      "Accuracy of the network on the 10000 test images: 88.43085106382979 %\n",
      "epoch : 24\n",
      "training loss: 0.0014, acc 93.5887 \n",
      "Accuracy of the network on the 10000 test images: 88.57978723404256 %\n",
      "epoch : 25\n",
      "training loss: 0.0014, acc 93.8679 \n",
      "Accuracy of the network on the 10000 test images: 88.32446808510639 %\n",
      "epoch : 26\n",
      "training loss: 0.0013, acc 94.1489 \n",
      "Accuracy of the network on the 10000 test images: 88.22872340425532 %\n",
      "epoch : 27\n",
      "training loss: 0.0013, acc 94.3546 \n",
      "Accuracy of the network on the 10000 test images: 88.54787234042553 %\n",
      "epoch : 28\n",
      "training loss: 0.0012, acc 94.6445 \n",
      "Accuracy of the network on the 10000 test images: 88.33510638297872 %\n",
      "epoch : 29\n",
      "training loss: 0.0012, acc 94.7855 \n",
      "Accuracy of the network on the 10000 test images: 88.32978723404256 %\n",
      "epoch : 30\n",
      "training loss: 0.0011, acc 95.1117 \n",
      "Accuracy of the network on the 10000 test images: 88.23936170212765 %\n",
      "epoch : 31\n",
      "training loss: 0.0011, acc 95.3422 \n",
      "Accuracy of the network on the 10000 test images: 88.11170212765957 %\n",
      "epoch : 32\n",
      "training loss: 0.0010, acc 95.5417 \n",
      "Accuracy of the network on the 10000 test images: 87.92553191489361 %\n",
      "epoch : 33\n",
      "training loss: 0.0010, acc 95.7766 \n",
      "Accuracy of the network on the 10000 test images: 88.36170212765957 %\n",
      "epoch : 34\n",
      "training loss: 0.0009, acc 95.9743 \n",
      "Accuracy of the network on the 10000 test images: 88.27127659574468 %\n",
      "epoch : 35\n",
      "training loss: 0.0009, acc 96.2908 \n",
      "Accuracy of the network on the 10000 test images: 88.09042553191489 %\n",
      "epoch : 36\n",
      "training loss: 0.0009, acc 96.4459 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 37\n",
      "training loss: 0.0008, acc 96.6489 \n",
      "Accuracy of the network on the 10000 test images: 88.10106382978724 %\n",
      "epoch : 38\n",
      "training loss: 0.0008, acc 96.8280 \n",
      "Accuracy of the network on the 10000 test images: 87.80851063829788 %\n",
      "epoch : 39\n",
      "training loss: 0.0008, acc 96.9814 \n",
      "Accuracy of the network on the 10000 test images: 87.81382978723404 %\n",
      "epoch : 40\n",
      "training loss: 0.0007, acc 97.1374 \n",
      "Accuracy of the network on the 10000 test images: 87.87765957446808 %\n",
      "epoch : 41\n",
      "training loss: 0.0007, acc 97.3369 \n",
      "Accuracy of the network on the 10000 test images: 87.91489361702128 %\n",
      "epoch : 42\n",
      "training loss: 0.0007, acc 97.4583 \n",
      "Accuracy of the network on the 10000 test images: 87.81382978723404 %\n",
      "epoch : 43\n",
      "training loss: 0.0006, acc 97.6392 \n",
      "Accuracy of the network on the 10000 test images: 87.93617021276596 %\n",
      "epoch : 44\n",
      "training loss: 0.0006, acc 97.7004 \n",
      "Accuracy of the network on the 10000 test images: 87.82446808510639 %\n",
      "epoch : 45\n",
      "training loss: 0.0006, acc 97.8245 \n",
      "Accuracy of the network on the 10000 test images: 87.80319148936171 %\n",
      "epoch : 46\n",
      "training loss: 0.0006, acc 97.9849 \n",
      "Accuracy of the network on the 10000 test images: 87.80319148936171 %\n",
      "epoch : 47\n",
      "training loss: 0.0005, acc 98.0434 \n",
      "Accuracy of the network on the 10000 test images: 87.78191489361703 %\n",
      "epoch : 48\n",
      "training loss: 0.0005, acc 98.1037 \n",
      "Accuracy of the network on the 10000 test images: 87.76063829787235 %\n",
      "epoch : 49\n",
      "training loss: 0.0005, acc 98.2004 \n",
      "Accuracy of the network on the 10000 test images: 87.57446808510639 %\n",
      "epoch : 50\n",
      "training loss: 0.0005, acc 98.3528 \n",
      "Accuracy of the network on the 10000 test images: 87.84042553191489 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7b0fbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:18:22.675986Z",
     "start_time": "2022-04-08T01:18:22.661026Z"
    },
    "id": "b7b0fbf5"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net11 - training loss'] = running_loss_history\n",
    "results['net11 - training accuracy'] = running_corrects_history\n",
    "results['net11 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f7b2",
   "metadata": {
    "id": "26e6f7b2"
   },
   "source": [
    "## 4 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4d236ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:18:22.755773Z",
     "start_time": "2022-04-08T01:18:22.676984Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4d236ce",
    "outputId": "e7195b89-739b-44c6-d20d-816c53714e24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_4_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_4_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), # extra\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12544, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_4_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "041336eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:18:22.771866Z",
     "start_time": "2022-04-08T01:18:22.756771Z"
    },
    "id": "041336eb"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7c79bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:26:13.251253Z",
     "start_time": "2022-04-08T01:18:22.772864Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "d7c79bd2",
    "outputId": "03fb6389-d464-42a3-e4aa-1fe9d38c242e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0092, acc 69.5612 \n",
      "Accuracy of the network on the 10000 test images: 83.28191489361703 %\n",
      "epoch : 2\n",
      "training loss: 0.0034, acc 85.3041 \n",
      "Accuracy of the network on the 10000 test images: 86.18085106382979 %\n",
      "epoch : 3\n",
      "training loss: 0.0029, acc 87.1684 \n",
      "Accuracy of the network on the 10000 test images: 87.08510638297872 %\n",
      "epoch : 4\n",
      "training loss: 0.0026, acc 88.0612 \n",
      "Accuracy of the network on the 10000 test images: 87.25531914893617 %\n",
      "epoch : 5\n",
      "training loss: 0.0024, acc 89.0310 \n",
      "Accuracy of the network on the 10000 test images: 87.63297872340425 %\n",
      "epoch : 6\n",
      "training loss: 0.0023, acc 89.3821 \n",
      "Accuracy of the network on the 10000 test images: 88.38829787234043 %\n",
      "epoch : 7\n",
      "training loss: 0.0022, acc 89.7988 \n",
      "Accuracy of the network on the 10000 test images: 88.56382978723404 %\n",
      "epoch : 8\n",
      "training loss: 0.0021, acc 90.3316 \n",
      "Accuracy of the network on the 10000 test images: 88.42553191489361 %\n",
      "epoch : 9\n",
      "training loss: 0.0020, acc 90.5745 \n",
      "Accuracy of the network on the 10000 test images: 88.73404255319149 %\n",
      "epoch : 10\n",
      "training loss: 0.0019, acc 90.8200 \n",
      "Accuracy of the network on the 10000 test images: 88.39893617021276 %\n",
      "epoch : 11\n",
      "training loss: 0.0019, acc 91.1658 \n",
      "Accuracy of the network on the 10000 test images: 88.90957446808511 %\n",
      "epoch : 12\n",
      "training loss: 0.0018, acc 91.4778 \n",
      "Accuracy of the network on the 10000 test images: 88.49468085106383 %\n",
      "epoch : 13\n",
      "training loss: 0.0017, acc 91.7598 \n",
      "Accuracy of the network on the 10000 test images: 88.92553191489361 %\n",
      "epoch : 14\n",
      "training loss: 0.0017, acc 91.9840 \n",
      "Accuracy of the network on the 10000 test images: 88.93617021276596 %\n",
      "epoch : 15\n",
      "training loss: 0.0016, acc 92.2890 \n",
      "Accuracy of the network on the 10000 test images: 88.92553191489361 %\n",
      "epoch : 16\n",
      "training loss: 0.0016, acc 92.6135 \n",
      "Accuracy of the network on the 10000 test images: 88.55319148936171 %\n",
      "epoch : 17\n",
      "training loss: 0.0015, acc 92.9956 \n",
      "Accuracy of the network on the 10000 test images: 88.69680851063829 %\n",
      "epoch : 18\n",
      "training loss: 0.0014, acc 93.1835 \n",
      "Accuracy of the network on the 10000 test images: 89.05851063829788 %\n",
      "epoch : 19\n",
      "training loss: 0.0014, acc 93.4583 \n",
      "Accuracy of the network on the 10000 test images: 88.90957446808511 %\n",
      "epoch : 20\n",
      "training loss: 0.0013, acc 93.7287 \n",
      "Accuracy of the network on the 10000 test images: 89.04787234042553 %\n",
      "epoch : 21\n",
      "training loss: 0.0013, acc 93.9681 \n",
      "Accuracy of the network on the 10000 test images: 88.96808510638297 %\n",
      "epoch : 22\n",
      "training loss: 0.0012, acc 94.3316 \n",
      "Accuracy of the network on the 10000 test images: 88.88297872340425 %\n",
      "epoch : 23\n",
      "training loss: 0.0012, acc 94.5576 \n",
      "Accuracy of the network on the 10000 test images: 88.79255319148936 %\n",
      "epoch : 24\n",
      "training loss: 0.0011, acc 94.8280 \n",
      "Accuracy of the network on the 10000 test images: 88.87765957446808 %\n",
      "epoch : 25\n",
      "training loss: 0.0011, acc 95.0762 \n",
      "Accuracy of the network on the 10000 test images: 88.51595744680851 %\n",
      "epoch : 26\n",
      "training loss: 0.0010, acc 95.3741 \n",
      "Accuracy of the network on the 10000 test images: 88.52659574468085 %\n",
      "epoch : 27\n",
      "training loss: 0.0010, acc 95.6782 \n",
      "Accuracy of the network on the 10000 test images: 88.6063829787234 %\n",
      "epoch : 28\n",
      "training loss: 0.0009, acc 95.8936 \n",
      "Accuracy of the network on the 10000 test images: 88.52127659574468 %\n",
      "epoch : 29\n",
      "training loss: 0.0009, acc 96.1410 \n",
      "Accuracy of the network on the 10000 test images: 88.20744680851064 %\n",
      "epoch : 30\n",
      "training loss: 0.0008, acc 96.3732 \n",
      "Accuracy of the network on the 10000 test images: 88.54787234042553 %\n",
      "epoch : 31\n",
      "training loss: 0.0008, acc 96.5957 \n",
      "Accuracy of the network on the 10000 test images: 88.20744680851064 %\n",
      "epoch : 32\n",
      "training loss: 0.0008, acc 96.8546 \n",
      "Accuracy of the network on the 10000 test images: 88.29787234042553 %\n",
      "epoch : 33\n",
      "training loss: 0.0007, acc 97.1055 \n",
      "Accuracy of the network on the 10000 test images: 88.42553191489361 %\n",
      "epoch : 34\n",
      "training loss: 0.0007, acc 97.2181 \n",
      "Accuracy of the network on the 10000 test images: 88.29255319148936 %\n",
      "epoch : 35\n",
      "training loss: 0.0006, acc 97.4734 \n",
      "Accuracy of the network on the 10000 test images: 88.47340425531915 %\n",
      "epoch : 36\n",
      "training loss: 0.0006, acc 97.6197 \n",
      "Accuracy of the network on the 10000 test images: 88.06382978723404 %\n",
      "epoch : 37\n",
      "training loss: 0.0006, acc 97.7225 \n",
      "Accuracy of the network on the 10000 test images: 88.10106382978724 %\n",
      "epoch : 38\n",
      "training loss: 0.0006, acc 97.8608 \n",
      "Accuracy of the network on the 10000 test images: 88.1063829787234 %\n",
      "epoch : 39\n",
      "training loss: 0.0005, acc 98.0168 \n",
      "Accuracy of the network on the 10000 test images: 87.96808510638297 %\n",
      "epoch : 40\n",
      "training loss: 0.0005, acc 98.1436 \n",
      "Accuracy of the network on the 10000 test images: 88.0372340425532 %\n",
      "epoch : 41\n",
      "training loss: 0.0005, acc 98.2571 \n",
      "Accuracy of the network on the 10000 test images: 88.22340425531915 %\n",
      "epoch : 42\n",
      "training loss: 0.0005, acc 98.3901 \n",
      "Accuracy of the network on the 10000 test images: 88.13297872340425 %\n",
      "epoch : 43\n",
      "training loss: 0.0004, acc 98.5142 \n",
      "Accuracy of the network on the 10000 test images: 87.86702127659575 %\n",
      "epoch : 44\n",
      "training loss: 0.0004, acc 98.5426 \n",
      "Accuracy of the network on the 10000 test images: 87.90957446808511 %\n",
      "epoch : 45\n",
      "training loss: 0.0004, acc 98.6135 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 46\n",
      "training loss: 0.0004, acc 98.7651 \n",
      "Accuracy of the network on the 10000 test images: 87.93085106382979 %\n",
      "epoch : 47\n",
      "training loss: 0.0004, acc 98.7757 \n",
      "Accuracy of the network on the 10000 test images: 88.24468085106383 %\n",
      "epoch : 48\n",
      "training loss: 0.0004, acc 98.9051 \n",
      "Accuracy of the network on the 10000 test images: 87.87234042553192 %\n",
      "epoch : 49\n",
      "training loss: 0.0003, acc 98.9415 \n",
      "Accuracy of the network on the 10000 test images: 87.6063829787234 %\n",
      "epoch : 50\n",
      "training loss: 0.0003, acc 99.0337 \n",
      "Accuracy of the network on the 10000 test images: 88.0372340425532 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b859642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:26:13.266213Z",
     "start_time": "2022-04-08T01:26:13.252251Z"
    },
    "id": "8b859642"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net12 - training loss'] = running_loss_history\n",
    "results['net12 - training accuracy'] = running_corrects_history\n",
    "results['net12 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2b81f",
   "metadata": {
    "id": "7ac2b81f"
   },
   "source": [
    "## 6 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "435c8114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:26:13.407883Z",
     "start_time": "2022-04-08T01:26:13.267211Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "435c8114",
    "outputId": "0ae7f8d0-8d2f-4248-9868-41af9d6b5f39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_6_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Flatten(start_dim=1, end_dim=-1)\n",
       "    (23): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (24): ReLU()\n",
       "    (25): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_6_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_6_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "363ac576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:26:13.423922Z",
     "start_time": "2022-04-08T01:26:13.408880Z"
    },
    "id": "363ac576"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f58b454d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:38:28.039792Z",
     "start_time": "2022-04-08T01:26:13.424900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "f58b454d",
    "outputId": "4d057841-a49a-4bab-b210-a79b876f4918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0073, acc 74.6862 \n",
      "Accuracy of the network on the 10000 test images: 84.14893617021276 %\n",
      "epoch : 2\n",
      "training loss: 0.0029, acc 87.0231 \n",
      "Accuracy of the network on the 10000 test images: 86.69680851063829 %\n",
      "epoch : 3\n",
      "training loss: 0.0025, acc 88.5523 \n",
      "Accuracy of the network on the 10000 test images: 87.8563829787234 %\n",
      "epoch : 4\n",
      "training loss: 0.0023, acc 89.2855 \n",
      "Accuracy of the network on the 10000 test images: 88.31914893617021 %\n",
      "epoch : 5\n",
      "training loss: 0.0022, acc 89.9087 \n",
      "Accuracy of the network on the 10000 test images: 88.88297872340425 %\n",
      "epoch : 6\n",
      "training loss: 0.0020, acc 90.4255 \n",
      "Accuracy of the network on the 10000 test images: 89.08510638297872 %\n",
      "epoch : 7\n",
      "training loss: 0.0019, acc 90.8378 \n",
      "Accuracy of the network on the 10000 test images: 89.10106382978724 %\n",
      "epoch : 8\n",
      "training loss: 0.0018, acc 91.3200 \n",
      "Accuracy of the network on the 10000 test images: 88.9627659574468 %\n",
      "epoch : 9\n",
      "training loss: 0.0017, acc 91.5390 \n",
      "Accuracy of the network on the 10000 test images: 89.34042553191489 %\n",
      "epoch : 10\n",
      "training loss: 0.0017, acc 92.0585 \n",
      "Accuracy of the network on the 10000 test images: 89.31914893617021 %\n",
      "epoch : 11\n",
      "training loss: 0.0016, acc 92.4459 \n",
      "Accuracy of the network on the 10000 test images: 89.25 %\n",
      "epoch : 12\n",
      "training loss: 0.0015, acc 92.7775 \n",
      "Accuracy of the network on the 10000 test images: 89.41489361702128 %\n",
      "epoch : 13\n",
      "training loss: 0.0014, acc 93.1241 \n",
      "Accuracy of the network on the 10000 test images: 89.36170212765957 %\n",
      "epoch : 14\n",
      "training loss: 0.0013, acc 93.5381 \n",
      "Accuracy of the network on the 10000 test images: 89.43085106382979 %\n",
      "epoch : 15\n",
      "training loss: 0.0013, acc 93.9459 \n",
      "Accuracy of the network on the 10000 test images: 88.95212765957447 %\n",
      "epoch : 16\n",
      "training loss: 0.0012, acc 94.3546 \n",
      "Accuracy of the network on the 10000 test images: 89.45744680851064 %\n",
      "epoch : 17\n",
      "training loss: 0.0011, acc 94.7509 \n",
      "Accuracy of the network on the 10000 test images: 89.13297872340425 %\n",
      "epoch : 18\n",
      "training loss: 0.0010, acc 95.1498 \n",
      "Accuracy of the network on the 10000 test images: 88.9627659574468 %\n",
      "epoch : 19\n",
      "training loss: 0.0010, acc 95.3511 \n",
      "Accuracy of the network on the 10000 test images: 88.80319148936171 %\n",
      "epoch : 20\n",
      "training loss: 0.0009, acc 95.8298 \n",
      "Accuracy of the network on the 10000 test images: 88.84574468085107 %\n",
      "epoch : 21\n",
      "training loss: 0.0009, acc 96.1259 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 22\n",
      "training loss: 0.0008, acc 96.4539 \n",
      "Accuracy of the network on the 10000 test images: 88.86702127659575 %\n",
      "epoch : 23\n",
      "training loss: 0.0007, acc 96.7580 \n",
      "Accuracy of the network on the 10000 test images: 88.83510638297872 %\n",
      "epoch : 24\n",
      "training loss: 0.0007, acc 97.0629 \n",
      "Accuracy of the network on the 10000 test images: 88.87234042553192 %\n",
      "epoch : 25\n",
      "training loss: 0.0006, acc 97.2473 \n",
      "Accuracy of the network on the 10000 test images: 88.70744680851064 %\n",
      "epoch : 26\n",
      "training loss: 0.0006, acc 97.4699 \n",
      "Accuracy of the network on the 10000 test images: 88.57978723404256 %\n",
      "epoch : 27\n",
      "training loss: 0.0006, acc 97.7518 \n",
      "Accuracy of the network on the 10000 test images: 88.40957446808511 %\n",
      "epoch : 28\n",
      "training loss: 0.0005, acc 97.9007 \n",
      "Accuracy of the network on the 10000 test images: 88.51063829787235 %\n",
      "epoch : 29\n",
      "training loss: 0.0005, acc 97.9397 \n",
      "Accuracy of the network on the 10000 test images: 88.60106382978724 %\n",
      "epoch : 30\n",
      "training loss: 0.0005, acc 98.1649 \n",
      "Accuracy of the network on the 10000 test images: 88.52127659574468 %\n",
      "epoch : 31\n",
      "training loss: 0.0004, acc 98.3440 \n",
      "Accuracy of the network on the 10000 test images: 88.62765957446808 %\n",
      "epoch : 32\n",
      "training loss: 0.0004, acc 98.4282 \n",
      "Accuracy of the network on the 10000 test images: 88.64893617021276 %\n",
      "epoch : 33\n",
      "training loss: 0.0004, acc 98.4282 \n",
      "Accuracy of the network on the 10000 test images: 88.22872340425532 %\n",
      "epoch : 34\n",
      "training loss: 0.0004, acc 98.5434 \n",
      "Accuracy of the network on the 10000 test images: 88.60106382978724 %\n",
      "epoch : 35\n",
      "training loss: 0.0004, acc 98.5771 \n",
      "Accuracy of the network on the 10000 test images: 88.44680851063829 %\n",
      "epoch : 36\n",
      "training loss: 0.0004, acc 98.7296 \n",
      "Accuracy of the network on the 10000 test images: 88.52659574468085 %\n",
      "epoch : 37\n",
      "training loss: 0.0003, acc 98.7943 \n",
      "Accuracy of the network on the 10000 test images: 88.42021276595744 %\n",
      "epoch : 38\n",
      "training loss: 0.0003, acc 98.8342 \n",
      "Accuracy of the network on the 10000 test images: 88.29787234042553 %\n",
      "epoch : 39\n",
      "training loss: 0.0003, acc 98.9078 \n",
      "Accuracy of the network on the 10000 test images: 88.52659574468085 %\n",
      "epoch : 40\n",
      "training loss: 0.0003, acc 98.8333 \n",
      "Accuracy of the network on the 10000 test images: 88.2872340425532 %\n",
      "epoch : 41\n",
      "training loss: 0.0003, acc 98.8590 \n",
      "Accuracy of the network on the 10000 test images: 88.3563829787234 %\n",
      "epoch : 42\n",
      "training loss: 0.0003, acc 98.9317 \n",
      "Accuracy of the network on the 10000 test images: 88.17553191489361 %\n",
      "epoch : 43\n",
      "training loss: 0.0003, acc 99.0177 \n",
      "Accuracy of the network on the 10000 test images: 88.18085106382979 %\n",
      "epoch : 44\n",
      "training loss: 0.0003, acc 99.1809 \n",
      "Accuracy of the network on the 10000 test images: 88.55851063829788 %\n",
      "epoch : 45\n",
      "training loss: 0.0003, acc 99.2066 \n",
      "Accuracy of the network on the 10000 test images: 88.13829787234043 %\n",
      "epoch : 46\n",
      "training loss: 0.0002, acc 99.3830 \n",
      "Accuracy of the network on the 10000 test images: 88.43085106382979 %\n",
      "epoch : 47\n",
      "training loss: 0.0002, acc 99.3307 \n",
      "Accuracy of the network on the 10000 test images: 88.02659574468085 %\n",
      "epoch : 48\n",
      "training loss: 0.0002, acc 99.3812 \n",
      "Accuracy of the network on the 10000 test images: 88.36170212765957 %\n",
      "epoch : 49\n",
      "training loss: 0.0002, acc 99.3537 \n",
      "Accuracy of the network on the 10000 test images: 88.07446808510639 %\n",
      "epoch : 50\n",
      "training loss: 0.0002, acc 99.5745 \n",
      "Accuracy of the network on the 10000 test images: 88.08510638297872 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1dcb6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:38:28.054752Z",
     "start_time": "2022-04-08T01:38:28.040789Z"
    },
    "id": "f1dcb6c8"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net13 - training loss'] = running_loss_history\n",
    "results['net13 - training accuracy'] = running_corrects_history\n",
    "results['net13 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04074",
   "metadata": {
    "id": "5eb04074"
   },
   "source": [
    "## 8 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a407a44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:38:28.307077Z",
     "start_time": "2022-04-08T01:38:28.055750Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a407a44a",
    "outputId": "07d587ec-a4db-4be4-986e-bdbb5f5f9143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_8_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Flatten(start_dim=1, end_dim=-1)\n",
       "    (30): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (31): ReLU()\n",
       "    (32): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_8_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50176, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_8_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9da2c848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:38:28.322846Z",
     "start_time": "2022-04-08T01:38:28.308074Z"
    },
    "id": "9da2c848"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a06bb541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:59:48.056808Z",
     "start_time": "2022-04-08T01:38:28.323843Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "a06bb541",
    "outputId": "9c621f3f-fe33-4bb5-c401-05dff8fd77d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0064, acc 76.5895 \n",
      "Accuracy of the network on the 10000 test images: 86.01595744680851 %\n",
      "epoch : 2\n",
      "training loss: 0.0027, acc 87.6383 \n",
      "Accuracy of the network on the 10000 test images: 88.0372340425532 %\n",
      "epoch : 3\n",
      "training loss: 0.0023, acc 89.0541 \n",
      "Accuracy of the network on the 10000 test images: 88.32978723404256 %\n",
      "epoch : 4\n",
      "training loss: 0.0021, acc 90.0098 \n",
      "Accuracy of the network on the 10000 test images: 89.1063829787234 %\n",
      "epoch : 5\n",
      "training loss: 0.0020, acc 90.6383 \n",
      "Accuracy of the network on the 10000 test images: 88.91489361702128 %\n",
      "epoch : 6\n",
      "training loss: 0.0018, acc 91.2908 \n",
      "Accuracy of the network on the 10000 test images: 89.5 %\n",
      "epoch : 7\n",
      "training loss: 0.0017, acc 91.8041 \n",
      "Accuracy of the network on the 10000 test images: 89.12234042553192 %\n",
      "epoch : 8\n",
      "training loss: 0.0016, acc 92.3271 \n",
      "Accuracy of the network on the 10000 test images: 89.70212765957447 %\n",
      "epoch : 9\n",
      "training loss: 0.0015, acc 92.9592 \n",
      "Accuracy of the network on the 10000 test images: 89.38829787234043 %\n",
      "epoch : 10\n",
      "training loss: 0.0014, acc 93.4548 \n",
      "Accuracy of the network on the 10000 test images: 89.38297872340425 %\n",
      "epoch : 11\n",
      "training loss: 0.0012, acc 94.0071 \n",
      "Accuracy of the network on the 10000 test images: 89.34042553191489 %\n",
      "epoch : 12\n",
      "training loss: 0.0011, acc 94.5709 \n",
      "Accuracy of the network on the 10000 test images: 89.05319148936171 %\n",
      "epoch : 13\n",
      "training loss: 0.0010, acc 95.1002 \n",
      "Accuracy of the network on the 10000 test images: 88.98404255319149 %\n",
      "epoch : 14\n",
      "training loss: 0.0009, acc 95.5949 \n",
      "Accuracy of the network on the 10000 test images: 89.29787234042553 %\n",
      "epoch : 15\n",
      "training loss: 0.0008, acc 96.1223 \n",
      "Accuracy of the network on the 10000 test images: 88.95212765957447 %\n",
      "epoch : 16\n",
      "training loss: 0.0008, acc 96.6809 \n",
      "Accuracy of the network on the 10000 test images: 89.14893617021276 %\n",
      "epoch : 17\n",
      "training loss: 0.0007, acc 96.9184 \n",
      "Accuracy of the network on the 10000 test images: 88.64893617021276 %\n",
      "epoch : 18\n",
      "training loss: 0.0006, acc 97.3697 \n",
      "Accuracy of the network on the 10000 test images: 89.07978723404256 %\n",
      "epoch : 19\n",
      "training loss: 0.0006, acc 97.6481 \n",
      "Accuracy of the network on the 10000 test images: 88.72872340425532 %\n",
      "epoch : 20\n",
      "training loss: 0.0005, acc 97.9424 \n",
      "Accuracy of the network on the 10000 test images: 88.77127659574468 %\n",
      "epoch : 21\n",
      "training loss: 0.0005, acc 98.1933 \n",
      "Accuracy of the network on the 10000 test images: 88.97872340425532 %\n",
      "epoch : 22\n",
      "training loss: 0.0004, acc 98.3236 \n",
      "Accuracy of the network on the 10000 test images: 89.08510638297872 %\n",
      "epoch : 23\n",
      "training loss: 0.0004, acc 98.2092 \n",
      "Accuracy of the network on the 10000 test images: 88.69148936170212 %\n",
      "epoch : 24\n",
      "training loss: 0.0004, acc 98.2048 \n",
      "Accuracy of the network on the 10000 test images: 88.29255319148936 %\n",
      "epoch : 25\n",
      "training loss: 0.0004, acc 98.2420 \n",
      "Accuracy of the network on the 10000 test images: 88.17553191489361 %\n",
      "epoch : 26\n",
      "training loss: 0.0004, acc 98.4681 \n",
      "Accuracy of the network on the 10000 test images: 88.93617021276596 %\n",
      "epoch : 27\n",
      "training loss: 0.0003, acc 98.7473 \n",
      "Accuracy of the network on the 10000 test images: 88.63829787234043 %\n",
      "epoch : 28\n",
      "training loss: 0.0004, acc 98.4681 \n",
      "Accuracy of the network on the 10000 test images: 88.27659574468085 %\n",
      "epoch : 29\n",
      "training loss: 0.0003, acc 98.7447 \n",
      "Accuracy of the network on the 10000 test images: 88.3936170212766 %\n",
      "epoch : 30\n",
      "training loss: 0.0003, acc 98.9690 \n",
      "Accuracy of the network on the 10000 test images: 88.72340425531915 %\n",
      "epoch : 31\n",
      "training loss: 0.0003, acc 99.0922 \n",
      "Accuracy of the network on the 10000 test images: 88.42021276595744 %\n",
      "epoch : 32\n",
      "training loss: 0.0002, acc 99.3582 \n",
      "Accuracy of the network on the 10000 test images: 88.4627659574468 %\n",
      "epoch : 33\n",
      "training loss: 0.0002, acc 99.5036 \n",
      "Accuracy of the network on the 10000 test images: 88.55319148936171 %\n",
      "epoch : 34\n",
      "training loss: 0.0002, acc 99.6046 \n",
      "Accuracy of the network on the 10000 test images: 88.90425531914893 %\n",
      "epoch : 35\n",
      "training loss: 0.0001, acc 99.6995 \n",
      "Accuracy of the network on the 10000 test images: 88.65425531914893 %\n",
      "epoch : 36\n",
      "training loss: 0.0001, acc 99.8067 \n",
      "Accuracy of the network on the 10000 test images: 88.68617021276596 %\n",
      "epoch : 37\n",
      "training loss: 0.0001, acc 99.8803 \n",
      "Accuracy of the network on the 10000 test images: 88.94148936170212 %\n",
      "epoch : 38\n",
      "training loss: 0.0001, acc 99.6055 \n",
      "Accuracy of the network on the 10000 test images: 88.77127659574468 %\n",
      "epoch : 39\n",
      "training loss: 0.0001, acc 99.7261 \n",
      "Accuracy of the network on the 10000 test images: 88.82446808510639 %\n",
      "epoch : 40\n",
      "training loss: 0.0001, acc 99.9335 \n",
      "Accuracy of the network on the 10000 test images: 88.89893617021276 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 99.9973 \n",
      "Accuracy of the network on the 10000 test images: 89.09042553191489 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.18617021276596 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.09574468085107 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.18085106382979 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.19148936170212 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.12234042553192 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.03191489361703 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.12765957446808 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.11702127659575 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.17553191489361 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36d9cd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T01:59:48.071768Z",
     "start_time": "2022-04-08T01:59:48.057805Z"
    },
    "id": "36d9cd53"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net14 - training loss'] = running_loss_history\n",
    "results['net14 - training accuracy'] = running_corrects_history\n",
    "results['net14 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa0b44",
   "metadata": {
    "id": "11fa0b44"
   },
   "source": [
    "## 10 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd4f4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T06:45:07.228952Z",
     "start_time": "2022-04-08T06:45:06.655237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cd4f4dc",
    "outputId": "45f6c3f4-255f-4a5c-cd9b-44716ff5270e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_10_3(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (30): ReLU()\n",
       "    (31): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (32): ReLU()\n",
       "    (33): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): ReLU()\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): Flatten(start_dim=1, end_dim=-1)\n",
       "    (37): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (38): ReLU()\n",
       "    (39): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (40): ReLU()\n",
       "    (41): Linear(in_features=512, out_features=47, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "class Net_10_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            #block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            #block 2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            #block 3\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(128),\n",
    "             \n",
    "            #block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            #block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), #1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #2\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), max pooling operators removed after first block\n",
    "            nn.BatchNorm2d(512),\n",
    "        \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100352, 1024), #FC1    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), #FC2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 47)) #FC3\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "net = Net_10_3()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e42c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T06:45:10.710349Z",
     "start_time": "2022-04-08T06:45:10.702396Z"
    },
    "id": "22e42c81"
   },
   "outputs": [],
   "source": [
    "# criterion + optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c8aba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T07:30:13.234715Z",
     "start_time": "2022-04-08T06:45:48.073137Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "d0c8aba4",
    "outputId": "798dfe3b-b656-4431-8740-d9511f7d2867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "training loss: 0.0033, acc 85.3741 \n",
      "Accuracy of the network on the 10000 test images: 86.65957446808511 %\n",
      "epoch : 2\n",
      "training loss: 0.0024, acc 88.5922 \n",
      "Accuracy of the network on the 10000 test images: 88.00531914893617 %\n",
      "epoch : 3\n",
      "training loss: 0.0021, acc 89.8599 \n",
      "Accuracy of the network on the 10000 test images: 89.04255319148936 %\n",
      "epoch : 4\n",
      "training loss: 0.0019, acc 90.6888 \n",
      "Accuracy of the network on the 10000 test images: 89.15425531914893 %\n",
      "epoch : 5\n",
      "training loss: 0.0017, acc 91.5825 \n",
      "Accuracy of the network on the 10000 test images: 89.13829787234043 %\n",
      "epoch : 6\n",
      "training loss: 0.0016, acc 92.2979 \n",
      "Accuracy of the network on the 10000 test images: 89.34574468085107 %\n",
      "epoch : 7\n",
      "training loss: 0.0014, acc 93.1002 \n",
      "Accuracy of the network on the 10000 test images: 89.06382978723404 %\n",
      "epoch : 8\n",
      "training loss: 0.0012, acc 93.9131 \n",
      "Accuracy of the network on the 10000 test images: 88.83510638297872 %\n",
      "epoch : 9\n",
      "training loss: 0.0011, acc 94.7270 \n",
      "Accuracy of the network on the 10000 test images: 89.10106382978724 %\n",
      "epoch : 10\n",
      "training loss: 0.0010, acc 95.5319 \n",
      "Accuracy of the network on the 10000 test images: 88.62234042553192 %\n",
      "epoch : 11\n",
      "training loss: 0.0008, acc 96.2296 \n",
      "Accuracy of the network on the 10000 test images: 89.00531914893617 %\n",
      "epoch : 12\n",
      "training loss: 0.0007, acc 96.8067 \n",
      "Accuracy of the network on the 10000 test images: 88.96808510638297 %\n",
      "epoch : 13\n",
      "training loss: 0.0006, acc 97.2553 \n",
      "Accuracy of the network on the 10000 test images: 88.57446808510639 %\n",
      "epoch : 14\n",
      "training loss: 0.0005, acc 97.7722 \n",
      "Accuracy of the network on the 10000 test images: 88.51063829787235 %\n",
      "epoch : 15\n",
      "training loss: 0.0005, acc 98.0957 \n",
      "Accuracy of the network on the 10000 test images: 88.73404255319149 %\n",
      "epoch : 16\n",
      "training loss: 0.0005, acc 97.9628 \n",
      "Accuracy of the network on the 10000 test images: 88.42021276595744 %\n",
      "epoch : 17\n",
      "training loss: 0.0005, acc 98.0293 \n",
      "Accuracy of the network on the 10000 test images: 88.24468085106383 %\n",
      "epoch : 18\n",
      "training loss: 0.0005, acc 98.0310 \n",
      "Accuracy of the network on the 10000 test images: 88.62234042553192 %\n",
      "epoch : 19\n",
      "training loss: 0.0005, acc 97.9247 \n",
      "Accuracy of the network on the 10000 test images: 88.25 %\n",
      "epoch : 20\n",
      "training loss: 0.0004, acc 98.1844 \n",
      "Accuracy of the network on the 10000 test images: 88.4627659574468 %\n",
      "epoch : 21\n",
      "training loss: 0.0004, acc 98.2075 \n",
      "Accuracy of the network on the 10000 test images: 88.3936170212766 %\n",
      "epoch : 22\n",
      "training loss: 0.0004, acc 98.2624 \n",
      "Accuracy of the network on the 10000 test images: 88.44148936170212 %\n",
      "epoch : 23\n",
      "training loss: 0.0004, acc 98.5452 \n",
      "Accuracy of the network on the 10000 test images: 88.32978723404256 %\n",
      "epoch : 24\n",
      "training loss: 0.0003, acc 98.7181 \n",
      "Accuracy of the network on the 10000 test images: 88.57978723404256 %\n",
      "epoch : 25\n",
      "training loss: 0.0003, acc 98.7234 \n",
      "Accuracy of the network on the 10000 test images: 88.35106382978724 %\n",
      "epoch : 26\n",
      "training loss: 0.0003, acc 99.1002 \n",
      "Accuracy of the network on the 10000 test images: 88.57446808510639 %\n",
      "epoch : 27\n",
      "training loss: 0.0002, acc 99.2606 \n",
      "Accuracy of the network on the 10000 test images: 88.48404255319149 %\n",
      "epoch : 28\n",
      "training loss: 0.0002, acc 99.5692 \n",
      "Accuracy of the network on the 10000 test images: 88.51063829787235 %\n",
      "epoch : 29\n",
      "training loss: 0.0001, acc 99.5949 \n",
      "Accuracy of the network on the 10000 test images: 88.99468085106383 %\n",
      "epoch : 30\n",
      "training loss: 0.0001, acc 99.8076 \n",
      "Accuracy of the network on the 10000 test images: 88.6436170212766 %\n",
      "epoch : 31\n",
      "training loss: 0.0001, acc 99.8280 \n",
      "Accuracy of the network on the 10000 test images: 88.88297872340425 %\n",
      "epoch : 32\n",
      "training loss: 0.0001, acc 99.8218 \n",
      "Accuracy of the network on the 10000 test images: 88.87765957446808 %\n",
      "epoch : 33\n",
      "training loss: 0.0000, acc 99.9273 \n",
      "Accuracy of the network on the 10000 test images: 88.85106382978724 %\n",
      "epoch : 34\n",
      "training loss: 0.0000, acc 99.9894 \n",
      "Accuracy of the network on the 10000 test images: 89.25531914893617 %\n",
      "epoch : 35\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.30851063829788 %\n",
      "epoch : 36\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.34574468085107 %\n",
      "epoch : 37\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.37234042553192 %\n",
      "epoch : 38\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.3936170212766 %\n",
      "epoch : 39\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.42553191489361 %\n",
      "epoch : 40\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.38829787234043 %\n",
      "epoch : 41\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.44148936170212 %\n",
      "epoch : 42\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.45212765957447 %\n",
      "epoch : 43\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.36702127659575 %\n",
      "epoch : 44\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.32446808510639 %\n",
      "epoch : 45\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.38829787234043 %\n",
      "epoch : 46\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.31382978723404 %\n",
      "epoch : 47\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.32446808510639 %\n",
      "epoch : 48\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.37234042553192 %\n",
      "epoch : 49\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.31382978723404 %\n",
      "epoch : 50\n",
      "training loss: 0.0000, acc 100.0000 \n",
      "Accuracy of the network on the 10000 test images: 89.43617021276596 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "test_acc_history=[]\n",
    "\n",
    "for e in range(epochs): # training our model, put input according to every batch.\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        \n",
    "        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs) # every batch of 100 images are put as an input.\n",
    "        loss = criterion(outputs, labels) # Calc loss after each batch i/p by comparing it to actual labels. \n",
    "\n",
    "        optimizer.zero_grad() #setting the initial gradient to 0\n",
    "        loss.backward() # backpropagating the loss\n",
    "        optimizer.step() # updating the weights and bias values for every single step.\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainset) # loss per epoch\n",
    "    epoch_acc = 100*(running_corrects.float()/ len(trainset)) # accuracy per epoch\n",
    "    running_loss_history.append(epoch_loss) # appending for displaying \n",
    "    running_corrects_history.append(epoch_acc)\n",
    "                \n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    \n",
    "    # testing\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc_history.append(100 * correct / total)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea4134a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T07:30:13.250673Z",
     "start_time": "2022-04-08T07:30:13.235713Z"
    },
    "id": "6ea4134a"
   },
   "outputs": [],
   "source": [
    "running_loss_history = torch.tensor(running_loss_history)\n",
    "running_loss_history.numpy()\n",
    "running_corrects_history = torch.tensor(running_corrects_history)\n",
    "running_corrects_history.numpy()\n",
    "test_acc_history = torch.tensor(test_acc_history)\n",
    "test_acc_history.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results['net15 - training loss'] = running_loss_history\n",
    "results['net15 - training accuracy'] = running_corrects_history\n",
    "results['net15 - testing accuracy'] = test_acc_history\n",
    "\n",
    "results.to_csv('net_results_emnist.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "net_builder_EMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
